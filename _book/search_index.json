[
["index.html", "R for Psych Handbook Chapter 1 Preface", " R for Psych Handbook David John Baker 2018-03-25 Chapter 1 Preface This book serves as a collection of resources used in the LSU psychological statistics courses. It contains some lecture notes, as well as R code and data to run each of the examples in R. The book is still under construction. "],
["intro.html", "Chapter 2 Introduction to R", " Chapter 2 Introduction to R You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2017) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["data-manipulation-in-r.html", "Chapter 3 Data Manipulation in R", " Chapter 3 Data Manipulation in R Here is a review of existing methods. "],
["episotomology-of-statistics.html", "Chapter 4 Episotomology of Statistics", " Chapter 4 Episotomology of Statistics We describe our methods in this chapter. "],
["descriptive-statistics-z-scores-central-limit.html", "Chapter 5 Descriptive Statistics, z Scores, Central Limit 5.1 Descriptive Statistics and the Normal Distribution 5.2 Practice", " Chapter 5 Descriptive Statistics, z Scores, Central Limit 5.1 Descriptive Statistics and the Normal Distribution 5.1.1 Organizing Data Descriptive statistics are traditionally used to summarize data from observed samples. Most often, sample data are organized into distributions of information based on ascending scores. For example, we might have a table with some SAT-Verbal scores from a few different students. Before going on to think about this, also take note that the shape of this data (though very minimal) is in the tidy format. According to Hadley Wickham on the tidyr CRAN page for data to be tidy it must have the following properties: Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. This isn’t very important right now, but once you get to more complex designs, it will be good to have had thought about this before. student &lt;- c(1,2,3,4,5,6) SAT &lt;- c(480,530,560,650,720,760) satData &lt;- data.frame(student,SAT) satData ## student SAT ## 1 1 480 ## 2 2 530 ## 3 3 560 ## 4 4 650 ## 5 5 720 ## 6 6 760 5.1.2 Shape of Data When visualized, data can take on a variety of shapes. Below are a few of the shapes you might come across when analyzing data. The first, and probably least likely distribution you will find is the uniform or rectangular distribution. We can create this plot and the others by using the distribution functions from R’s functionality. In each case we are going to take 1,000 samples from 0 to 1. We’ll plot everything using ggplot2 so we can also get used to using it for our packages. library(ggplot2) set.seed(666) uniformData &lt;- runif(n=1000, min=0, max=100) distributions &lt;- data.frame(uniformData) # Make variable to remove ggplot elements cleanUpPlots &lt;- theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) ggplot(distributions, aes(distributions$uniformData)) + geom_histogram(binwidth = 5) + labs(x = &quot;Independent Variable&quot;, y = &quot;Frequency&quot;, title = &quot;Uniform Distribution&quot;) + cleanUpPlots Try to run the above code with different arguments in the binwidth argument. You’ll notice that the way you plot the data will actually represent it differntly. We can also have positively skewed and negatively skewed distributions. If a distribution is skewed, it usually means that the mode does not equal the mean. We’re going to approximate both of these with another one of R’s probability functions. positiveSkewData &lt;- rchisq(1000,6) # Chi Square distributions are positively skewed negativeSkewData &lt;- - positiveSkewData # Flip it! distributions &lt;- data.frame(uniformData, positiveSkewData, negativeSkewData) ggplot(distributions, aes(distributions$positiveSkewData)) + geom_histogram(binwidth = 1) + labs(x = &quot;Independent Variable&quot;, y = &quot;Frequency&quot;, title = &quot;Positive Skew Distribution&quot;) + cleanUpPlots ggplot(distributions, aes(distributions$negativeSkewData)) + geom_histogram(binwidth = 1) + labs(x = &quot;Independent Variable&quot;, y = &quot;Frequency&quot;, title = &quot;Negative Skew Distribution&quot;) + cleanUpPlots The most important distribution in the world of Frequentist statistics is a normal distribution. A normal distribution is defined by THIS HERE. normalData &lt;- rnorm(n = 1000,mean = 0,sd = 2) # Flip it! distributions &lt;- data.frame(uniformData, positiveSkewData, negativeSkewData, normalData) ggplot(distributions, aes(distributions$normalData)) + geom_histogram(binwidth = 1) + labs(x = &quot;Independent Variable&quot;, y = &quot;Frequency&quot;, title = &quot;Normal Distribution&quot;) + cleanUpPlots And the lastly we can have both leptokurtic and platykurtic distributions. leptoData &lt;- rnorm(n = 1000,mean = 0,sd = 2) platyData &lt;- rnorm(n = 1000,mean = 0,sd = 2) distributions &lt;- data.frame(uniformData, positiveSkewData, negativeSkewData, normalData,leptoData, platyData) ggplot(distributions, aes(distributions$leptoData)) + geom_histogram(binwidth = 1) + labs(x = &quot;Independent Variable&quot;, y = &quot;Frequency&quot;, title = &quot;Leptokurtic Distribution, FIX ME&quot;) + cleanUpPlots ggplot(distributions, aes(distributions$platyData)) + geom_histogram(binwidth = 1) + labs(x = &quot;Independent Variable&quot;, y = &quot;Frequency&quot;, title = &quot;Platykurtic Distribution, FIX ME&quot;) + cleanUpPlots Generally measurses of central tendency are used to characterize the most typical score in a distribution. For example we could calculate the mean or the median of our SAT data. The mean is calculated by adding up all our numbers, designated with the Greek letter Sigma \\(\\Sigma\\) then dividing by the amount of numbers we added up, or \\(n\\). As an equation it would look like this. \\[\\bar{X} = \\frac{(\\Sigma\\ x_i)}{n}\\] Take a second to talk yourself through that so later you will start to feel more comfortable with more complex equational notation. Some people find it helpful to just try to say the equation in plain English. In this case it would be the mean, or \\(\\bar{x}\\) is defined as or equal to what happens when you add up \\(\\Sigma\\) every single value \\(x\\) that you have going up to \\(i\\), then divide all those numbers by the amount of numbers you have, or \\(n\\). The median is defined by finding the middle number. If there is a tie because we have an even set of numbers, we take the mean of the middle two numbers. We can do both of these in R as well. Below we can either type in the numbers as you would with a calculator, or use a function in R. Notice that each step when its typed out is saved into an object. By starting to think this way, it will pave the way for writing more elegant code later on. # Typing it out our.data &lt;- c(480 + 530 + 560 + 650 + 720 + 760) how.many &lt;- length(our.data) our.data/how.many ## [1] 3700 # Inbuilt functions mean(satData$SAT) ## [1] 616.6667 median(satData$SAT) ## [1] 605 Notice here that for adding up the means by hand I could have done what programmers call hard coded the equation in. That would have looked like this. our.answer &lt;- c(480 + 530 + 560 + 650 + 720 + 760) / 6 our.answer ## [1] 616.6667 The problem with this, is that every time you get a new SAT score you have to both enter the score and update how many scores you are dividing by. Whenever you see a chance to take a shortcut like this, do it! It will save you tons of time in the future. 5.1.3 Important Considerations for Central Tendency There are three big considerations to think about when choosing numbers to represent your data. The mode is the most variable from sample to sample; the mean is the least variable The mean is the most sensitive to extreme scores; e.g., skewed distributions The mean is the most frequently used measure because The sum of the deviations around the mean is 0 The sum of the squared deviations is the smallest around the mean, rather than the mode or median; this is known as the least squares principle 5.1.4 Measures of Variability The range is simply the largest score minus the smallest score. It is the crudest measure of variability. In our dataset we would calculate it with the following code. 760 - 480 ## [1] 280 # OR range(satData$SAT) ## [1] 480 760 max(satData$SAT) - min(satData$SAT) ## [1] 280 The interquartile rangerepresents the spread between the score at the 75th and 25th percentiles. Boxplots are often used to graphically represent inner 50% of the scores. y &lt;- satData$SAT boxplot.example &lt;- data.frame( x = 1, y0 = min(y), y25 = quantile(y, 0.25), y50 = median(y), y75 = quantile(y, 0.75), y100 = max(y) ) ggplot(boxplot.example, aes(x)) + labs(title = &quot;Example of Boxplot&quot;) + geom_boxplot(aes(ymin = y0, lower = y25, middle = y50, upper = y75, ymax = y100), stat = &quot;identity&quot;) The variance is essentially the averaged squared deviation around the mean. Now there is a very important distinction that we will get to a bit later on, but that is the difference between a population value or \\(\\sigma^2\\) and a sample variance or \\(s^2\\). In order to do frequentist statistics, we need to assume that there is some sort of True value that the group we are measuring has and it is a fundamental property of the group! For more on this see CHAPTER 3 and the work of Zoltan Dienes. Somewhere, maybe written on a golden plate in heaven is the actual value of the average weight of a labrador retriever. The problem is we will never have access to that information so we need to estimate it by using a sample. The logic is that if we can truly draw in a random way from our entire population, in this case labrador retrievers, the central limit theroum will give us a good approximation of what that True value will be. Since we want to be clear about when we are talking about the Platonic, True value and the actual sample we collected, we use different Greek notation. The \\(\\sigma^2\\) refers to the Platonic value and the \\(\\sigma^2\\) is the sample. They are defined as follows: \\[\\sigma^2 = \\frac{\\Sigma(X_i - \\mu)^2}{N}\\] \\[s^2 = \\frac{\\Sigma(X_i - \\mu)^2}{n-1}\\] Note here that each of these formulas needs a mean. In the population equation that is defined as \\(\\mu\\) and in samples we used \\(\\bar{X}\\). In our case with the SAT scores, we are wanting to know the True value of the SAT scores of whatever population our six students are theorized to come from. To do the calulations below we need to know the mean which we calcualted above to be 616.67. Now since these scores are to serve as a represntive sample in hopes of getting at the true population value we need to use the formula reflecting the sample variance or \\(s^2\\). \\[s^2 = \\frac{(480-616.7)^2 + . . . + (760 - 616.676)^2}{6-1}\\] Doing this by hand we get an \\(s^2\\) value of 12346.67. Or running it in R, we would use. var(satData$SAT) ## [1] 12346.67 The standard deviation is the square root of the variance of the sample. \\[s = \\sqrt\\frac{\\Sigma(X_i - \\mu)^2}{n-1}\\] And since we know \\(s^2\\) from above, we can shorten this to \\[s = \\sqrt{s^2} = \\sqrt{12345.67} = 111.12\\] Or run it in R and get sd(satData$SAT) ## [1] 111.1156 Standard scores represent the distance of raw scores form their mean in standard deviation units. \\[z = \\frac{x_i - \\bar{X}}{s}\\] So if we needed to find the \\(z\\) score or standardized score for someone who got a 560 on their SAT we could compute the following. \\[z_{560} = \\frac{560 - 616.67}{111.12} = -0.51\\] Interpreted in-context, this would mean that if you scored a 560 on the SAT, based on our sample (which we think helps us get at the True popuation value), you would be scoring about less than 1 standard deviation (the unit of z) below the average. 5.1.5 Properties of z Scores Z scores are defined by having having three separte properties: The standardized distribution preserves the shape of the original raw score distribution The mean of the standardized distribution is always 0 The variance &amp; standard deviation are always 1 Many of the variables in behavioral sciences are distributed normally. In addition, the basis for parametric inferential statistics is based on the normal distribution. The normal distribution is unimodal, symmetrical, bell shaped, with a maximum height at the mean. The normal distribution is continuous and additionallythe normal distribution is asymptotic to the X axis—it never actually touches the X axis and theoretically goes on to infinity. The normal distribution is really a family of distributions defined by all possible combinations of means \\(\\mu\\) and standard deviations \\(\\sigma\\). We can use the standardized normal distribution to find areas of probability under the curve. With a normal distribution, there is always a fixed area under the curve which we take the reflect the probability of getting a score when sampling from a population. For example if we go 1 z unit (1 SD) away from the mean we find 34% of the total area of the curve there. If you then extend that out to the negative side, you then encapsulate 68% of the distribution. This would translate to a scenario where if you were to get a score at random from the distribution, 68% of the time you would get a score between 1 and -1 SD units from your mean. This process can be extended as seen in the figure below. z Scores and Areas Under the Curve We could also calculate the area between two z scores as shown here. And we could also look at how much area under the distribution exists beyond two standard deviations beyond the mean. z Scores and Areas Under the Curve Or we could pick any z score values and find the area under the mean! z Scores and Areas Under the Curve 5.2 Practice We can now start to put this to use. Here are some past homework examples. During tryouts, a sample of ballet dancers were rated on their athletic ability andoverall knowledge of the art. Below are the ratings for each dancer (a score above 75 percent means that the dancer will join the troupe ballet &lt;- c(83, 98, 45, 69, 52, 94, 82, 74, 71, 83, 62, 85, 90, 97, 61, 74, 74, 88) What is the median percentage? median(ballet) ## [1] 78 What is the mean percentage? mean(ballet) ## [1] 76.77778 What is the standard deviation for the sample (assume we don’t know any population characteristics)? sd(ballet) ## [1] 15.02373 Demonstrate the least squares principle by showing that the sum of squares (SS) around the mean is smaller than the sum of squares around the median (remember to show your work for each). ballet_mean &lt;- mean(ballet) ballet_median &lt;- median(ballet) ballet_sd &lt;- sd(ballet) Now we can use these values to do our math! sum((ballet - ballet_mean)^2) ## [1] 3837.111 sum((ballet - ballet_median)^2) ## [1] 3864 # Then having R do the final work for us sum((ballet - ballet_mean)^2) &gt; sum((ballet - ballet_median)^2) ## [1] FALSE What are the standardized (z) scores for the raw scores 73, 99, and 66? If you know the population mean and the sd, you can calulate a z score using the formula \\[z = \\frac{x_i - \\bar{X}}{s}\\] Or in our case (73 - ballet_mean)/ ballet_sd ## [1] -0.2514541 (99 - ballet_mean)/ ballet_sd ## [1] 1.479142 (66 - ballet_mean)/ ballet_sd ## [1] -0.7173837 What proportion of scores exceeds a raw score of 73? pnorm(q = 73, mean = ballet_mean, sd = ballet_sd) ## [1] 0.4007315 To get the other side of the probability we can remember that we can treat the line above as an object! 1 - pnorm(q = 73, mean = ballet_mean, sd = ballet_sd) ## [1] 0.5992685 What proportion of scores lies between the raw scores of 75 and 100? Let’s be clever for this one and just put the two equations together for this one. Or if you want, you could save them into objects. pnorm(q = 100, mean = ballet_mean, sd = ballet_sd) - pnorm(q = 75, mean = ballet_mean, sd = ballet_sd) ## [1] 0.4860093 pnorm(q = 76, mean = ballet_mean, sd = ballet_sd) ## [1] 0.479356 What raw score represents the 55thpercentile? To find out what raw score represents a percentile we can go back and use the formula from above, just rearranged a bit. \\[z = \\frac{x_i - \\bar{X}}{s}\\] or with a bit of basic algerbra \\[x_i = (z * s) + \\bar{X}\\] (.05 * ballet_sd) + ballet_mean ## [1] 77.52896 Between what raw scores does the middle 60% of the distribution lie? Lastly, we then need to find first what z scores map on 30% on either side of the disribution, then convert those z scores to raw scores on our data using the z score formula. First we find the z score associated with what is 30% left and right of the mean (it will be the same number, only negative). In this case, it is +/-.84. With that established, we then first solve for x \\[-0.84 = \\frac{x - 76.78}{15.02}\\] Giving us a value of 64.1 And we do it again with the positive number. \\[0.84 = \\frac{x - 76.78}{15.02}\\] Resulting in 89.547. "],
["data-transformations-and-confidence-intervals.html", "Chapter 6 Data Transformations and Confidence Intervals", " Chapter 6 Data Transformations and Confidence Intervals We have finished a nice book. "],
["power-and-effect-size-measures.html", "Chapter 7 Power and Effect Size Measures", " Chapter 7 Power and Effect Size Measures attempt here "],
["correlation-and-regression.html", "Chapter 8 Correlation and Regression", " Chapter 8 Correlation and Regression attempt here "],
["matched-t-test.html", "Chapter 9 Matched T Test", " Chapter 9 Matched T Test Template file "],
["one-way-anova.html", "Chapter 10 One Way ANOVA", " Chapter 10 One Way ANOVA Template file "],
["multiple-comparisons.html", "Chapter 11 Multiple Comparisons", " Chapter 11 Multiple Comparisons Template file "],
["simple-effects-interactions-mixed-designs.html", "Chapter 12 Simple Effects, Interactions, Mixed Designs", " Chapter 12 Simple Effects, Interactions, Mixed Designs Template file "],
["repeated-measures-anova.html", "Chapter 13 Repeated Measures ANOVA:", " Chapter 13 Repeated Measures ANOVA: Template file "],
["mixed-two-way-anova.html", "Chapter 14 Mixed Two Way ANOVA", " Chapter 14 Mixed Two Way ANOVA Template file "],
["multiple-regression.html", "Chapter 15 Multiple Regression", " Chapter 15 Multiple Regression Template file "],
["chi-square.html", "Chapter 16 Chi-Square", " Chapter 16 Chi-Square Template file "],
["non-parametric-data.html", "Chapter 17 Non-Parametric Data", " Chapter 17 Non-Parametric Data Template file "],
["advanced-data-cleaning.html", "Chapter 18 Advanced Data Cleaning", " Chapter 18 Advanced Data Cleaning Template file "],
["advanced-multiple-regression.html", "Chapter 19 Advanced Multiple Regression", " Chapter 19 Advanced Multiple Regression Template file "],
["logistic-regression.html", "Chapter 20 Logistic Regression", " Chapter 20 Logistic Regression Template file "],
["mediation-and-moderation.html", "Chapter 21 Mediation and Moderation", " Chapter 21 Mediation and Moderation Template file "],
["ancova.html", "Chapter 22 ANCOVA 22.1 Theory 22.2 Practice", " Chapter 22 ANCOVA 22.1 Theory 22.1.1 Reducing Noise Just like the ANOVA (Analysis of Variance), the ANCOVA (Analysis of Covariance) is used to analyze experiments by calculating an F ratio for more than 2 groups. As with any F calculation, differences in the dependent variable are caused by two things: The independent variable (signal, systematic variation) Error (noise, unsysematic variation) The F statistic captures this \\[ F = \\frac{Variation\\ Due\\ to\\ IV}{Variation\\ Due\\ to\\ Error} \\] When there is small variation within groups and large variation between groups, we get a large F. CHART HERE When there is large variatin within groups and small variation between groups, we get a small F. The idea with the ANCOVA is that you can reduce your error term (the denominator from above) by choosing a covariate (CV) that is related to the DV and will soak up some random variation in your F test to give you a clearer picture of what is going on. Usually this means controlling for some sort of variable. The classic example is if you wanted to give some kids a test of some mental ability in a between subjects design but you don’t want their age to skew your results. You might have had a control condition, an intervention, and some sort of alternate intervention. Typically you would run an ANOVA on the three groups, but since you know age has been accounted for and you want to remove that effect from the model, you enter age as a covariate in this calculation. Outside of this there are three major applications for ANCOVA. Increase test sensitivity by using the CV(s) to account for more of the error variance Adjust DV scores to what they would be if everyone scored the same on the CV(s) Adjustment of a DV for other DVs taken as CVs Note that use of a CV can adjust DV scores and show a larger effect or the CV can even eliminate the effect. Looking at this another way: Reduces random error by increasing the size of F Reduces systematic error by adjusting for differences in means May increase differences by soaking up error It’s a good idea to use ANCOVA when you are removing variance in the DV related to covariate, but not related to the grouping variable. This decreases the error term and increases power. It’sa bad idea to use ANCOVA when groups differ on their mean level of the covariate. Usually here the covariate and the grouping variable are not independent. An example of this might be when “controlling” for anxiety when studying people with and without depression. Clearly people with depression will have higher levels of anxiety than their controls by nature of having depression! 22.1.2 Assumptions of ANOVA All ANOVA assumptions apply (errors are RIND) Random Independent Normally distributed The Covariate z is unrelated to x, and that z is related to, and in a sense, acts as a suppressor of y The covariate has a linear relationship with the dependent variable Variance of groups are equal. Correlation between y and z is equal for all levels of x (Homogeneity of Regression) You test for homogeneity of regression slopes by including a covariate by group interaction. If it’s significant, you violated an assumption and can’t to ANCOVA! CHART CHART 22.1.3 Best Practice Things you should control for: 1. Broad Descriptors (Age, Social Class) 2. General Ability (intellgience, memory, speed of movement) 3. Personality measures (extraversion, neuroticism) 4. Things you think are relevant 5. Pick CVs that are correlated with DV * Height good for basketball * Height poor for test of math skills 6. CV should not be influenced by treatment 22.2 Practice In order to get our hands dirty with ANCOVA we’re going to look at a dataset where a bunch of sheep were slaughtered. We have three types of sheep (ewe, wether, ram) and a measure of fatness and weight. Let’s first run an ANOVA to see if fatness differs between animal type. library(data.table) ## Warning: package &#39;data.table&#39; was built under R version 3.4.2 deadsheep &lt;- fread(&quot;datasets/deadsheep.csv&quot;) deadsheep[, Animal := as.factor(animal)] sheep.model.1 &lt;- aov(fatness ~ Animal, data=deadsheep) summary(sheep.model.1) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Animal 2 124.4 62.21 3.165 0.0566 . ## Residuals 30 589.6 19.65 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Looking at the output, we note that There was no significant effect of fatness on levels of animal, \\[F(2,30)=3.165, p &gt; .05\\]. Now since we know that the whole weight of the animal might be confounding our answer (it’s related to the DV of fatness, but not nessecarily related to the type of sheep), let’s run an ANCOVA with carcass weight as a covariate and see if that changes our answer. Before we run the ANCOVA, we need to first check that our IV is not significantly related to our CV. We do this by running an ANOVA predicting weigth by animal. #Test IND of IV and CV ind.sheep &lt;- aov(weight ~ Animal, data=deadsheep) ## Check CV for IV indep, want non-sig summary(ind.sheep) # p=.104 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Animal 2 14.20 7.102 2.438 0.104 ## Residuals 30 87.41 2.914 Here we get\\[ F(2,30) = 2.44, p&gt; .05\\] and can move forward. Let’s now run our ANCOVA. Note that R’s default is Type I Sum of Squares, we need to load the car package to give us the correct sum of squares. For further reading on this, check out the Andy Field book on this. sheepANCOVA &lt;- aov(fatness ~ Animal + weight, data=deadsheep) library(car) Anova(sheepANCOVA, type=&quot;III&quot;) ## Anova Table (Type III tests) ## ## Response: fatness ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 117.17 1 26.529 1.669e-05 *** ## Animal 332.31 2 37.621 8.772e-09 *** ## weight 461.56 1 104.506 3.994e-11 *** ## Residuals 128.08 29 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can then report The covariate, weight, was not significantly related to the the fatness of the animal, \\[F(1, 29) =,p=.0565 \\] and that there was a significant effect of the animal after controlling for weight of the animal, \\[F(2,28) = 37.62, p &lt; .05.\\]. Let’s now check on our marginal means. These are our group means after we have controlled for our covariate. For this we need the effects package. library(effects) ## Loading required package: carData ## ## Attaching package: &#39;carData&#39; ## The following objects are masked from &#39;package:car&#39;: ## ## Guyer, UN, Vocab ## lattice theme set by effectsTheme() ## See ?effectsTheme for details. mean.sheep &lt;- effect(&quot;Animal&quot;, sheepANCOVA, se=TRUE) summary(mean.sheep) ## ## Animal effect ## Animal ## Ewe Ram Wether ## 18.90219 10.53996 14.83058 ## ## Lower 95 Percent Confidence Limits ## Animal ## Ewe Ram Wether ## 17.565871 9.183322 13.532447 ## ## Upper 95 Percent Confidence Limits ## Animal ## Ewe Ram Wether ## 20.23851 11.89660 16.12870 We can then note the estimated marginal means here (18.9 for Ewe, 10.53 for Ram, and 14.83 for Wether) are mean values for fatness once the CV of weight has been controlled for. Lastly, let’s test the homogenaity of the regression slopes assumption (though we should have done this first!). We do this by running the model with an interaction term included. hoRS.sheep &lt;- aov(fatness ~ weight + Animal + animal:weight, data=deadsheep) Anova(hoRS.sheep, type=&quot;III&quot;) ## Anova Table (Type III tests) ## ## Response: fatness ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 64.571 1 14.6355 0.0007007 *** ## weight 180.144 1 40.8308 7.624e-07 *** ## Animal 5.293 2 0.5998 0.5560672 ## weight:animal 8.957 2 1.0151 0.3757923 ## Residuals 119.123 27 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The interaction term is not significant, thus no violation. We can also plot this to see it. All regression lines should be pointing in the same direction when we plot the groups separately. library(ggplot2) ggplot(deadsheep, aes(x = weight, y = fatness, color = Animal)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + labs(title = &quot;Homogeneity of Regression Slopes&quot;, x = &quot;Weight&quot;, y = &quot;Fatness&quot;) If one of these lines would be pointed downwards, we would have an interaction. "],
["manova.html", "Chapter 23 MANOVA", " Chapter 23 MANOVA Template file "],
["repeated-measures-anova-1.html", "Chapter 24 Repeated Measures ANOVA", " Chapter 24 Repeated Measures ANOVA Template file "],
["factor-analysis.html", "Chapter 25 Factor Analysis", " Chapter 25 Factor Analysis Template file "],
["mixed-effects-models.html", "Chapter 26 Mixed Effects Models", " Chapter 26 Mixed Effects Models Template file "],
["confirmatory-factor-analysis.html", "Chapter 27 Confirmatory Factor Analysis", " Chapter 27 Confirmatory Factor Analysis Template file "],
["this-is-a-template-file.html", "Chapter 28 This is a template file", " Chapter 28 This is a template file Template file "],
["references.html", "References", " References "]
]
