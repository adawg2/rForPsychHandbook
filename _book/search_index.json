[
["index.html", "R for Psych Handbook Chapter 1 Preface", " R for Psych Handbook David John Baker 2018-03-27 Chapter 1 Preface This book serves as a collection of resources used in the LSU psychological statistics courses. It contains some lecture notes, as well as R code and data to run each of the examples in R. The book is still under construction and if you would like to help out with it, please get in touch! The majority of the content comes from having taken and then TA’s for the LSU psychology department’s stats courses. Was given oppertunity to work for psych department as music student, partially because of R, and writing this handbook was way of saying thank you for the great experience that I had. Content was generated by both Jason Hicks and Jason Harman, I just provided R as the glue to hold it all together. Idea is that anyone going through the LSU stats rotation would be able to if they wanted put in the extra work and learn R while they are doing the assigments in SPSS. Data and content are from previous years, form same, content different. As noted in next chapter, it’s hard to learn R, but worth doing it. Personally found that R has opened up more oppertunities than anything else I have invested in. See it as investing in self. Got started with it because of guy who advised my Masters thesis. If you feel like you can’t do this, know that reason I started R is I knew so little about computers, Daniel had to show me how to do an IFELSE() statement in Excel. At that point figured if I was going to learn, might as well be frustrated and get a better product out of it. One thing to learn a lot by running these examples, though the best way forward is to find your own dataset and start to set up project with it. Beauty of R is that you are trying to become a researcher. R is cullinary school of statistics. SPSS is a microwave. Saw that on twitter. Let’s get cooking. "],
["intro.html", "Chapter 2 Introduction to R 2.1 Getting Started 2.2 Setting The Working Directory 2.3 Rationale 2.4 R as Calculator 2.5 Data Exploration 2.6 Indexing 2.7 Whirlwind Tour of R 2.8 Functions for Psychologists 2.9 Resources", " Chapter 2 Introduction to R Before starting out, it’s worth mentioning that R has a steep learning curve compared to other statistical softwares. While there are tons of blog posts as to why you should learn R, I will keep my list quick so if you get discouraged at any point, you can come back to this list and get reinspired before R starts paying you back. The R community is fantastic, check out #rstats on Twitter (this is really my favorite reason) R will always be free because the people behind it believe in open source principles LINK. Time spent learning R is time spent learning how computers work, if you learn about R, you are also learning computer programming. Time spent in something like SPSS or SAS does not easily tranfer to other programs. On r-jobs.com the way they decide to split jobs is jobs that make above and below $100,000. R is your ticket out of academia, if you need it. 2.1 Getting Started Since this book is about statistics and R, the introduction to all things R is a bit shorter than other guides. If you need help, find a friend (or email the author of this book) and they will get you started. The first things you need to do is download R from CRAN, then get the latest version of RStudio. RStudio is an IDE that makes working in R a lot easier. Do not use R without it. Once you have that, you can start to play around with this. 2.2 Setting The Working Directory At the end a long writing session you normally have to find a place to save that journal article you are working on. You do this by clicking ‘Save As’ then finding where to stick it and what to call it. If you are programming you need to do this right away by saving your script (the what) and setting your working directory (the where). Once you have a new script set up, now we can start to run the next commands. This chapter covers a few basic things The rationale behind to writing scripts and code R as Calculator Data Structures Manipulating Data Tons of functions Whirlwind tour of R for psychologists The goals of this chapter are to: Learn Basics of R and RStudio Practice concepts with simple examples Familiarize with vast array of functions Have vocabulary and “R” way to think about problem solving 2.3 Rationale Easily reproducible Less Human Error in cleaning Time invested in programming transfers Pretty Graphs Open Science Huge integration 2.4 R as Calculator The Console of R is where all the action happens. You can use it just like you would use a calculator. Try to do some basic math operations in it. 2 + 2 ## [1] 4 5 - 2 ## [1] 3 10 / 4 ## [1] 2.5 9 * 200 ## [1] 1800 sqrt(81) ## [1] 9 10 &gt; 4 ## [1] TRUE 2 &lt; -1 ## [1] FALSE You don’t always want to print your output and retype it in. Idea is to be very lazy (efficient). Save some math to an object with the &lt;- operator, then manipulate that. foo &lt;- 2 * 3 foo * 6 ## [1] 36 Notice what has popped up in your environment in RStudio! Let’s get more efficient. yearsInGradSchool &lt;- c(2,1,4,5,6,7,3,2,4,5,3) talk about the c function. yearsInGradSchool * 3 ## [1] 6 3 12 15 18 21 9 6 12 15 9 Or this yearsInGradSchool - 2 ## [1] 0 -1 2 3 4 5 1 0 2 3 1 yearsInGradSchool &lt; 2 ## [1] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE mean(yearsInGradSchool) ## [1] 3.818182 sd(yearsInGradSchool) ## [1] 1.834022 hist(yearsInGradSchool) scale(yearsInGradSchool) ## [,1] ## [1,] -0.99136319 ## [2,] -1.53661295 ## [3,] 0.09913632 ## [4,] 0.64438608 ## [5,] 1.18963583 ## [6,] 1.73488559 ## [7,] -0.44611344 ## [8,] -0.99136319 ## [9,] 0.09913632 ## [10,] 0.64438608 ## [11,] -0.44611344 ## attr(,&quot;scaled:center&quot;) ## [1] 3.818182 ## attr(,&quot;scaled:scale&quot;) ## [1] 1.834022 range(yearsInGradSchool) ## [1] 1 7 min(yearsInGradSchool) ## [1] 1 class(yearsInGradSchool) ## [1] &quot;numeric&quot; str(yearsInGradSchool) ## num [1:11] 2 1 4 5 6 7 3 2 4 5 ... summary(yearsInGradSchool) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 2.500 4.000 3.818 5.000 7.000 yearsInGradSchool &lt;- c(2,1,4,5,6,7,3,2,4,5,3) classesTaken &lt;- c(5,2,5,7,9,9,2,8,4,7,2) gradData &lt;- data.frame(yearsInGradSchool,classesTaken) cor(yearsInGradSchool,classesTaken) ## [1] 0.6763509 Basic plots, arguments. ggplot and libraries plot(yearsInGradSchool,classesTaken, data = gradData, main = &quot;My Plot&quot;, xlab = &quot;Years in Grad School&quot;, ylab = &quot;Classes Taken&quot;) ## Warning in plot.window(...): &quot;data&quot; is not a graphical parameter ## Warning in plot.xy(xy, type, ...): &quot;data&quot; is not a graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): &quot;data&quot; is not ## a graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): &quot;data&quot; is not ## a graphical parameter ## Warning in box(...): &quot;data&quot; is not a graphical parameter ## Warning in title(...): &quot;data&quot; is not a graphical parameter 2.5 Data Exploration str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... class(iris) ## [1] &quot;data.frame&quot; summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## Accessing individual ‘columns’ is done with the $ operator iris$Sepal.Length ## [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 ## [18] 5.1 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 ## [35] 4.9 5.0 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 ## [52] 6.4 6.9 5.5 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 ## [69] 6.2 5.6 5.9 6.1 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 ## [86] 6.0 6.7 6.3 5.6 5.5 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 ## [103] 7.1 6.3 6.5 7.6 4.9 7.3 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 ## [120] 6.0 6.9 5.6 7.7 6.3 6.7 7.2 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 ## [137] 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8 6.7 6.7 6.3 6.5 6.2 5.9 Can you use this to plot the different numeric values against each other? What would the follow commands do? hist(scale(iris$Sepal.Length)) iris$Sepal.Length.scale &lt;- scale(iris$Sepal.Length) 2.6 Indexing Let’s combine logical indexing with creating new objects. What do the follow commands do? Why? iris[1,1] ## [1] 5.1 iris[2,] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 2 4.9 3 1.4 0.2 setosa ## Sepal.Length.scale ## 2 -1.1392 iris[,5] ## [1] setosa setosa setosa setosa setosa setosa ## [7] setosa setosa setosa setosa setosa setosa ## [13] setosa setosa setosa setosa setosa setosa ## [19] setosa setosa setosa setosa setosa setosa ## [25] setosa setosa setosa setosa setosa setosa ## [31] setosa setosa setosa setosa setosa setosa ## [37] setosa setosa setosa setosa setosa setosa ## [43] setosa setosa setosa setosa setosa setosa ## [49] setosa setosa versicolor versicolor versicolor versicolor ## [55] versicolor versicolor versicolor versicolor versicolor versicolor ## [61] versicolor versicolor versicolor versicolor versicolor versicolor ## [67] versicolor versicolor versicolor versicolor versicolor versicolor ## [73] versicolor versicolor versicolor versicolor versicolor versicolor ## [79] versicolor versicolor versicolor versicolor versicolor versicolor ## [85] versicolor versicolor versicolor versicolor versicolor versicolor ## [91] versicolor versicolor versicolor versicolor versicolor versicolor ## [97] versicolor versicolor versicolor versicolor virginica virginica ## [103] virginica virginica virginica virginica virginica virginica ## [109] virginica virginica virginica virginica virginica virginica ## [115] virginica virginica virginica virginica virginica virginica ## [121] virginica virginica virginica virginica virginica virginica ## [127] virginica virginica virginica virginica virginica virginica ## [133] virginica virginica virginica virginica virginica virginica ## [139] virginica virginica virginica virginica virginica virginica ## [145] virginica virginica virginica virginica virginica virginica ## Levels: setosa versicolor virginica iris[iris$Sepal.Length &lt; 5,] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## 12 4.8 3.4 1.6 0.2 setosa ## 13 4.8 3.0 1.4 0.1 setosa ## 14 4.3 3.0 1.1 0.1 setosa ## 23 4.6 3.6 1.0 0.2 setosa ## 25 4.8 3.4 1.9 0.2 setosa ## 30 4.7 3.2 1.6 0.2 setosa ## 31 4.8 3.1 1.6 0.2 setosa ## 35 4.9 3.1 1.5 0.2 setosa ## 38 4.9 3.6 1.4 0.1 setosa ## 39 4.4 3.0 1.3 0.2 setosa ## 42 4.5 2.3 1.3 0.3 setosa ## 43 4.4 3.2 1.3 0.2 setosa ## 46 4.8 3.0 1.4 0.3 setosa ## 48 4.6 3.2 1.4 0.2 setosa ## 58 4.9 2.4 3.3 1.0 versicolor ## 107 4.9 2.5 4.5 1.7 virginica ## Sepal.Length.scale ## 2 -1.139200 ## 3 -1.380727 ## 4 -1.501490 ## 7 -1.501490 ## 9 -1.743017 ## 10 -1.139200 ## 12 -1.259964 ## 13 -1.259964 ## 14 -1.863780 ## 23 -1.501490 ## 25 -1.259964 ## 30 -1.380727 ## 31 -1.259964 ## 35 -1.139200 ## 38 -1.139200 ## 39 -1.743017 ## 42 -1.622254 ## 43 -1.743017 ## 46 -1.259964 ## 48 -1.501490 ## 58 -1.139200 ## 107 -1.139200 iris[,c(1:4)] ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3.0 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5.0 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## 11 5.4 3.7 1.5 0.2 ## 12 4.8 3.4 1.6 0.2 ## 13 4.8 3.0 1.4 0.1 ## 14 4.3 3.0 1.1 0.1 ## 15 5.8 4.0 1.2 0.2 ## 16 5.7 4.4 1.5 0.4 ## 17 5.4 3.9 1.3 0.4 ## 18 5.1 3.5 1.4 0.3 ## 19 5.7 3.8 1.7 0.3 ## 20 5.1 3.8 1.5 0.3 ## 21 5.4 3.4 1.7 0.2 ## 22 5.1 3.7 1.5 0.4 ## 23 4.6 3.6 1.0 0.2 ## 24 5.1 3.3 1.7 0.5 ## 25 4.8 3.4 1.9 0.2 ## 26 5.0 3.0 1.6 0.2 ## 27 5.0 3.4 1.6 0.4 ## 28 5.2 3.5 1.5 0.2 ## 29 5.2 3.4 1.4 0.2 ## 30 4.7 3.2 1.6 0.2 ## 31 4.8 3.1 1.6 0.2 ## 32 5.4 3.4 1.5 0.4 ## 33 5.2 4.1 1.5 0.1 ## 34 5.5 4.2 1.4 0.2 ## 35 4.9 3.1 1.5 0.2 ## 36 5.0 3.2 1.2 0.2 ## 37 5.5 3.5 1.3 0.2 ## 38 4.9 3.6 1.4 0.1 ## 39 4.4 3.0 1.3 0.2 ## 40 5.1 3.4 1.5 0.2 ## 41 5.0 3.5 1.3 0.3 ## 42 4.5 2.3 1.3 0.3 ## 43 4.4 3.2 1.3 0.2 ## 44 5.0 3.5 1.6 0.6 ## 45 5.1 3.8 1.9 0.4 ## 46 4.8 3.0 1.4 0.3 ## 47 5.1 3.8 1.6 0.2 ## 48 4.6 3.2 1.4 0.2 ## 49 5.3 3.7 1.5 0.2 ## 50 5.0 3.3 1.4 0.2 ## 51 7.0 3.2 4.7 1.4 ## 52 6.4 3.2 4.5 1.5 ## 53 6.9 3.1 4.9 1.5 ## 54 5.5 2.3 4.0 1.3 ## 55 6.5 2.8 4.6 1.5 ## 56 5.7 2.8 4.5 1.3 ## 57 6.3 3.3 4.7 1.6 ## 58 4.9 2.4 3.3 1.0 ## 59 6.6 2.9 4.6 1.3 ## 60 5.2 2.7 3.9 1.4 ## 61 5.0 2.0 3.5 1.0 ## 62 5.9 3.0 4.2 1.5 ## 63 6.0 2.2 4.0 1.0 ## 64 6.1 2.9 4.7 1.4 ## 65 5.6 2.9 3.6 1.3 ## 66 6.7 3.1 4.4 1.4 ## 67 5.6 3.0 4.5 1.5 ## 68 5.8 2.7 4.1 1.0 ## 69 6.2 2.2 4.5 1.5 ## 70 5.6 2.5 3.9 1.1 ## 71 5.9 3.2 4.8 1.8 ## 72 6.1 2.8 4.0 1.3 ## 73 6.3 2.5 4.9 1.5 ## 74 6.1 2.8 4.7 1.2 ## 75 6.4 2.9 4.3 1.3 ## 76 6.6 3.0 4.4 1.4 ## 77 6.8 2.8 4.8 1.4 ## 78 6.7 3.0 5.0 1.7 ## 79 6.0 2.9 4.5 1.5 ## 80 5.7 2.6 3.5 1.0 ## 81 5.5 2.4 3.8 1.1 ## 82 5.5 2.4 3.7 1.0 ## 83 5.8 2.7 3.9 1.2 ## 84 6.0 2.7 5.1 1.6 ## 85 5.4 3.0 4.5 1.5 ## 86 6.0 3.4 4.5 1.6 ## 87 6.7 3.1 4.7 1.5 ## 88 6.3 2.3 4.4 1.3 ## 89 5.6 3.0 4.1 1.3 ## 90 5.5 2.5 4.0 1.3 ## 91 5.5 2.6 4.4 1.2 ## 92 6.1 3.0 4.6 1.4 ## 93 5.8 2.6 4.0 1.2 ## 94 5.0 2.3 3.3 1.0 ## 95 5.6 2.7 4.2 1.3 ## 96 5.7 3.0 4.2 1.2 ## 97 5.7 2.9 4.2 1.3 ## 98 6.2 2.9 4.3 1.3 ## 99 5.1 2.5 3.0 1.1 ## 100 5.7 2.8 4.1 1.3 ## 101 6.3 3.3 6.0 2.5 ## 102 5.8 2.7 5.1 1.9 ## 103 7.1 3.0 5.9 2.1 ## 104 6.3 2.9 5.6 1.8 ## 105 6.5 3.0 5.8 2.2 ## 106 7.6 3.0 6.6 2.1 ## 107 4.9 2.5 4.5 1.7 ## 108 7.3 2.9 6.3 1.8 ## 109 6.7 2.5 5.8 1.8 ## 110 7.2 3.6 6.1 2.5 ## 111 6.5 3.2 5.1 2.0 ## 112 6.4 2.7 5.3 1.9 ## 113 6.8 3.0 5.5 2.1 ## 114 5.7 2.5 5.0 2.0 ## 115 5.8 2.8 5.1 2.4 ## 116 6.4 3.2 5.3 2.3 ## 117 6.5 3.0 5.5 1.8 ## 118 7.7 3.8 6.7 2.2 ## 119 7.7 2.6 6.9 2.3 ## 120 6.0 2.2 5.0 1.5 ## 121 6.9 3.2 5.7 2.3 ## 122 5.6 2.8 4.9 2.0 ## 123 7.7 2.8 6.7 2.0 ## 124 6.3 2.7 4.9 1.8 ## 125 6.7 3.3 5.7 2.1 ## 126 7.2 3.2 6.0 1.8 ## 127 6.2 2.8 4.8 1.8 ## 128 6.1 3.0 4.9 1.8 ## 129 6.4 2.8 5.6 2.1 ## 130 7.2 3.0 5.8 1.6 ## 131 7.4 2.8 6.1 1.9 ## 132 7.9 3.8 6.4 2.0 ## 133 6.4 2.8 5.6 2.2 ## 134 6.3 2.8 5.1 1.5 ## 135 6.1 2.6 5.6 1.4 ## 136 7.7 3.0 6.1 2.3 ## 137 6.3 3.4 5.6 2.4 ## 138 6.4 3.1 5.5 1.8 ## 139 6.0 3.0 4.8 1.8 ## 140 6.9 3.1 5.4 2.1 ## 141 6.7 3.1 5.6 2.4 ## 142 6.9 3.1 5.1 2.3 ## 143 5.8 2.7 5.1 1.9 ## 144 6.8 3.2 5.9 2.3 ## 145 6.7 3.3 5.7 2.5 ## 146 6.7 3.0 5.2 2.3 ## 147 6.3 2.5 5.0 1.9 ## 148 6.5 3.0 5.2 2.0 ## 149 6.2 3.4 5.4 2.3 ## 150 5.9 3.0 5.1 1.8 iris[c(1,2,3,4,5,6,8),c(1:3,5)] ## Sepal.Length Sepal.Width Petal.Length Species ## 1 5.1 3.5 1.4 setosa ## 2 4.9 3.0 1.4 setosa ## 3 4.7 3.2 1.3 setosa ## 4 4.6 3.1 1.5 setosa ## 5 5.0 3.6 1.4 setosa ## 6 5.4 3.9 1.7 setosa ## 8 5.0 3.4 1.5 setosa Setosas &lt;- iris[iris$Species == &quot;setosa&quot;,] This could be an entire lecture by itself!!! 2.7 Whirlwind Tour of R R’s power comes in the fact that you download packages to put on top of base R Turning Data tables into already formatted APA Latex Tables (stargazer, xtable) Creating Publication Quality Graphs (ggplot2) Text manipulation (stringr) Exploring data and not making chart after chart after chart (psych) Every statistical test you could want (psych, cars, ezanova, lavaan) Software to plot not so normal output (SEMplots) Making Websites in R These slides were written in R Quickly processing huge datasets (data.table, dplyr) Tons of Machine Learning library(ggplot2) ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species), xlab = &quot;Sepal Length&quot;, ylab = &quot;Sepal Width&quot;, main = &quot;My Plot&quot;) + geom_point() Or stuff for data exploration library(psych) ## ## Attaching package: &#39;psych&#39; ## The following objects are masked from &#39;package:ggplot2&#39;: ## ## %+%, alpha pairs.panels(iris) 2.8 Functions for Psychologists nlme() and lme4() for Multilevel Modeling lavaan() for Latent Variable Analysis ezAnova() for ANOVA based testing; the anova() function does model comparisons profileR for Repeated Measures MANOVA glm() and lm() for linear models caret() for Machine Learning 2.9 Resources LINK THESE IN swirl stackoverflow.com Twitter Your peers R Community is fantastic (tidyverse!!!) 2.9.1 Template Stuff You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species Sepal.Length.scale 5.1 3.5 1.4 0.2 setosa -0.89767388 4.9 3.0 1.4 0.2 setosa -1.13920048 4.7 3.2 1.3 0.2 setosa -1.38072709 4.6 3.1 1.5 0.2 setosa -1.50149039 5.0 3.6 1.4 0.2 setosa -1.01843718 5.4 3.9 1.7 0.4 setosa -0.53538397 4.6 3.4 1.4 0.3 setosa -1.50149039 5.0 3.4 1.5 0.2 setosa -1.01843718 4.4 2.9 1.4 0.2 setosa -1.74301699 4.9 3.1 1.5 0.1 setosa -1.13920048 5.4 3.7 1.5 0.2 setosa -0.53538397 4.8 3.4 1.6 0.2 setosa -1.25996379 4.8 3.0 1.4 0.1 setosa -1.25996379 4.3 3.0 1.1 0.1 setosa -1.86378030 5.8 4.0 1.2 0.2 setosa -0.05233076 5.7 4.4 1.5 0.4 setosa -0.17309407 5.4 3.9 1.3 0.4 setosa -0.53538397 5.1 3.5 1.4 0.3 setosa -0.89767388 5.7 3.8 1.7 0.3 setosa -0.17309407 5.1 3.8 1.5 0.3 setosa -0.89767388 You can write citations, too. For example, we are using the bookdown package (Xie 2017) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["data-manipulation-in-r.html", "Chapter 3 Data Manipulation in R", " Chapter 3 Data Manipulation in R # #====================================================================================================== # # LSUserRs Data Cleaning Template (Title Your Script) # # Say a couple of the things that it does here. Who wrote it? # # When was the last edit? What does it do? Does it work with any data type? # # Rubber duck this as much as possible because you won&#39;t remember # # what you did in 6 months. Especially if you come up with something clever. # #====================================================================================================== # # TRY YOUR BEST TO NOT JUST COPY AND PASTE CODE, GOOGLE WHAT YOU WANT, GET FAMILIAR WITH A FUNCTION&#39;S # # ARGUMENTS AND EMBRACE YOUR INNER NERD AND READ THE DOCUMENTATION!! # #====================================================================================================== # # Import Libraries # #-------------------------------------------------- # # Load all libraries at the start of your script, remember they have to be installed first! # library(stringr) # library(data.table) # library(psych) # # #====================================================================================================== # # Set up your working directory # #-------------------------------------------------- # # #====================================================================================================== # # Load in your data # #-------------------------------------------------- # # After telling R where to look in your computer/dropbox/google drive/R Project grab what you need! # # Make sure to load in both datasets as we will want them both in our analysis. # # We also want to make sure to clean both datasets as we are going. # experiment.data &lt;- read.csv(&quot;datasets/Demographic_Data.csv&quot;) # item.level.data &lt;- read.csv(&quot;datasets/ItemLevelData.csv&quot;) # # # #====================================================================================================== # # Inspect the structure of your data # #-------------------------------------------------- # # R is going to do its best to guess what kind of data each of the columns of your spreadsheet are. # # Sometimes it thinks things are lists (especially if importing from SPSS) # # Go through each variable as you would with other programs and make sure you set it to what # # you think will be most useful later. The big thing to check here is categorical variables, and # # if you see any sort of string/character variables. # # # View(experiment.data) # Looks OK on surface levels.... # str(experiment.data) # Check to see if R guessed correctly on data types # # # Using read.csv() R had a couple of bad guesses on variables we might need. # # We will have to reassign the variable types or else we&#39;ll run into trouble later. # # In R we use Factor for grouping and analysis, best practice is to not set it as that # # until you are OK with the format. It&#39;s easist to manipulate a character or string. # # experiment.data$inst &lt;- as.character(experiment.data$inst) # experiment.data$Gender &lt;- as.character(experiment.data$Gender) # experiment.data$Major &lt;- as.character(experiment.data$Major) # experiment.data$Minor &lt;- as.character(experiment.data$Minor) # experiment.data$BeginTrain &lt;- as.character(experiment.data$BeginTrain) # experiment.data$AbsPitch &lt;- as.character(experiment.data$AbsPitch) # experiment.data$Live12Mo &lt;- as.numeric(experiment.data$Live12Mo) # experiment.data$ActListenMin &lt;- as.character(experiment.data$ActListenMin) # # str(experiment.data) # Notice how that our character columns now have &quot; &quot; around them. # # # #====================================================================================================== # # Check for Import Errors # #-------------------------------------------------- # # Use a combination of the names(), View(), table(), is.na(), and complete.cases() to get a brief summary of what is # # going on in your data set to be sure there were minimal import errors and your data looks like # # you want it to. It might also be worth plottting some variables and use some common sense to find mistakes. # # Are there any participants with 999 as their subject number? Negative values where there shouldn&#39;t be? # # If there are, note them and fix these before starting any sort of statistical screening! # # table(complete.cases(experiment.data)) # Not all observations have everything! # complete.cases(experiment.data) # table(is.na(experiment.data)) # # # Gotta decide what to do about it!! # # #====================================================================================================== # # Cleaning Free Text Response Data # #-------------------------------------------------- # # In your data cleaning before you might have noticed that participants were able to freely respond # # with whatever gender they wanted. Most data look to fall within the normal binary, but the computer # # needs things to be exactly the same before making an easy split? # # What would be the laziest, most effecient way to fix the gender column? What format does the variable # # have to be in order to make the changes that you need? # # When you have it figured out, make sure to run the code from top to bottom to make sure things go in # # the right order!!! As we are not dealing with huge amounts of data, the table() function will help out. # # # Let&#39;s now take a look at some of these problem ones # # Why, for example is Begin Training not working? Print the variable to see. # # experiment.data$BeginTrain # # # Some people didn&#39;t respond, one person decided to tell us what grade they started. # # There are 2 ways to go about fixing this. We could &quot;hardcode&quot; the problem if this # # is the only time we will do this analysis on this dataset or we could try to write a # # line of code that doesn&#39;t care what exact position the error is. # # On line 250 in this object is the thing that needs swapping out. # # We can access it with R&#39;s indexing. Counting from index we see it&#39;s in line 250. # # experiment.data$BeginTrain[250] # # # Quick, ask yourself why we don&#39;t use the comma here?! # # If you were set on using the comma, what would you change? # # Ok, now let&#39;s swap in the value we want with &lt;- # # Remember we are putting a value into a character operator so it has to have &quot;&quot; # # experiment.data$BeginTrain[250] &lt;- &quot;12&quot; # experiment.data$BeginTrain # # # Nice, no more text data, but what if it&#39;s not always in 250? # # For example, what do we do with all these blank spaces? # # Let&#39;s use R&#39;s inbuilt ifelse() function to go through this vector and swap # # out what we want! # # ifelse(experiment.data$BeginTrain == &quot;&quot;,&quot;0&quot;,experiment.data$BeginTrain) # # # This works by going through each entry and doing the conditional on the value! # # Let&#39;s now write over our old column and in the same step make everything a number. # experiment.data$BeginTrain &lt;- as.numeric(ifelse(experiment.data$BeginTrain == &quot;&quot;,&quot;0&quot;,experiment.data$BeginTrain)) # # #Tah Dah!! # experiment.data$BeginTrain # # # Let&#39;s now clean up the Gender column, first let&#39;s look at it # experiment.data$Gender # Are there any common trends? # # table(experiment.data$Gender) # # # Pretty much two answers, how do we make them all say one thing? # # Let&#39;s use the stringr package for this. Import it up top. # # # Clean Gender # # experiment.data$Gender &lt;- str_to_lower(experiment.data$Gender) # experiment.data$Gender &lt;- str_replace(experiment.data$Gender,&quot;^.*f.*$&quot;,&quot;Female&quot;) # experiment.data$Gender &lt;- str_replace(experiment.data$Gender,&quot;^m.*$&quot;,&quot;Male&quot;) # experiment.data$Gender &lt;- str_replace(experiment.data$Gender,&quot;^country$&quot;,&quot;No Response&quot;) # experiment.data$Gender &lt;- str_replace(experiment.data$Gender,&quot;&quot;,&quot;No Response&quot;) # experiment.data$Gender[30] &lt;- &quot;No Response&quot; #Something Might Be Up w this datapoint? # # experiment.data$Gender &lt;- as.factor(experiment.data$Gender) # # #-------------------------------------------------- # # Can we do same thing for AP? # experiment.data$AbsPitch # experiment.data$AbsPitch &lt;- str_to_lower(experiment.data$AbsPitch) # experiment.data$AbsPitch &lt;- str_replace(experiment.data$AbsPitch,&quot;^.*n.*$&quot;,&quot;no&quot;) # experiment.data$AbsPitch[30] &lt;- &quot;no&quot; # experiment.data$AbsPitch &lt;- as.factor(experiment.data$AbsPitch) # table(experiment.data$AbsPitch) # # # #====================================================================================================== # # Merging Data # #-------------------------------------------------- # # Often we will have data from other spreadsheets we want to attach such as demographic data # # to behavioral responses. Using the data.table functionality, let&#39;s merge our two csv # # files together so that we have every variable accessible to us for this analysis. # # Note I like to work with the data.table package, though there are other ways to do this! # # # In order to do this, we need 1 shared column between the two datasets. # # For most psychology cases, this is probably going to be a participant ID number. # # Note that for this to work, you need the columns to have an exact match of name! # # First let&#39;s check that they are the same!! # names(experiment.data) # names(item.level.data) # # # First off our subject ID columns are not the same. Let&#39;s swap that. # setnames(item.level.data,&quot;tmp.dat.subject.1.&quot;,&quot;SubjectNo&quot;) # setnames(experiment.data,&quot;Sub&quot;,&quot;SubjectNo&quot;) # Make this clearer!!! # # # If you need to do more than 1, use the c() operator! # # Now if we look at this column, it&#39;s all messe up. # # The code below fixes it, if you want to learn more about regex, check it out # # if not, just skip below. # # item.level.data$SubjectNo &lt;- str_replace_all(string = item.level.data$SubjectNo, pattern = &quot;.csv&quot;, replacement = &quot;&quot;) # item.level.data$SubjectNo &lt;- str_replace_all(string = item.level.data$SubjectNo, pattern = &quot;C&quot;, replacement = &quot;&quot;) # item.level.data$SubjectNo &lt;- str_replace_all(string = item.level.data$SubjectNo, pattern = &quot;M&quot;, replacement = &quot;&quot;) # item.level.data$SubjectNo &lt;- str_replace_all(string = item.level.data$SubjectNo, pattern = &quot;CM&quot;, replacement = &quot;&quot;) # item.level.data$SubjectNo &lt;- as.numeric(item.level.data$SubjectNo) # # # Let&#39;s just quickly check to see if all the subject numbers make sense # hist(experiment.data$SubjectNo) # Cause for alarm! Negative values and placeholders! # # # Drop those # experiment.data &lt;- experiment.data[experiment.data$SubjectNo &gt; 0 &amp; experiment.data$SubjectNo &lt; 1000,] # # # Note this works because the SubjectNo variable is numeric # hist(experiment.data$SubjectNo) # hist(item.level.data$SubjectNo) # # # Ok, finally we merge our datasets. What we are doing here is called an &quot;inner join&quot; # # Here we willkeep all of the ROWS of the dataset in the middle of the command # # Note we need to swap over our key to be a character value. # item.level.data &lt;- data.table(item.level.data) # experiment.data &lt;- data.table(experiment.data) # # item.level.data # # exp.data &lt;- item.level.data[experiment.data, on=&quot;SubjectNo&quot;] # # exp.data # # # View(exp.data) # Use View to hover over column number # # # Let&#39;s reorganize our columns so individual stuff is at the back # # We could do this with data.table, but it&#39;s a different syntax so let&#39;s swap back # # Normally you try to stick to minimal switching, but we&#39;re just taking # # a big tour du R right now and learning to think # exp.data &lt;- data.frame(exp.data) # exp.data &lt;- exp.data[,c(1,40:100,2:39)] # View(exp.data) # # #====================================================================================================== # # Checking for Univariate Outliers # #-------------------------------------------------- # # For this example, let&#39;s imagine a univariate outlier is one with a zscore # # greater than 3. While we could write a bit of code to look for this, let&#39;s use # # the pairs.panels() function in the psych pacakge to just get used to looking at our data # # The function is not the biggest fan of huge datasets, so let&#39;s index our # # dataset to only grab what we need. Try to change the values and look # # at variables of interest. # # pairs.panels(exp.data[,2:7], lm = TRUE) # # # But of course we need to look at numbers in terms of their zscores! # # Let&#39;s first standardize our entire dataset using the apply function # # Note we only can do this on numeric values! # # # The apply function takes 3 argument # # The first is what you want to manipulate, the second is if it&#39;s rows 1 or columns 2 # # (remeber this because it&#39;s always rows then columns!), and the function. # # You can even write your own (though we&#39;ll get to functions later) # gmsi.z.scores &lt;- apply(exp.data[2:7],2,scale) # # exp.data.with.z &lt;- cbind(exp.data, gmsi.z.scores) # # # Now we can index this to find values above whatever theshold we want! # # table(gmsi.z.scores &gt; 2) # gmsi.z.indexer &lt;- gmsi.z.scores &gt; 2 # gmsi.z.scores[gmsi.z.indexer] # See what they are, find them , decide to get rid of # # # #====================================================================================================== # # Checking for Multivariate Outliers # #-------------------------------------------------- # # A bit tricker, I leanred how to do this off a blog post. # gmsi.responses &lt;- exp.data[,c(63:100)] # # mahal &lt;- mahalanobis(gmsi.responses, # colMeans(gmsi.responses, na.rm = TRUE), # cov(gmsi.responses, # use = &quot;pairwise.complete.obs&quot;)) ## Create Distance Measures # # cutoff &lt;- qchisq(.999, ncol(gmsi.responses)) ## Create cutoff object .001 signifiance and DF = obs # summary(mahal &lt; cutoff) ## 11 Subjects greater than 70 cutoff # # # Add On Variables # exp.data$mahal &lt;- mahal # exp.data &lt;- data.table(exp.data) # To use easier indexer, needs data.table # exp.data[exp.data$mahal &lt; cutoff] # # #====================================================================================================== # # Checking for Skew and Kurtosis # #-------------------------------------------------- # apply(gmsi.responses, 2, skew) # apply(gmsi.responses, 2 , kurtosi) # # #====================================================================================================== # # Exporting Data # #-------------------------------------------------- # # It&#39;s best practice to separate your cleaning and your analysis into separate scripts. # # Export the dataset you have into a new csv file into a directory that would make sense to # # someone who has never seen your project before. # # write.csv(exp.data,&quot;My_Experiment_Data.csv&quot;) # #====================================================================================================== "],
["episotomology-of-statistics.html", "Chapter 4 Episotomology of Statistics", " Chapter 4 Episotomology of Statistics We describe our methods in this chapter. "],
["descriptive-statistics-z-scores-central-limit.html", "Chapter 5 Descriptive Statistics, z Scores, Central Limit 5.1 Descriptive Statistics and the Normal Distribution 5.2 Practice", " Chapter 5 Descriptive Statistics, z Scores, Central Limit 5.1 Descriptive Statistics and the Normal Distribution 5.1.1 Organizing Data Descriptive statistics are traditionally used to summarize data from observed samples. Most often, sample data are organized into distributions of information based on ascending scores. For example, we might have a table with some SAT-Verbal scores from a few different students. Before going on to think about this, also take note that the shape of this data (though very minimal) is in the tidy format. According to Hadley Wickham on the tidyr CRAN page for data to be tidy it must have the following properties: Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. This isn’t very important right now, but once you get to more complex designs, it will be good to have had thought about this before. student &lt;- c(1,2,3,4,5,6) SAT &lt;- c(480,530,560,650,720,760) satData &lt;- data.frame(student,SAT) satData ## student SAT ## 1 1 480 ## 2 2 530 ## 3 3 560 ## 4 4 650 ## 5 5 720 ## 6 6 760 5.1.2 Shape of Data When visualized, data can take on a variety of shapes. Below are a few of the shapes you might come across when analyzing data. The first, and probably least likely distribution you will find is the uniform or rectangular distribution. We can create this plot and the others by using the distribution functions from R’s functionality. In each case we are going to take 1,000 samples from 0 to 1. We’ll plot everything using ggplot2 so we can also get used to using it for our packages. library(ggplot2) set.seed(666) uniformData &lt;- runif(n=1000, min=0, max=100) distributions &lt;- data.frame(uniformData) # Make variable to remove ggplot elements cleanUpPlots &lt;- theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) ggplot(distributions, aes(distributions$uniformData)) + geom_histogram(binwidth = 5) + labs(x = &quot;Independent Variable&quot;, y = &quot;Frequency&quot;, title = &quot;Uniform Distribution&quot;) + cleanUpPlots Try to run the above code with different arguments in the binwidth argument. You’ll notice that the way you plot the data will actually represent it differntly. We can also have positively skewed and negatively skewed distributions. If a distribution is skewed, it usually means that the mode does not equal the mean. We’re going to approximate both of these with another one of R’s probability functions. positiveSkewData &lt;- rchisq(1000,6) # Chi Square distributions are positively skewed negativeSkewData &lt;- - positiveSkewData # Flip it! distributions &lt;- data.frame(uniformData, positiveSkewData, negativeSkewData) ggplot(distributions, aes(distributions$positiveSkewData)) + geom_histogram(binwidth = 1) + labs(x = &quot;Independent Variable&quot;, y = &quot;Frequency&quot;, title = &quot;Positive Skew Distribution&quot;) + cleanUpPlots ggplot(distributions, aes(distributions$negativeSkewData)) + geom_histogram(binwidth = 1) + labs(x = &quot;Independent Variable&quot;, y = &quot;Frequency&quot;, title = &quot;Negative Skew Distribution&quot;) + cleanUpPlots The most important distribution in the world of Frequentist statistics is a normal distribution. A normal distribution is defined by THIS HERE. normalData &lt;- rnorm(n = 1000,mean = 0,sd = 2) # Flip it! distributions &lt;- data.frame(uniformData, positiveSkewData, negativeSkewData, normalData) ggplot(distributions, aes(distributions$normalData)) + geom_histogram(binwidth = 1) + labs(x = &quot;Independent Variable&quot;, y = &quot;Frequency&quot;, title = &quot;Normal Distribution&quot;) + cleanUpPlots And the lastly we can have both leptokurtic and platykurtic distributions. leptoData &lt;- rnorm(n = 1000,mean = 0,sd = 2) platyData &lt;- rnorm(n = 1000,mean = 0,sd = 2) distributions &lt;- data.frame(uniformData, positiveSkewData, negativeSkewData, normalData,leptoData, platyData) ggplot(distributions, aes(distributions$leptoData)) + geom_histogram(binwidth = 1) + labs(x = &quot;Independent Variable&quot;, y = &quot;Frequency&quot;, title = &quot;Leptokurtic Distribution, FIX ME&quot;) + cleanUpPlots ggplot(distributions, aes(distributions$platyData)) + geom_histogram(binwidth = 1) + labs(x = &quot;Independent Variable&quot;, y = &quot;Frequency&quot;, title = &quot;Platykurtic Distribution, FIX ME&quot;) + cleanUpPlots Generally measurses of central tendency are used to characterize the most typical score in a distribution. For example we could calculate the mean or the median of our SAT data. The mean is calculated by adding up all our numbers, designated with the Greek letter Sigma \\(\\Sigma\\) then dividing by the amount of numbers we added up, or \\(n\\). As an equation it would look like this. \\[\\bar{X} = \\frac{(\\Sigma\\ x_i)}{n}\\] Take a second to talk yourself through that so later you will start to feel more comfortable with more complex equational notation. Some people find it helpful to just try to say the equation in plain English. In this case it would be the mean, or \\(\\bar{x}\\) is defined as or equal to what happens when you add up \\(\\Sigma\\) every single value \\(x\\) that you have going up to \\(i\\), then divide all those numbers by the amount of numbers you have, or \\(n\\). The median is defined by finding the middle number. If there is a tie because we have an even set of numbers, we take the mean of the middle two numbers. We can do both of these in R as well. Below we can either type in the numbers as you would with a calculator, or use a function in R. Notice that each step when its typed out is saved into an object. By starting to think this way, it will pave the way for writing more elegant code later on. # Typing it out our.data &lt;- c(480 + 530 + 560 + 650 + 720 + 760) how.many &lt;- length(our.data) our.data/how.many ## [1] 3700 # Inbuilt functions mean(satData$SAT) ## [1] 616.6667 median(satData$SAT) ## [1] 605 Notice here that for adding up the means by hand I could have done what programmers call hard coded the equation in. That would have looked like this. our.answer &lt;- c(480 + 530 + 560 + 650 + 720 + 760) / 6 our.answer ## [1] 616.6667 The problem with this, is that every time you get a new SAT score you have to both enter the score and update how many scores you are dividing by. Whenever you see a chance to take a shortcut like this, do it! It will save you tons of time in the future. 5.1.3 Important Considerations for Central Tendency There are three big considerations to think about when choosing numbers to represent your data. The mode is the most variable from sample to sample; the mean is the least variable The mean is the most sensitive to extreme scores; e.g., skewed distributions The mean is the most frequently used measure because The sum of the deviations around the mean is 0 The sum of the squared deviations is the smallest around the mean, rather than the mode or median; this is known as the least squares principle 5.1.4 Measures of Variability The range is simply the largest score minus the smallest score. It is the crudest measure of variability. In our dataset we would calculate it with the following code. 760 - 480 ## [1] 280 # OR range(satData$SAT) ## [1] 480 760 max(satData$SAT) - min(satData$SAT) ## [1] 280 The interquartile rangerepresents the spread between the score at the 75th and 25th percentiles. Boxplots are often used to graphically represent inner 50% of the scores. y &lt;- satData$SAT boxplot.example &lt;- data.frame( x = 1, y0 = min(y), y25 = quantile(y, 0.25), y50 = median(y), y75 = quantile(y, 0.75), y100 = max(y) ) ggplot(boxplot.example, aes(x)) + labs(title = &quot;Example of Boxplot&quot;) + geom_boxplot(aes(ymin = y0, lower = y25, middle = y50, upper = y75, ymax = y100), stat = &quot;identity&quot;) The variance is essentially the averaged squared deviation around the mean. Now there is a very important distinction that we will get to a bit later on, but that is the difference between a population value or \\(\\sigma^2\\) and a sample variance or \\(s^2\\). In order to do frequentist statistics, we need to assume that there is some sort of True value that the group we are measuring has and it is a fundamental property of the group! For more on this see CHAPTER 3 and the work of Zoltan Dienes. Somewhere, maybe written on a golden plate in heaven is the actual value of the average weight of a labrador retriever. The problem is we will never have access to that information so we need to estimate it by using a sample. The logic is that if we can truly draw in a random way from our entire population, in this case labrador retrievers, the central limit theroum will give us a good approximation of what that True value will be. Since we want to be clear about when we are talking about the Platonic, True value and the actual sample we collected, we use different Greek notation. The \\(\\sigma^2\\) refers to the Platonic value and the \\(\\sigma^2\\) is the sample. They are defined as follows: \\[\\sigma^2 = \\frac{\\Sigma(X_i - \\mu)^2}{N}\\] \\[s^2 = \\frac{\\Sigma(X_i - \\mu)^2}{n-1}\\] Note here that each of these formulas needs a mean. In the population equation that is defined as \\(\\mu\\) and in samples we used \\(\\bar{X}\\). In our case with the SAT scores, we are wanting to know the True value of the SAT scores of whatever population our six students are theorized to come from. To do the calulations below we need to know the mean which we calcualted above to be 616.67. Now since these scores are to serve as a represntive sample in hopes of getting at the true population value we need to use the formula reflecting the sample variance or \\(s^2\\). \\[s^2 = \\frac{(480-616.7)^2 + . . . + (760 - 616.676)^2}{6-1}\\] Doing this by hand we get an \\(s^2\\) value of 12346.67. Or running it in R, we would use. var(satData$SAT) ## [1] 12346.67 The standard deviation is the square root of the variance of the sample. \\[s = \\sqrt\\frac{\\Sigma(X_i - \\mu)^2}{n-1}\\] And since we know \\(s^2\\) from above, we can shorten this to \\[s = \\sqrt{s^2} = \\sqrt{12345.67} = 111.12\\] Or run it in R and get sd(satData$SAT) ## [1] 111.1156 Standard scores represent the distance of raw scores form their mean in standard deviation units. \\[z = \\frac{x_i - \\bar{X}}{s}\\] So if we needed to find the \\(z\\) score or standardized score for someone who got a 560 on their SAT we could compute the following. \\[z_{560} = \\frac{560 - 616.67}{111.12} = -0.51\\] Interpreted in-context, this would mean that if you scored a 560 on the SAT, based on our sample (which we think helps us get at the True popuation value), you would be scoring about less than 1 standard deviation (the unit of z) below the average. 5.1.5 Properties of z Scores Z scores are defined by having having three separte properties: The standardized distribution preserves the shape of the original raw score distribution The mean of the standardized distribution is always 0 The variance &amp; standard deviation are always 1 Many of the variables in behavioral sciences are distributed normally. In addition, the basis for parametric inferential statistics is based on the normal distribution. The normal distribution is unimodal, symmetrical, bell shaped, with a maximum height at the mean. The normal distribution is continuous and additionallythe normal distribution is asymptotic to the X axis—it never actually touches the X axis and theoretically goes on to infinity. The normal distribution is really a family of distributions defined by all possible combinations of means \\(\\mu\\) and standard deviations \\(\\sigma\\). We can use the standardized normal distribution to find areas of probability under the curve. With a normal distribution, there is always a fixed area under the curve which we take the reflect the probability of getting a score when sampling from a population. For example if we go 1 z unit (1 SD) away from the mean we find 34% of the total area of the curve there. If you then extend that out to the negative side, you then encapsulate 68% of the distribution. This would translate to a scenario where if you were to get a score at random from the distribution, 68% of the time you would get a score between 1 and -1 SD units from your mean. This process can be extended as seen in the figure below. z Scores and Areas Under the Curve We could also calculate the area between two z scores as shown here. And we could also look at how much area under the distribution exists beyond two standard deviations beyond the mean. z Scores and Areas Under the Curve Or we could pick any z score values and find the area under the mean! z Scores and Areas Under the Curve 5.2 Practice We can now start to put this to use. Here are some past homework examples. During tryouts, a sample of ballet dancers were rated on their athletic ability andoverall knowledge of the art. Below are the ratings for each dancer (a score above 75 percent means that the dancer will join the troupe 83, 98, 45, 69, 52, 94, 82, 74, 71, 83, 62, 85, 90, 97, 61, 74, 74, 88 Let’s put them into R so we can use answer a few questions about our data. ballet &lt;- c(83, 98, 45, 69, 52, 94, 82, 74, 71, 83, 62, 85, 90, 97, 61, 74, 74, 88) What is the median percentage? median(ballet) ## [1] 78 What is the mean percentage? mean(ballet) ## [1] 76.77778 What is the standard deviation for the sample (assume we don’t know any population characteristics)? sd(ballet) ## [1] 15.02373 Demonstrate the least squares principle by showing that the sum of squares (SS) around the mean is smaller than the sum of squares around the median (remember to show your work for each). ballet_mean &lt;- mean(ballet) ballet_median &lt;- median(ballet) ballet_sd &lt;- sd(ballet) Now we can use these values to do our math! sum((ballet - ballet_mean)^2) ## [1] 3837.111 sum((ballet - ballet_median)^2) ## [1] 3864 # Then having R do the final work for us sum((ballet - ballet_mean)^2) &gt; sum((ballet - ballet_median)^2) ## [1] FALSE What are the standardized (z) scores for the raw scores 73, 99, and 66? If you know the population mean and the sd, you can calulate a z score using the formula \\[z = \\frac{x_i - \\bar{X}}{s}\\] Or in our case (73 - ballet_mean)/ ballet_sd ## [1] -0.2514541 (99 - ballet_mean)/ ballet_sd ## [1] 1.479142 (66 - ballet_mean)/ ballet_sd ## [1] -0.7173837 What proportion of scores exceeds a raw score of 73? pnorm(q = 73, mean = ballet_mean, sd = ballet_sd) ## [1] 0.4007315 To get the other side of the probability we can remember that we can treat the line above as an object! 1 - pnorm(q = 73, mean = ballet_mean, sd = ballet_sd) ## [1] 0.5992685 What proportion of scores lies between the raw scores of 75 and 100? Let’s be clever for this one and just put the two equations together for this one. Or if you want, you could save them into objects. pnorm(q = 100, mean = ballet_mean, sd = ballet_sd) - pnorm(q = 75, mean = ballet_mean, sd = ballet_sd) ## [1] 0.4860093 pnorm(q = 76, mean = ballet_mean, sd = ballet_sd) ## [1] 0.479356 What raw score represents the 55thpercentile? To find out what raw score represents a percentile we can go back and use the formula from above, just rearranged a bit. \\[z = \\frac{x_i - \\bar{X}}{s}\\] or with a bit of basic algerbra \\[x_i = (z * s) + \\bar{X}\\] (.05 * ballet_sd) + ballet_mean ## [1] 77.52896 Between what raw scores does the middle 60% of the distribution lie? Lastly, we then need to find first what z scores map on 30% on either side of the disribution, then convert those z scores to raw scores on our data using the z score formula. First we find the z score associated with what is 30% left and right of the mean (it will be the same number, only negative). In this case, it is +/-.84. With that established, we then first solve for x \\[-0.84 = \\frac{x - 76.78}{15.02}\\] Giving us a value of 64.1 And we do it again with the positive number. \\[0.84 = \\frac{x - 76.78}{15.02}\\] Resulting in 89.547. "],
["sampling-distributions.html", "Chapter 6 Sampling Distributions", " Chapter 6 Sampling Distributions In this chapter, we’ll cover three ideas/questions. What are inferential statistics and the logic behind them What the underlying distribution of all hypothetical sample estimates is known as the sampling distribution, and it constitutes the third of the three important distributions. Several important implications follow from an understanding of the sampling distribution as a normal distribution and from the central limit theorem Spoken about a bit before in the other chapter, we have both sample statistics like \\(\\bar{X}\\) and population parameters \\(\\mu\\). The idea of how frequentist inferential statistics is as follows. Samples must be selected randomly in order to make appropopriate inferences about the parent population. Sample estimates must be compared to an underlying distributionof estimates of all other hypothetical samples of that same sizefrom the parent population. Based on this comparison and the associated probability of obtaining certain outcomes, inferences can be made about population parameters. Sampling It’s important to note that there are three different distributions that we typically talk about. Two you should be familiar with – the populatation and the sample. The third is the sampling distribution which is a distribution of sample means. The sampling distribution of the mean is generated by considering all possible sample means of a given sample size. As is demonstratd from the image below, in A we can see there is some sort of distribution, then with one sample (notice the \\(\\bar{X}\\)), we now have one wide sample. As we increase that to \\(N = 16\\), the sampling distribution becomes more narrow. This narrowing is reflective of the idea we are coming in on the true value of the population via our random sampling. The central limit theorem states that as the sample size \\(n\\) increases, the sampling distribution of the mean for simple random samples of \\(n\\) cases, taken from a population with a mean equal to \\(\\mu\\) and a finite variance equal to \\(\\sigma^2\\), approximates a normal distribution. From this, three points follow: 1.The shape of the sampling distribution is normal 2. The mean of the sampling distribution is \\(\\mu\\) 3. The standard deviation of the sampling distribution, or standard error of the mean, is \\[\\frac{\\sigma}{\\sqrt{n}}= \\sigma_\\bar{X}\\] Several important implications follow from an understanding of the sampling distribution as a normal distribution and from the central limit theorem. Because we know the mean and standard error, we can calculate the probability of selecting a random sample mean that is at or more extreme than a particular value on the distribution. \\[z = \\frac{\\bar{X}-\\mu}{\\sigma_\\bar{X}}\\] We can appeal to the table of z scores on the standard normal distribution to find the probability. For example, consider a sampling distribution of SAT scores with a mean of 455 and a standard error of 8.33. This standard error was generated with \\(n\\)= 144 and \\(\\sigma\\)= 100. So if you wanted to find the liklihood of finding as ample mean equal to or more than extreme of 480, we would plug it into the follow equation. \\[z = \\frac{480 - 455}{8.33} = 3.00\\] And if we look that up a table of z distributions, we got a probability of \\(p=.0013\\). Sampling Because we know the mean and standard error, we can calculate the probability of selecting a random sample mean that is at or more extreme than a particular value on the distribution. As sample size (\\(n\\)) increases, the variability of the sampling distribution (\\(\\sigma_\\bar{X}\\)) decreases. Even when the parent population is not normally distributed, the sampling distribution becomes normal as sample size (n) increases. You can see this demonstrated in THIS LINK. "],
["hypothesis-testing.html", "Chapter 7 Hypothesis Testing 7.1 Steps of Hypothesis Testing 7.2 Two Sample", " Chapter 7 Hypothesis Testing The sampling distribution of the mean helps us to make hypotheses about the likelihood that a given sample mean comes from a sampling distribution with a given mean. Stated differently, a hypothesis test helps us determine whether the observed difference between a sample mean and a hypothetical population mean is either negligible or meaningful. The Null Hypothesis \\(H_o : \\mu = some value\\) The alternative Hypothesis \\(H_o : != \\mu = some value\\) The Alternative Hypothesis A sample mean of a given sample size is produced and compared to the hypothetical sampling distribution’s mean to test the null hypothesis Imagine we know the population (for some reason) and the True value is 455. True Value is 455 Descision Tree The hypothesis test is based on inference (i.e., inductive reasoning), and therefore there is a chance that mistaken inferences will be made. The 2 ×2 matrix of decision outcomes given the state of nature and the decision made Matrix A Type I erroris produced when we mistakenly reject the null. It is associated with probability alpha (\\(\\alpha\\)), or the level of significance. We conventionally set this level to be .05in psychology, but there are considerations to be made for increasing or decreasing this value (e.g., .01 or .10). A Type II erroris produced when we mistakenly fail to reject the null. It is associated with probability beta (\\(\\beta\\)), which is related to, but not the same as, alpha. The level of significance creates the bounds for the rejection region—the extreme region(s) under the sampling distribution equal to αif the null hypothesis is true. Matrix The directionality of the test should also be considered. Matrix Hypothesis tests differ slightly when population parameters, such as \\(\\sigma^2\\), are known versus unknown. The general formula for a test statistic \\(Statistic - parameter /SE\\) When \\(\\sigma^2\\)is known, we use the standard error of the normal distribution as the denominator. The result is the ztest. Z TEST FORMULA STANDARD ERROR HERE When \\(\\sigma^2\\) is unknown, we use the t distributionas our sampling distribution, with a standard error that must be estimated from \\(s^2\\) or \\(s\\). The result is the ttest. T TEST AND STANDARD ERROR FORMULA HERE Left off on Page 11 in onesampleNHST.png The concept of degrees of freedom (df)must be considered for the ttest. For each sample drawn, df= n–1. VARIANCE FORMULSA DF on t statistic 7.1 Steps of Hypothesis Testing On a standardized anagram task, \\(\\mu\\)= 26 anagrams solved with a \\(\\simga\\)= 4. A researcher tests whether the arousal from anxiety is distracting and will decrease performance. A sample of \\(n\\)= 14 anxiety patients is tested on the task. There average performance is 23.36 anagrams. Step one: State the null and alternative hypotheses $H_O = : = 26 $ $H_O = : != 26 $ Consider directionality. Step two: Set the criterion for rejecting H0. Alpha is usually set to .05, but could be other values depending on the research context. Again, directionality is important to consider. Step three: Select the sample and collect your data. Step four: Locate the region of rejection and the critical value(s) of your test statistic. Again, directionality is important to consider. Step five: Compute the appropriate test statistic. σis known, so we use the ztest. Convert me Convert me Step six: Decide whether to reject H0. Is -2.47 more extreme than the critical value? Step five: Compute the appropriate test statistic. \\(\\sigma\\) is unknown, so we use the ttest. Step six: Decide whether to reject H0. Is -3.00 more extreme than the critical value? df= 13, look up critical value in table C.3 and find ±1.77. T distribution here How do we report this result in a typical research article? “The mean number of anagrams solved by anxiety patients (M= 23.36) was significantly lower than the mean established by test norms (M= 26), t(13) = 3.00, p&lt; .05.” Sometimes you’ll find people report the pvalue lower than .01 if it passes this criterion as well. For example, t(13) = 3.00, p&lt; .01. Don’t be confused by the meaning of this, however. 7.1.1 Other important considerations. The hypothesis test is a test of the NULL hypothesis, assuming that the null is true. Thus, the test gives you the probability of your sample mean being that different (or more) from the population mean by chance IFFthe null is true. Statistical significance is not the same as practical significance. Being able to report the result of a hypothesis test statistically versus being able to describe the result to a lay person. Relate the inference back to the original research question! 7.2 Two Sample In cases where we wish to compare two sample means, the hypothesis testing logic is essentially the same as with the one-sample tests, with some slight differences in the null hypothesis, in the sampling distribution, and in the computation. When different people (or animals) contributed to the two samples, the comparison distribution that represents the null hypothesis is a sampling distribution of differences between means. The hypothesis test is therefore referred to as an independent-samples test. When both sample means were produced by the same participants, we conduct what is known as a dependent-samples test. This is a test of the average difference between the scores in one condition and the scores in another condition—thus, the unit of measurement is a difference score. Nondirectional Null Hypothesis $H_O : _1 - _2 = 0 | H_O : _1 = _2 $ Nondirectional Alternative Hypothesis $H_a : _1 - _2 != 0 | H_a : _1 = _2 $ Directional Null Hypothesis $H_O : _1 &gt; _2 | _1 &lt; _2 $ Directional Alternative Hypothesis $H_a : _1 &lt; _2 | H_a : _1 &gt; _2 $ Convert me This is basically a subtraction of one sampling distribution from another, to produce a distribution of possible differences between sampling distributions. $ _1 - _2$ * The mean of this sampling distribution is $ _1 - _2$ * The shape of this sampling distribution is approximately normal. * When 2is known for each distribution, the standard error of the difference between means is s2is considered a pooled estimateof the population variance because the individual estimates are literally summed together in the computation: \\(s^2 = \\frac{SS_1 + SS_2}{n_1 + n_2 -2}\\) If you know the individual group variances or standard deviations, then \\(s^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 -2}\\) Convert me \\(t = \\frac{(\\bar{X_1}-\\bar{X_2})-(\\bar{\\mu_1}-\\bar{\\mu_2})}{s_{\\bar{X_1}-\\bar{X_2}}}\\) Example of the independent samples ttest The instructor of an introductory psychology course is interested in knowing if there is a difference in the mean grades on the final exam between the fall and spring semester classes. Summary data for the two samples is below: Convert me Are the final exam grades for the two classes equivalent? Step one: State the null and alternative hypotheses \\(H_o:\\mu_1 = \\mu_2\\) \\(H_a:\\mu_1 != \\mu_2\\) b.Step two: Set the criterion for rejecting H0. Alpha is usually set to .05, but could be other values depending on the research context. Make sure you’ve considered directionality! c.Step three: Select the sample and collect your data. d.Step four: Locate your region of rejection and critical values. Locate your region of rejection and critical values. \\(t_{cv,dv=298, \\alpha=.05}= +/- 1.96\\) Step five: Compute the appropriate statistic. We were never given \\(\\sigma\\) or \\(\\sigma^2\\), so we use the t test. Convert me Convert me Convert me Convert me Step six: Decide whether to reject H0. Is -4.86 more extreme than the critical value? \\(t_{cv,dv=298, \\alpha=.05}= +/- 1.96\\) The effect sizerefers to the magnitude of the phenomenon being tested and is calculated as Convert me This statistic reflects the standardized distance between two populationmeans. J. Cohen provides guidelines to interpret the value of d: “The average final exam score from the fall semester (M= 82.4) was significantly lower than the average score from the spring semester (M= 84.2), t(298) = 4.86, p&lt; .05.” Small: = 0.25Medium: = 0.50Large: = 1.0 “The average final exam score from the fall semester (M= 82.4) was significantly lower than the average score from the spring semester (M= 84.2), t(298) = 4.86, p&lt; .05, Cohen’s d= 0.53.” "],
["power-confidence-intervals-effect-size-measures.html", "Chapter 8 Power, Confidence Intervals, Effect Size Measures 8.1 Power 8.2 Confidence Ientervals", " Chapter 8 Power, Confidence Intervals, Effect Size Measures 8.1 Power We have discussed the fact that the conclusions drawn from hypothesis tests are essentially inferences about population parameters, based on sample information. But we have thus far neglected a discussion of what statistical factors should be considered in planning and assessing research-based hypothesis tests. By minimizing the probability of a Type II error (β), we are at the same time increasing the amount of powerof our hypothesis test (1-β). Power is defined as the probability of rejecting the null hypothesis when it is false (i.e., should be rejected). Four important factors affect the power of a statistical test. Knowing any three of these factors mathematically fixes the fourth. Thus, one can use these factors in determining the appropriate design for a particular study. Sample size is most often the targeted factor in formulating such a plan. Table Careful planning of research involves minimizing Type I andType II errors. One rule of thumb is that \\(\\beta\\) should be no more than .20 (e.g., if \\(\\alpha\\) = .05, \\(\\beta\\) = .20). c.For the t test, if the null hypothesis distribution is centered on a t value of 0, then the noncentralt distribution represents the alternative hypothesis distribution, centered on \\(\\delta\\). -This represents the average t value one would expect for a given effect size and sample size Noncentral T 8.1.1 Test Equations One Sample tests $ = d$ \\(d = \\frac{\\bar{X}-\\mu}{\\sigma}\\) \\(g = \\frac{\\bar{X}-\\mu}{s}\\) Two sample tests \\(\\delta = d\\sqrt{\\frac{n}{2}}\\) \\(d = \\frac{\\mu_1-\\mu_2}{\\sigma}\\) \\(g = \\frac{\\bar{X_1}=\\bar{X_2}}{s_p}\\) For unequal n \\(n_n = \\frac{2n_2n_2}{n_1 + n_2}\\) \\(g = t\\sqrt{\\frac{n_1 + n_2}{n_1n_2}}\\) Noncentral T Effect size (ES) or standardized effect size (e.g., d). i. The difference between population means (e.g., \\(\\mu1-\\mu2\\)). ii. The population standard deviation (\\(\\sigma\\)). Noncentral T 8.1.2 Factors of Power Effect size (ES) or standardized effect size (e.g., d). The difference between population means (e.g., \\(\\mu_1-\\mu_2\\)). The population standard deviation (\\(\\sigma\\)). Sample size (\\(n\\)). Significance level (\\(\\alpha\\)). Directionality of the hypothesis test (one-tailed vs. two-tailed). One sample case \\(\\delta = d\\sqrt{n}\\) \\(n = (\\frac{\\delta}{d})^2\\) two sample case \\(delta = d\\sqrt{\\frac{n}{2}}\\) \\(n = 2(\\frac{\\delta}{d})^2\\) A clinical psychologist wants to test the hypothesis that people who seek treatment for psychological problems have higher IQs than the general population. To test her hypothesis, she wants to use the IQ values from 25 randomly selected clients and also to calculate the power to find a 5-point difference in IQ. The mean of the population would be 100 and, therefore, the mean of her clients a 105. The population SD for IQ is 15. (This scenario is from Howell’s 2002 text “Statistical Methods for Psychology”) Known: \\(\\mu_{client}=105\\) \\(\\mu_{pop}\\) \\(\\sigma_{pop} = 15\\) Calculate it with \\(d = \\frac{105-100}{15} = 0.33\\) \\(\\delta = d\\sqrt{n} = 0.33\\sqrt{25}=1.65\\) Given this information and an expected alpha (two-tailed) of .05, we can find in Table A.4 in the Cohen text that the power is between .25 and .50, and more exactly about halfway in between (around .38). What does this value of .38 mean? If power should be at or above .80, what does the clinician do? Increase alpha? Decrease the population SD? Increase the difference between population means? Increase sample size? For a power level of .80, δneeds to be 2.80, from Table A.4 \\(n = (\\frac{\\delta}{d})^2=(\\frac{2.80}{0.33}^2 = 8.48^2 = 71.91\\) 8.2 Confidence Ientervals Sample measures of central tendency, such as the mean, are considered point estimates of population parameters. Confidence intervals are considered a type of interval estimation for population parameters. Computation of confidence intervals. Capture percentage, prediction, and replication Over repeated sampling from a known distribution, the confidence interval represents the percentage of such intervals that contain the population mean. Can be set at any percentage: 90%, 95%, 99% Based on characteristics of the sampling distribution (zor t) and therefore highly related to the manner in which sampling distributions are used for NHST  But CIs and pvalues from NHST are not the same thing!!! Hicks CI 1 Confidence Interval for single sample mean \\(\\bar{X} = +- (t_{cv})s_{\\bar{X}}\\) Confidence interval for a mean difference scores (dependent samples) \\(\\bar{D} +- (t_{cv})s_{\\bar{D}\\) Confidence interval for a difference between sample means (independent samples) \\((\\bar{X_1}-\\bar{X_2}) +- (t_{cv})(s_{\\bar{X_1}-\\bar{X_2}})\\) The instructor of an introductory psychology course is interested in knowing if there is a difference in the mean grades on the final exam between the fall and spring semester classes. Summary data for the two samples is below: What are the 95% confidence intervals around each sample mean, and around the difference between the sample means? fall &lt;- c(82.4,150,11.56) spring &lt;- c(84.2,150,11.44) stat &lt;- c(&quot;Mean&quot;,&quot;N&quot;,&quot;s2&quot;) grades &lt;- data.frame(stat,fall, spring) grades ## stat fall spring ## 1 Mean 82.40 84.20 ## 2 N 150.00 150.00 ## 3 s2 11.56 11.44 What are the 95% confidence intervals around each sample mean, and around the difference between the sample means? Confidence Interval for single sample mean \\(\\bar{X} = +- (t_{cv})s_{\\bar{X}}\\) This has critical value of 1.97 +-. First need to find the standard error with each one. $s_{X} = = = = 0.28 $ $s_{X} = = = = 0.28 $ Now do CI for both \\(\\bar{X} = +- (t_{cv})s_{\\bar{X}}\\) \\(82.4 +- (1.97)(0.28)\\) \\(82.4 +- 0.55\\) \\((81.85, 82.95)\\) \\(84.2 +- (1.97)(0.28)\\) \\(84.4 +- 0.55\\) \\((83.65, 84.75)\\) Calculate that now for differences \\((\\bar{X_1}-\\bar{X_2}) +- (t_{cv})(s_{\\bar{X_1}-\\bar{X_2}})\\) \\(s_{\\bar{X_1}-\\bar{X_2}}= \\sqrt{0.07 + 0.07} = \\sqrt{0.14}=0.37\\) Alpha value associated with this is 1.96. \\(82.4 - 84.2) +- (1.96)(0.37)\\) \\(-1.8 +- 0.73 = (-2.53,-1.07)\\) Important points to keep in mind regarding confidence intervals: They are two-tailed by nature (i.e., on either side of a sample mean). For a given sample size, increasing the level of confidence (e.g., from 95% to 99%) increases the interval width. The narrower the interval (at a given level of confidence!) reflects better statistical precision. Sample size directly affects the width of the interval by affecting the standard error estimate. 8.2.1 Capture Percentage Capture percentage, prediction, and replication What is the likelihood that a subsequent experiment will replicate? Concept of capture percentage: likelihood that a subsequent sample mean will fall into the CI of the current sample. Hicks CI 2 "],
["correlation-and-regression.html", "Chapter 9 Correlation and Regression", " Chapter 9 Correlation and Regression attempt here "],
["matched-t-test.html", "Chapter 10 Matched T Test 10.1 Theory", " Chapter 10 Matched T Test 10.1 Theory When both sample means were produced by the same participants, we conduct what is known as a dependent-samplestest. This is a test of the average difference between the scores in one condition and the scores in another condition—thus, the unit of measurement is a difference score. \\(\\bar{D}=\\Sigma D /n\\) \\(D= X_{i1} - X_{i2}\\) The mean of this sampling distribution is \\(\\delta\\)= 0. The shape of this sampling distribution is approximately normal. The standard error of the sampling distribution of mean difference scores is \\(s_{\\bar{D}} = \\frac{s_D}{\\sqrt{n}}\\) \\(s_d = \\sqrt{\\frac{\\Sigma{(D-\\bar{D})^2}}{n-1}}\\) The test statistic is \\(t = \\frac{\\bar{D}-\\delta}{s_{\\bar{D}}}\\) \\(s_\\bar{D} =\\frac{s_D}{\\sqrt{n}}\\) The effect size is calculated \\(d =\\frac{\\bar{D}}{S_D}\\) \\(S_d = \\sqrt{\\frac{\\Sigma{(D-\\bar{D})^2}}{n-1}}\\) You are investigating whether the older or younger male in a pair of brothers tends to be more extroverted. So you test where each one falls on an introversion-extroversion scale. The results are as follows: # Dep younger &lt;- c(10,11,18,12, 15) older &lt;- c(18,17,19,16,15) dep &lt;- data.frame(younger, older) Step one: State the null and alternative hypotheses \\(H_o : \\delta = \\mu_1 - \\mu_2 = 0\\) \\(H_a : \\delta = \\mu_1 - \\mu_2 != 0\\) Step two: Set the criterion for rejecting H0. Alpha is usually set to .05, but could be other values depending on the research context. Make sure you’ve considered directionality! Step three: Select the sample and collect your data. Step four: Locate the region of rejection and critical values. \\(t_{cv,dv=4, \\alpha=.05}= +/- 2.77\\) Step five: Compute the appropriate statistic. We were never given \\(\\sigma\\)or \\(\\sigma^2\\), so we use the t test. Convert me Convert me Convert me \\(s_D=3.55\\) \\(s_\\bar{D} =\\frac{s_D}{\\sqrt{n}}= \\frac{3.35}{\\sqrt{5}}\\) \\(t = \\frac{\\bar{D}-\\delta}{s_\\bar{D}} = \\frac{-3.8-0}{1.50}= -2.53\\) Step six: Decide whether to reject H0. Is -2.53 more extreme than the critical value? \\(t_{cv,dv=4, \\alpha=.05}= +/- 2.77\\) “The average extroversion value for the younger male siblings (M= 13.2) did not differ significantly from the extroversion value for the older siblings (M= 17.0), t(4) = 2.53, p&gt; .05.” Effect size computation \\(d = \\frac{\\bar{D}}{s_D}= \\frac{-3.8}{3.35} = -1.13\\) Small: = 0.25Medium: = 0.50Large: = 1.0 “The average extroversion value for the younger male siblings (M= 13.2) did not differ significantly from the extroversion value for the older siblings (M= 17.0), t(4) = 2.53, p&gt; .05, Cohen’s d= 1.13.” So, why does the effect size calculation disagree with the result of the hypothesis test? Template file "],
["one-way-anova.html", "Chapter 11 One Way ANOVA", " Chapter 11 One Way ANOVA Template file "],
["multiple-comparisons.html", "Chapter 12 Multiple Comparisons", " Chapter 12 Multiple Comparisons Template file "],
["simple-effects-interactions-mixed-designs.html", "Chapter 13 Simple Effects, Interactions, Mixed Designs", " Chapter 13 Simple Effects, Interactions, Mixed Designs Template file "],
["repeated-measures-anova.html", "Chapter 14 Repeated Measures ANOVA:", " Chapter 14 Repeated Measures ANOVA: Template file "],
["mixed-two-way-anova.html", "Chapter 15 Mixed Two Way ANOVA", " Chapter 15 Mixed Two Way ANOVA Template file "],
["multiple-regression.html", "Chapter 16 Multiple Regression", " Chapter 16 Multiple Regression Template file "],
["chi-square.html", "Chapter 17 Chi-Square", " Chapter 17 Chi-Square Template file "],
["non-parametric-data.html", "Chapter 18 Non-Parametric Data", " Chapter 18 Non-Parametric Data Template file "],
["advanced-data-cleaning.html", "Chapter 19 Advanced Data Cleaning", " Chapter 19 Advanced Data Cleaning Template file "],
["advanced-multiple-regression.html", "Chapter 20 Advanced Multiple Regression", " Chapter 20 Advanced Multiple Regression Template file "],
["logistic-regression.html", "Chapter 21 Logistic Regression", " Chapter 21 Logistic Regression Template file "],
["mediation-and-moderation.html", "Chapter 22 Mediation and Moderation", " Chapter 22 Mediation and Moderation Template file "],
["ancova.html", "Chapter 23 ANCOVA 23.1 Theory 23.2 Practice", " Chapter 23 ANCOVA 23.1 Theory 23.1.1 Reducing Noise Just like the ANOVA (Analysis of Variance), the ANCOVA (Analysis of Covariance) is used to analyze experiments by calculating an F ratio for more than 2 groups. As with any F calculation, differences in the dependent variable are caused by two things: The independent variable (signal, systematic variation) Error (noise, unsysematic variation) The F statistic captures this \\[ F = \\frac{Variation\\ Due\\ to\\ IV}{Variation\\ Due\\ to\\ Error} \\] When there is small variation within groups and large variation between groups, we get a large F. CHART HERE When there is large variatin within groups and small variation between groups, we get a small F. The idea with the ANCOVA is that you can reduce your error term (the denominator from above) by choosing a covariate (CV) that is related to the DV and will soak up some random variation in your F test to give you a clearer picture of what is going on. Usually this means controlling for some sort of variable. The classic example is if you wanted to give some kids a test of some mental ability in a between subjects design but you don’t want their age to skew your results. You might have had a control condition, an intervention, and some sort of alternate intervention. Typically you would run an ANOVA on the three groups, but since you know age has been accounted for and you want to remove that effect from the model, you enter age as a covariate in this calculation. Outside of this there are three major applications for ANCOVA. Increase test sensitivity by using the CV(s) to account for more of the error variance Adjust DV scores to what they would be if everyone scored the same on the CV(s) Adjustment of a DV for other DVs taken as CVs Note that use of a CV can adjust DV scores and show a larger effect or the CV can even eliminate the effect. Looking at this another way: Reduces random error by increasing the size of F Reduces systematic error by adjusting for differences in means May increase differences by soaking up error It’s a good idea to use ANCOVA when you are removing variance in the DV related to covariate, but not related to the grouping variable. This decreases the error term and increases power. It’sa bad idea to use ANCOVA when groups differ on their mean level of the covariate. Usually here the covariate and the grouping variable are not independent. An example of this might be when “controlling” for anxiety when studying people with and without depression. Clearly people with depression will have higher levels of anxiety than their controls by nature of having depression! 23.1.2 Assumptions of ANOVA All ANOVA assumptions apply (errors are RIND) Random Independent Normally distributed The Covariate z is unrelated to x, and that z is related to, and in a sense, acts as a suppressor of y The covariate has a linear relationship with the dependent variable Variance of groups are equal. Correlation between y and z is equal for all levels of x (Homogeneity of Regression) You test for homogeneity of regression slopes by including a covariate by group interaction. If it’s significant, you violated an assumption and can’t to ANCOVA! CHART CHART 23.1.3 Best Practice Things you should control for: 1. Broad Descriptors (Age, Social Class) 2. General Ability (intellgience, memory, speed of movement) 3. Personality measures (extraversion, neuroticism) 4. Things you think are relevant 5. Pick CVs that are correlated with DV * Height good for basketball * Height poor for test of math skills 6. CV should not be influenced by treatment 23.2 Practice In order to get our hands dirty with ANCOVA we’re going to look at a dataset where a bunch of sheep were slaughtered. We have three types of sheep (ewe, wether, ram) and a measure of fatness and weight. Let’s first run an ANOVA to see if fatness differs between animal type. library(data.table) ## Warning: package &#39;data.table&#39; was built under R version 3.4.2 deadsheep &lt;- fread(&quot;datasets/deadsheep.csv&quot;) deadsheep[, Animal := as.factor(animal)] sheep.model.1 &lt;- aov(fatness ~ Animal, data=deadsheep) summary(sheep.model.1) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Animal 2 124.4 62.21 3.165 0.0566 . ## Residuals 30 589.6 19.65 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Looking at the output, we note that There was no significant effect of fatness on levels of animal, \\[F(2,30)=3.165, p &gt; .05\\]. Now since we know that the whole weight of the animal might be confounding our answer (it’s related to the DV of fatness, but not nessecarily related to the type of sheep), let’s run an ANCOVA with carcass weight as a covariate and see if that changes our answer. Before we run the ANCOVA, we need to first check that our IV is not significantly related to our CV. We do this by running an ANOVA predicting weigth by animal. #Test IND of IV and CV ind.sheep &lt;- aov(weight ~ Animal, data=deadsheep) ## Check CV for IV indep, want non-sig summary(ind.sheep) # p=.104 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Animal 2 14.20 7.102 2.438 0.104 ## Residuals 30 87.41 2.914 Here we get\\[ F(2,30) = 2.44, p&gt; .05\\] and can move forward. Let’s now run our ANCOVA. Note that R’s default is Type I Sum of Squares, we need to load the car package to give us the correct sum of squares. For further reading on this, check out the Andy Field book on this. sheepANCOVA &lt;- aov(fatness ~ Animal + weight, data=deadsheep) library(car) ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:psych&#39;: ## ## logit Anova(sheepANCOVA, type=&quot;III&quot;) ## Anova Table (Type III tests) ## ## Response: fatness ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 117.17 1 26.529 1.669e-05 *** ## Animal 332.31 2 37.621 8.772e-09 *** ## weight 461.56 1 104.506 3.994e-11 *** ## Residuals 128.08 29 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can then report The covariate, weight, was not significantly related to the the fatness of the animal, \\[F(1, 29) =,p=.0565 \\] and that there was a significant effect of the animal after controlling for weight of the animal, \\[F(2,28) = 37.62, p &lt; .05.\\]. Let’s now check on our marginal means. These are our group means after we have controlled for our covariate. For this we need the effects package. library(effects) ## Loading required package: carData ## ## Attaching package: &#39;carData&#39; ## The following objects are masked from &#39;package:car&#39;: ## ## Guyer, UN, Vocab ## lattice theme set by effectsTheme() ## See ?effectsTheme for details. mean.sheep &lt;- effect(&quot;Animal&quot;, sheepANCOVA, se=TRUE) summary(mean.sheep) ## ## Animal effect ## Animal ## Ewe Ram Wether ## 18.90219 10.53996 14.83058 ## ## Lower 95 Percent Confidence Limits ## Animal ## Ewe Ram Wether ## 17.565871 9.183322 13.532447 ## ## Upper 95 Percent Confidence Limits ## Animal ## Ewe Ram Wether ## 20.23851 11.89660 16.12870 We can then note the estimated marginal means here (18.9 for Ewe, 10.53 for Ram, and 14.83 for Wether) are mean values for fatness once the CV of weight has been controlled for. Lastly, let’s test the homogenaity of the regression slopes assumption (though we should have done this first!). We do this by running the model with an interaction term included. hoRS.sheep &lt;- aov(fatness ~ weight + Animal + animal:weight, data=deadsheep) Anova(hoRS.sheep, type=&quot;III&quot;) ## Anova Table (Type III tests) ## ## Response: fatness ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 64.571 1 14.6355 0.0007007 *** ## weight 180.144 1 40.8308 7.624e-07 *** ## Animal 5.293 2 0.5998 0.5560672 ## weight:animal 8.957 2 1.0151 0.3757923 ## Residuals 119.123 27 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The interaction term is not significant, thus no violation. We can also plot this to see it. All regression lines should be pointing in the same direction when we plot the groups separately. library(ggplot2) ggplot(deadsheep, aes(x = weight, y = fatness, color = Animal)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + labs(title = &quot;Homogeneity of Regression Slopes&quot;, x = &quot;Weight&quot;, y = &quot;Fatness&quot;) If one of these lines would be pointed downwards, we would have an interaction. "],
["manova.html", "Chapter 24 MANOVA", " Chapter 24 MANOVA Template file "],
["repeated-measures-anova-1.html", "Chapter 25 Repeated Measures ANOVA", " Chapter 25 Repeated Measures ANOVA Template file "],
["factor-analysis.html", "Chapter 26 Factor Analysis", " Chapter 26 Factor Analysis Template file "],
["mixed-effects-models.html", "Chapter 27 Mixed Effects Models", " Chapter 27 Mixed Effects Models Template file "],
["confirmatory-factor-analysis.html", "Chapter 28 Confirmatory Factor Analysis", " Chapter 28 Confirmatory Factor Analysis Template file "],
["this-is-a-template-file.html", "Chapter 29 This is a template file", " Chapter 29 This is a template file Template file "],
["references.html", "References", " References "]
]
