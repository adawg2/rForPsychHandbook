[
["index.html", "R for Psych Handbook Chapter 1 Preface", " R for Psych Handbook David John Baker 2018-04-12 Chapter 1 Preface Welcome to the R For Psychologists Handbook!. This book serves as a collection of resources used in the Department of Psychology at Louisiana State University (LSU) for the statistics classes required of all Graduate Student. The book is not meant to serve as a comprehensive statistics textbook, for that please check out either Explaining Psychological Statistics or Andy Field’s Discovering Statistics with R. The book is meant to be a reference manual that gives a brief overview of a concept, then the code needed to run that analysis in R. I try to add in references in each of the chapters in order for anyone to explore the concepts at length, but the material in this handbook should be enough to get any psychologist through the 4111 Intermediate Statistics courses and the 7111 Multivariate Statistics courses. The majority of the content comes from having taken and then TA’ed for both of these courses and collecting a lot of material on my own. The content was generated by both Jason Hicks and Jason Harman, I just provided R as the glue to hold it all together. The idea behind the book is that anyone going through the LSU stats rotation would be able to put in the extra work and learn R while they are doing the assigments in SPSS. As noted in next chapter, it’s hard to learn R, but worth doing it. Personally found that R has opened up more oppertunities than anything else I have invested in. You don’t need any programming experience to start and if you feel like you can’t do this, know that reason I started R is I knew so little about computers during my Masters that my adivsor had to show me how to do an IFELSE() statement in Excel. At that point figured if I was going to learn, might as well be frustrated and get something good out of it. One thing to learn a lot by running these examples, though the best way forward is to find your own dataset and start to set up project with it. The book is still under construction and if you would like to help out with it, please get in touch! Beauty of R is that you are trying to become a researcher. R is cullinary school of statistics. SPSS is a microwave. Saw that on twitter. Let’s get cooking. "],
["intro.html", "Chapter 2 Introduction to R 2.1 Getting Started 2.2 Setting The Working Directory 2.3 The Basics 2.4 R as Calculator 2.5 Data Exploration 2.6 Indexing 2.7 Whirlwind Tour of R 2.8 Functions for Psychologists 2.9 Resources", " Chapter 2 Introduction to R Before starting out, it’s worth mentioning that R has a steep learning curve compared to other statistical softwares. While there are tons of blog posts as to why you should learn R, I will keep my list quick so if you get discouraged at any point, you can come back to this list and get reinspired before R starts paying you back. The R community is fantastic, check out #rstats on Twitter as well as everyone affiliated with the TidyVerse R will always be free because the people behind it believe in open source principles . Time spent learning R is time spent learning how computers work. If you learn about R, you are also learning computer programming. Time spent in something like SPSS or SAS does not easily tranfer to other programs. On r-jobs.com the way they decide to split jobs is jobs that make above and below $100,000. R is your ticket out of academia, if you need it. It’s also insane to think people would learn so much about statistics, the hardest part about becoming a data scientist, without learning the software to get you in the door. When you make analyses and graphs in R they are very easy to reproduce. You just press ‘Run’ again. If you do your data cleaning in R, then each step is documented. There is less chance for human error. It makes gorgeous graphs. There are a lot of ways that R intergrates into other software. This book is written in bookdown, my website is written in blogdown, you can also make interactive data applications. 2.1 Getting Started Since this book is about statistics and R, the introduction to all things R is a bit shorter than other guides. If you need help, find a friend (or email the author of this book) and they will get you started. The first things you need to do is download R from CRAN, then get the latest version of RStudio. RStudio is an IDE that makes working in R a lot easier. If you use R without RStudio, you are basically a masochist. Once you have that, you can start to play around with this. 2.2 Setting The Working Directory At the end a long writing session you normally have to find a place to save that journal article you are working on. You do this by clicking ‘Save As’ then finding where to stick it and what to call it. If you are programming you need to do this entire act of picking whre you are working right away by saving your script (the what) and setting your working directory (the where). This is the most tedious part of starting to learn R, but once you are over this hump, the others will not feel as bad. While these instructions below are meant to be comprehensive, it’s much easier to learn this part of the process with someone walking you through the steps. [WORKING DIRECTORY SET UP GUIDE] 2.3 The Basics With this script set up, we can now start to play around with R! This chapter covers a few basic things: The rationale behind to writing scripts and code R as Calculator Data Structures Manipulating Data Tons of functions Whirlwind tour of R for psychologists The goals of this chapter are to: Learn Basics of R and RStudio Practice concepts with simple examples Familiarize with vast array of functions Have vocabulary and “R” way to think about problem solving 2.4 R as Calculator The Console of R is where all the action happens. You can use it just like you would use a calculator. Try to do some basic math operations in it. 2 + 2 ## [1] 4 5 - 2 ## [1] 3 10 / 4 ## [1] 2.5 9 * 200 ## [1] 1800 sqrt(81) ## [1] 9 10 &gt; 4 ## [1] TRUE 2 &lt; -1 ## [1] FALSE You don’t always want to print your output and retype it in. Idea is to be very lazy (efficient). Save some math to an object with the &lt;- operator, then manipulate that. foo &lt;- 2 * 3 foo * 6 ## [1] 36 Notice what has popped up in your environment in RStudio! Let’s get more efficient. yearsInGradSchool &lt;- c(2,1,4,5,6,7,3,2,4,5,3) talk about the c function. yearsInGradSchool * 3 ## [1] 6 3 12 15 18 21 9 6 12 15 9 Or this yearsInGradSchool - 2 ## [1] 0 -1 2 3 4 5 1 0 2 3 1 yearsInGradSchool &lt; 2 ## [1] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE mean(yearsInGradSchool) ## [1] 3.818182 sd(yearsInGradSchool) ## [1] 1.834022 hist(yearsInGradSchool) scale(yearsInGradSchool) ## [,1] ## [1,] -0.99136319 ## [2,] -1.53661295 ## [3,] 0.09913632 ## [4,] 0.64438608 ## [5,] 1.18963583 ## [6,] 1.73488559 ## [7,] -0.44611344 ## [8,] -0.99136319 ## [9,] 0.09913632 ## [10,] 0.64438608 ## [11,] -0.44611344 ## attr(,&quot;scaled:center&quot;) ## [1] 3.818182 ## attr(,&quot;scaled:scale&quot;) ## [1] 1.834022 range(yearsInGradSchool) ## [1] 1 7 min(yearsInGradSchool) ## [1] 1 class(yearsInGradSchool) ## [1] &quot;numeric&quot; str(yearsInGradSchool) ## num [1:11] 2 1 4 5 6 7 3 2 4 5 ... summary(yearsInGradSchool) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 2.500 4.000 3.818 5.000 7.000 yearsInGradSchool &lt;- c(2,1,4,5,6,7,3,2,4,5,3) classesTaken &lt;- c(5,2,5,7,9,9,2,8,4,7,2) gradData &lt;- data.frame(yearsInGradSchool,classesTaken) cor(yearsInGradSchool,classesTaken) ## [1] 0.6763509 Basic plots, arguments. ggplot and libraries plot(yearsInGradSchool,classesTaken, data = gradData, main = &quot;My Plot&quot;, xlab = &quot;Years in Grad School&quot;, ylab = &quot;Classes Taken&quot;) ## Warning in plot.window(...): &quot;data&quot; is not a graphical parameter ## Warning in plot.xy(xy, type, ...): &quot;data&quot; is not a graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): &quot;data&quot; is not ## a graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): &quot;data&quot; is not ## a graphical parameter ## Warning in box(...): &quot;data&quot; is not a graphical parameter ## Warning in title(...): &quot;data&quot; is not a graphical parameter 2.5 Data Exploration str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... class(iris) ## [1] &quot;data.frame&quot; summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## Accessing individual ‘columns’ is done with the $ operator iris$Sepal.Length ## [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 ## [18] 5.1 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 ## [35] 4.9 5.0 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 ## [52] 6.4 6.9 5.5 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 ## [69] 6.2 5.6 5.9 6.1 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 ## [86] 6.0 6.7 6.3 5.6 5.5 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 ## [103] 7.1 6.3 6.5 7.6 4.9 7.3 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 ## [120] 6.0 6.9 5.6 7.7 6.3 6.7 7.2 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 ## [137] 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8 6.7 6.7 6.3 6.5 6.2 5.9 Can you use this to plot the different numeric values against each other? What would the follow commands do? hist(scale(iris$Sepal.Length)) iris$Sepal.Length.scale &lt;- scale(iris$Sepal.Length) 2.6 Indexing Let’s combine logical indexing with creating new objects. What do the follow commands do? Why? iris[1,1] ## [1] 5.1 iris[2,] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 2 4.9 3 1.4 0.2 setosa ## Sepal.Length.scale ## 2 -1.1392 iris[,5] ## [1] setosa setosa setosa setosa setosa setosa ## [7] setosa setosa setosa setosa setosa setosa ## [13] setosa setosa setosa setosa setosa setosa ## [19] setosa setosa setosa setosa setosa setosa ## [25] setosa setosa setosa setosa setosa setosa ## [31] setosa setosa setosa setosa setosa setosa ## [37] setosa setosa setosa setosa setosa setosa ## [43] setosa setosa setosa setosa setosa setosa ## [49] setosa setosa versicolor versicolor versicolor versicolor ## [55] versicolor versicolor versicolor versicolor versicolor versicolor ## [61] versicolor versicolor versicolor versicolor versicolor versicolor ## [67] versicolor versicolor versicolor versicolor versicolor versicolor ## [73] versicolor versicolor versicolor versicolor versicolor versicolor ## [79] versicolor versicolor versicolor versicolor versicolor versicolor ## [85] versicolor versicolor versicolor versicolor versicolor versicolor ## [91] versicolor versicolor versicolor versicolor versicolor versicolor ## [97] versicolor versicolor versicolor versicolor virginica virginica ## [103] virginica virginica virginica virginica virginica virginica ## [109] virginica virginica virginica virginica virginica virginica ## [115] virginica virginica virginica virginica virginica virginica ## [121] virginica virginica virginica virginica virginica virginica ## [127] virginica virginica virginica virginica virginica virginica ## [133] virginica virginica virginica virginica virginica virginica ## [139] virginica virginica virginica virginica virginica virginica ## [145] virginica virginica virginica virginica virginica virginica ## Levels: setosa versicolor virginica iris[iris$Sepal.Length &lt; 5,] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## 12 4.8 3.4 1.6 0.2 setosa ## 13 4.8 3.0 1.4 0.1 setosa ## 14 4.3 3.0 1.1 0.1 setosa ## 23 4.6 3.6 1.0 0.2 setosa ## 25 4.8 3.4 1.9 0.2 setosa ## 30 4.7 3.2 1.6 0.2 setosa ## 31 4.8 3.1 1.6 0.2 setosa ## 35 4.9 3.1 1.5 0.2 setosa ## 38 4.9 3.6 1.4 0.1 setosa ## 39 4.4 3.0 1.3 0.2 setosa ## 42 4.5 2.3 1.3 0.3 setosa ## 43 4.4 3.2 1.3 0.2 setosa ## 46 4.8 3.0 1.4 0.3 setosa ## 48 4.6 3.2 1.4 0.2 setosa ## 58 4.9 2.4 3.3 1.0 versicolor ## 107 4.9 2.5 4.5 1.7 virginica ## Sepal.Length.scale ## 2 -1.139200 ## 3 -1.380727 ## 4 -1.501490 ## 7 -1.501490 ## 9 -1.743017 ## 10 -1.139200 ## 12 -1.259964 ## 13 -1.259964 ## 14 -1.863780 ## 23 -1.501490 ## 25 -1.259964 ## 30 -1.380727 ## 31 -1.259964 ## 35 -1.139200 ## 38 -1.139200 ## 39 -1.743017 ## 42 -1.622254 ## 43 -1.743017 ## 46 -1.259964 ## 48 -1.501490 ## 58 -1.139200 ## 107 -1.139200 iris[,c(1:4)] ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3.0 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5.0 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## 11 5.4 3.7 1.5 0.2 ## 12 4.8 3.4 1.6 0.2 ## 13 4.8 3.0 1.4 0.1 ## 14 4.3 3.0 1.1 0.1 ## 15 5.8 4.0 1.2 0.2 ## 16 5.7 4.4 1.5 0.4 ## 17 5.4 3.9 1.3 0.4 ## 18 5.1 3.5 1.4 0.3 ## 19 5.7 3.8 1.7 0.3 ## 20 5.1 3.8 1.5 0.3 ## 21 5.4 3.4 1.7 0.2 ## 22 5.1 3.7 1.5 0.4 ## 23 4.6 3.6 1.0 0.2 ## 24 5.1 3.3 1.7 0.5 ## 25 4.8 3.4 1.9 0.2 ## 26 5.0 3.0 1.6 0.2 ## 27 5.0 3.4 1.6 0.4 ## 28 5.2 3.5 1.5 0.2 ## 29 5.2 3.4 1.4 0.2 ## 30 4.7 3.2 1.6 0.2 ## 31 4.8 3.1 1.6 0.2 ## 32 5.4 3.4 1.5 0.4 ## 33 5.2 4.1 1.5 0.1 ## 34 5.5 4.2 1.4 0.2 ## 35 4.9 3.1 1.5 0.2 ## 36 5.0 3.2 1.2 0.2 ## 37 5.5 3.5 1.3 0.2 ## 38 4.9 3.6 1.4 0.1 ## 39 4.4 3.0 1.3 0.2 ## 40 5.1 3.4 1.5 0.2 ## 41 5.0 3.5 1.3 0.3 ## 42 4.5 2.3 1.3 0.3 ## 43 4.4 3.2 1.3 0.2 ## 44 5.0 3.5 1.6 0.6 ## 45 5.1 3.8 1.9 0.4 ## 46 4.8 3.0 1.4 0.3 ## 47 5.1 3.8 1.6 0.2 ## 48 4.6 3.2 1.4 0.2 ## 49 5.3 3.7 1.5 0.2 ## 50 5.0 3.3 1.4 0.2 ## 51 7.0 3.2 4.7 1.4 ## 52 6.4 3.2 4.5 1.5 ## 53 6.9 3.1 4.9 1.5 ## 54 5.5 2.3 4.0 1.3 ## 55 6.5 2.8 4.6 1.5 ## 56 5.7 2.8 4.5 1.3 ## 57 6.3 3.3 4.7 1.6 ## 58 4.9 2.4 3.3 1.0 ## 59 6.6 2.9 4.6 1.3 ## 60 5.2 2.7 3.9 1.4 ## 61 5.0 2.0 3.5 1.0 ## 62 5.9 3.0 4.2 1.5 ## 63 6.0 2.2 4.0 1.0 ## 64 6.1 2.9 4.7 1.4 ## 65 5.6 2.9 3.6 1.3 ## 66 6.7 3.1 4.4 1.4 ## 67 5.6 3.0 4.5 1.5 ## 68 5.8 2.7 4.1 1.0 ## 69 6.2 2.2 4.5 1.5 ## 70 5.6 2.5 3.9 1.1 ## 71 5.9 3.2 4.8 1.8 ## 72 6.1 2.8 4.0 1.3 ## 73 6.3 2.5 4.9 1.5 ## 74 6.1 2.8 4.7 1.2 ## 75 6.4 2.9 4.3 1.3 ## 76 6.6 3.0 4.4 1.4 ## 77 6.8 2.8 4.8 1.4 ## 78 6.7 3.0 5.0 1.7 ## 79 6.0 2.9 4.5 1.5 ## 80 5.7 2.6 3.5 1.0 ## 81 5.5 2.4 3.8 1.1 ## 82 5.5 2.4 3.7 1.0 ## 83 5.8 2.7 3.9 1.2 ## 84 6.0 2.7 5.1 1.6 ## 85 5.4 3.0 4.5 1.5 ## 86 6.0 3.4 4.5 1.6 ## 87 6.7 3.1 4.7 1.5 ## 88 6.3 2.3 4.4 1.3 ## 89 5.6 3.0 4.1 1.3 ## 90 5.5 2.5 4.0 1.3 ## 91 5.5 2.6 4.4 1.2 ## 92 6.1 3.0 4.6 1.4 ## 93 5.8 2.6 4.0 1.2 ## 94 5.0 2.3 3.3 1.0 ## 95 5.6 2.7 4.2 1.3 ## 96 5.7 3.0 4.2 1.2 ## 97 5.7 2.9 4.2 1.3 ## 98 6.2 2.9 4.3 1.3 ## 99 5.1 2.5 3.0 1.1 ## 100 5.7 2.8 4.1 1.3 ## 101 6.3 3.3 6.0 2.5 ## 102 5.8 2.7 5.1 1.9 ## 103 7.1 3.0 5.9 2.1 ## 104 6.3 2.9 5.6 1.8 ## 105 6.5 3.0 5.8 2.2 ## 106 7.6 3.0 6.6 2.1 ## 107 4.9 2.5 4.5 1.7 ## 108 7.3 2.9 6.3 1.8 ## 109 6.7 2.5 5.8 1.8 ## 110 7.2 3.6 6.1 2.5 ## 111 6.5 3.2 5.1 2.0 ## 112 6.4 2.7 5.3 1.9 ## 113 6.8 3.0 5.5 2.1 ## 114 5.7 2.5 5.0 2.0 ## 115 5.8 2.8 5.1 2.4 ## 116 6.4 3.2 5.3 2.3 ## 117 6.5 3.0 5.5 1.8 ## 118 7.7 3.8 6.7 2.2 ## 119 7.7 2.6 6.9 2.3 ## 120 6.0 2.2 5.0 1.5 ## 121 6.9 3.2 5.7 2.3 ## 122 5.6 2.8 4.9 2.0 ## 123 7.7 2.8 6.7 2.0 ## 124 6.3 2.7 4.9 1.8 ## 125 6.7 3.3 5.7 2.1 ## 126 7.2 3.2 6.0 1.8 ## 127 6.2 2.8 4.8 1.8 ## 128 6.1 3.0 4.9 1.8 ## 129 6.4 2.8 5.6 2.1 ## 130 7.2 3.0 5.8 1.6 ## 131 7.4 2.8 6.1 1.9 ## 132 7.9 3.8 6.4 2.0 ## 133 6.4 2.8 5.6 2.2 ## 134 6.3 2.8 5.1 1.5 ## 135 6.1 2.6 5.6 1.4 ## 136 7.7 3.0 6.1 2.3 ## 137 6.3 3.4 5.6 2.4 ## 138 6.4 3.1 5.5 1.8 ## 139 6.0 3.0 4.8 1.8 ## 140 6.9 3.1 5.4 2.1 ## 141 6.7 3.1 5.6 2.4 ## 142 6.9 3.1 5.1 2.3 ## 143 5.8 2.7 5.1 1.9 ## 144 6.8 3.2 5.9 2.3 ## 145 6.7 3.3 5.7 2.5 ## 146 6.7 3.0 5.2 2.3 ## 147 6.3 2.5 5.0 1.9 ## 148 6.5 3.0 5.2 2.0 ## 149 6.2 3.4 5.4 2.3 ## 150 5.9 3.0 5.1 1.8 iris[c(1,2,3,4,5,6,8),c(1:3,5)] ## Sepal.Length Sepal.Width Petal.Length Species ## 1 5.1 3.5 1.4 setosa ## 2 4.9 3.0 1.4 setosa ## 3 4.7 3.2 1.3 setosa ## 4 4.6 3.1 1.5 setosa ## 5 5.0 3.6 1.4 setosa ## 6 5.4 3.9 1.7 setosa ## 8 5.0 3.4 1.5 setosa Setosas &lt;- iris[iris$Species == &quot;setosa&quot;,] This could be an entire lecture by itself!!! 2.7 Whirlwind Tour of R R’s power comes in the fact that you download packages to put on top of base R Turning Data tables into already formatted APA Latex Tables (stargazer, xtable) Creating Publication Quality Graphs (ggplot2) Text manipulation (stringr) Exploring data and not making chart after chart after chart (psych) Every statistical test you could want (psych, cars, ezanova, lavaan) Software to plot not so normal output (SEMplots) Making Websites in R These slides were written in R Quickly processing huge datasets (data.table, dplyr) Tons of Machine Learning library(ggplot2) ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species), xlab = &quot;Sepal Length&quot;, ylab = &quot;Sepal Width&quot;, main = &quot;My Plot&quot;) + geom_point() Or stuff for data exploration library(psych) ## ## Attaching package: &#39;psych&#39; ## The following objects are masked from &#39;package:ggplot2&#39;: ## ## %+%, alpha pairs.panels(iris) 2.8 Functions for Psychologists nlme() and lme4() for Multilevel Modeling lavaan() for Latent Variable Analysis ezAnova() for ANOVA based testing; the anova() function does model comparisons profileR for Repeated Measures MANOVA glm() and lm() for linear models caret() for Machine Learning 2.9 Resources LINK THESE IN swirl stackoverflow.com Twitter Your peers R Community is fantastic (tidyverse!!!) 2.9.1 Template Stuff You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species Sepal.Length.scale 5.1 3.5 1.4 0.2 setosa -0.89767388 4.9 3.0 1.4 0.2 setosa -1.13920048 4.7 3.2 1.3 0.2 setosa -1.38072709 4.6 3.1 1.5 0.2 setosa -1.50149039 5.0 3.6 1.4 0.2 setosa -1.01843718 5.4 3.9 1.7 0.4 setosa -0.53538397 4.6 3.4 1.4 0.3 setosa -1.50149039 5.0 3.4 1.5 0.2 setosa -1.01843718 4.4 2.9 1.4 0.2 setosa -1.74301699 4.9 3.1 1.5 0.1 setosa -1.13920048 5.4 3.7 1.5 0.2 setosa -0.53538397 4.8 3.4 1.6 0.2 setosa -1.25996379 4.8 3.0 1.4 0.1 setosa -1.25996379 4.3 3.0 1.1 0.1 setosa -1.86378030 5.8 4.0 1.2 0.2 setosa -0.05233076 5.7 4.4 1.5 0.4 setosa -0.17309407 5.4 3.9 1.3 0.4 setosa -0.53538397 5.1 3.5 1.4 0.3 setosa -0.89767388 5.7 3.8 1.7 0.3 setosa -0.17309407 5.1 3.8 1.5 0.3 setosa -0.89767388 You can write citations, too. For example, we are using the bookdown package (Xie 2017) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["data-manipulation-in-r.html", "Chapter 3 Data Manipulation in R", " Chapter 3 Data Manipulation in R # #====================================================================================================== # # LSUserRs Data Cleaning Template (Title Your Script) # # Say a couple of the things that it does here. Who wrote it? # # When was the last edit? What does it do? Does it work with any data type? # # Rubber duck this as much as possible because you won&#39;t remember # # what you did in 6 months. Especially if you come up with something clever. # #====================================================================================================== # # TRY YOUR BEST TO NOT JUST COPY AND PASTE CODE, GOOGLE WHAT YOU WANT, GET FAMILIAR WITH A FUNCTION&#39;S # # ARGUMENTS AND EMBRACE YOUR INNER NERD AND READ THE DOCUMENTATION!! # #====================================================================================================== # # Import Libraries # #-------------------------------------------------- # # Load all libraries at the start of your script, remember they have to be installed first! # library(stringr) # library(data.table) # library(psych) # # #====================================================================================================== # # Set up your working directory # #-------------------------------------------------- # # #====================================================================================================== # # Load in your data # #-------------------------------------------------- # # After telling R where to look in your computer/dropbox/google drive/R Project grab what you need! # # Make sure to load in both datasets as we will want them both in our analysis. # # We also want to make sure to clean both datasets as we are going. # experiment.data &lt;- read.csv(&quot;datasets/Demographic_Data.csv&quot;) # item.level.data &lt;- read.csv(&quot;datasets/ItemLevelData.csv&quot;) # # # #====================================================================================================== # # Inspect the structure of your data # #-------------------------------------------------- # # R is going to do its best to guess what kind of data each of the columns of your spreadsheet are. # # Sometimes it thinks things are lists (especially if importing from SPSS) # # Go through each variable as you would with other programs and make sure you set it to what # # you think will be most useful later. The big thing to check here is categorical variables, and # # if you see any sort of string/character variables. # # # View(experiment.data) # Looks OK on surface levels.... # str(experiment.data) # Check to see if R guessed correctly on data types # # # Using read.csv() R had a couple of bad guesses on variables we might need. # # We will have to reassign the variable types or else we&#39;ll run into trouble later. # # In R we use Factor for grouping and analysis, best practice is to not set it as that # # until you are OK with the format. It&#39;s easist to manipulate a character or string. # # experiment.data$inst &lt;- as.character(experiment.data$inst) # experiment.data$Gender &lt;- as.character(experiment.data$Gender) # experiment.data$Major &lt;- as.character(experiment.data$Major) # experiment.data$Minor &lt;- as.character(experiment.data$Minor) # experiment.data$BeginTrain &lt;- as.character(experiment.data$BeginTrain) # experiment.data$AbsPitch &lt;- as.character(experiment.data$AbsPitch) # experiment.data$Live12Mo &lt;- as.numeric(experiment.data$Live12Mo) # experiment.data$ActListenMin &lt;- as.character(experiment.data$ActListenMin) # # str(experiment.data) # Notice how that our character columns now have &quot; &quot; around them. # # # #====================================================================================================== # # Check for Import Errors # #-------------------------------------------------- # # Use a combination of the names(), View(), table(), is.na(), and complete.cases() to get a brief summary of what is # # going on in your data set to be sure there were minimal import errors and your data looks like # # you want it to. It might also be worth plottting some variables and use some common sense to find mistakes. # # Are there any participants with 999 as their subject number? Negative values where there shouldn&#39;t be? # # If there are, note them and fix these before starting any sort of statistical screening! # # table(complete.cases(experiment.data)) # Not all observations have everything! # complete.cases(experiment.data) # table(is.na(experiment.data)) # # # Gotta decide what to do about it!! # # #====================================================================================================== # # Cleaning Free Text Response Data # #-------------------------------------------------- # # In your data cleaning before you might have noticed that participants were able to freely respond # # with whatever gender they wanted. Most data look to fall within the normal binary, but the computer # # needs things to be exactly the same before making an easy split? # # What would be the laziest, most effecient way to fix the gender column? What format does the variable # # have to be in order to make the changes that you need? # # When you have it figured out, make sure to run the code from top to bottom to make sure things go in # # the right order!!! As we are not dealing with huge amounts of data, the table() function will help out. # # # Let&#39;s now take a look at some of these problem ones # # Why, for example is Begin Training not working? Print the variable to see. # # experiment.data$BeginTrain # # # Some people didn&#39;t respond, one person decided to tell us what grade they started. # # There are 2 ways to go about fixing this. We could &quot;hardcode&quot; the problem if this # # is the only time we will do this analysis on this dataset or we could try to write a # # line of code that doesn&#39;t care what exact position the error is. # # On line 250 in this object is the thing that needs swapping out. # # We can access it with R&#39;s indexing. Counting from index we see it&#39;s in line 250. # # experiment.data$BeginTrain[250] # # # Quick, ask yourself why we don&#39;t use the comma here?! # # If you were set on using the comma, what would you change? # # Ok, now let&#39;s swap in the value we want with &lt;- # # Remember we are putting a value into a character operator so it has to have &quot;&quot; # # experiment.data$BeginTrain[250] &lt;- &quot;12&quot; # experiment.data$BeginTrain # # # Nice, no more text data, but what if it&#39;s not always in 250? # # For example, what do we do with all these blank spaces? # # Let&#39;s use R&#39;s inbuilt ifelse() function to go through this vector and swap # # out what we want! # # ifelse(experiment.data$BeginTrain == &quot;&quot;,&quot;0&quot;,experiment.data$BeginTrain) # # # This works by going through each entry and doing the conditional on the value! # # Let&#39;s now write over our old column and in the same step make everything a number. # experiment.data$BeginTrain &lt;- as.numeric(ifelse(experiment.data$BeginTrain == &quot;&quot;,&quot;0&quot;,experiment.data$BeginTrain)) # # #Tah Dah!! # experiment.data$BeginTrain # # # Let&#39;s now clean up the Gender column, first let&#39;s look at it # experiment.data$Gender # Are there any common trends? # # table(experiment.data$Gender) # # # Pretty much two answers, how do we make them all say one thing? # # Let&#39;s use the stringr package for this. Import it up top. # # # Clean Gender # # experiment.data$Gender &lt;- str_to_lower(experiment.data$Gender) # experiment.data$Gender &lt;- str_replace(experiment.data$Gender,&quot;^.*f.*$&quot;,&quot;Female&quot;) # experiment.data$Gender &lt;- str_replace(experiment.data$Gender,&quot;^m.*$&quot;,&quot;Male&quot;) # experiment.data$Gender &lt;- str_replace(experiment.data$Gender,&quot;^country$&quot;,&quot;No Response&quot;) # experiment.data$Gender &lt;- str_replace(experiment.data$Gender,&quot;&quot;,&quot;No Response&quot;) # experiment.data$Gender[30] &lt;- &quot;No Response&quot; #Something Might Be Up w this datapoint? # # experiment.data$Gender &lt;- as.factor(experiment.data$Gender) # # #-------------------------------------------------- # # Can we do same thing for AP? # experiment.data$AbsPitch # experiment.data$AbsPitch &lt;- str_to_lower(experiment.data$AbsPitch) # experiment.data$AbsPitch &lt;- str_replace(experiment.data$AbsPitch,&quot;^.*n.*$&quot;,&quot;no&quot;) # experiment.data$AbsPitch[30] &lt;- &quot;no&quot; # experiment.data$AbsPitch &lt;- as.factor(experiment.data$AbsPitch) # table(experiment.data$AbsPitch) # # # #====================================================================================================== # # Merging Data # #-------------------------------------------------- # # Often we will have data from other spreadsheets we want to attach such as demographic data # # to behavioral responses. Using the data.table functionality, let&#39;s merge our two csv # # files together so that we have every variable accessible to us for this analysis. # # Note I like to work with the data.table package, though there are other ways to do this! # # # In order to do this, we need 1 shared column between the two datasets. # # For most psychology cases, this is probably going to be a participant ID number. # # Note that for this to work, you need the columns to have an exact match of name! # # First let&#39;s check that they are the same!! # names(experiment.data) # names(item.level.data) # # # First off our subject ID columns are not the same. Let&#39;s swap that. # setnames(item.level.data,&quot;tmp.dat.subject.1.&quot;,&quot;SubjectNo&quot;) # setnames(experiment.data,&quot;Sub&quot;,&quot;SubjectNo&quot;) # Make this clearer!!! # # # If you need to do more than 1, use the c() operator! # # Now if we look at this column, it&#39;s all messe up. # # The code below fixes it, if you want to learn more about regex, check it out # # if not, just skip below. # # item.level.data$SubjectNo &lt;- str_replace_all(string = item.level.data$SubjectNo, pattern = &quot;.csv&quot;, replacement = &quot;&quot;) # item.level.data$SubjectNo &lt;- str_replace_all(string = item.level.data$SubjectNo, pattern = &quot;C&quot;, replacement = &quot;&quot;) # item.level.data$SubjectNo &lt;- str_replace_all(string = item.level.data$SubjectNo, pattern = &quot;M&quot;, replacement = &quot;&quot;) # item.level.data$SubjectNo &lt;- str_replace_all(string = item.level.data$SubjectNo, pattern = &quot;CM&quot;, replacement = &quot;&quot;) # item.level.data$SubjectNo &lt;- as.numeric(item.level.data$SubjectNo) # # # Let&#39;s just quickly check to see if all the subject numbers make sense # hist(experiment.data$SubjectNo) # Cause for alarm! Negative values and placeholders! # # # Drop those # experiment.data &lt;- experiment.data[experiment.data$SubjectNo &gt; 0 &amp; experiment.data$SubjectNo &lt; 1000,] # # # Note this works because the SubjectNo variable is numeric # hist(experiment.data$SubjectNo) # hist(item.level.data$SubjectNo) # # # Ok, finally we merge our datasets. What we are doing here is called an &quot;inner join&quot; # # Here we willkeep all of the ROWS of the dataset in the middle of the command # # Note we need to swap over our key to be a character value. # item.level.data &lt;- data.table(item.level.data) # experiment.data &lt;- data.table(experiment.data) # # item.level.data # # exp.data &lt;- item.level.data[experiment.data, on=&quot;SubjectNo&quot;] # # exp.data # # # View(exp.data) # Use View to hover over column number # # # Let&#39;s reorganize our columns so individual stuff is at the back # # We could do this with data.table, but it&#39;s a different syntax so let&#39;s swap back # # Normally you try to stick to minimal switching, but we&#39;re just taking # # a big tour du R right now and learning to think # exp.data &lt;- data.frame(exp.data) # exp.data &lt;- exp.data[,c(1,40:100,2:39)] # View(exp.data) # # #====================================================================================================== # # Checking for Univariate Outliers # #-------------------------------------------------- # # For this example, let&#39;s imagine a univariate outlier is one with a zscore # # greater than 3. While we could write a bit of code to look for this, let&#39;s use # # the pairs.panels() function in the psych pacakge to just get used to looking at our data # # The function is not the biggest fan of huge datasets, so let&#39;s index our # # dataset to only grab what we need. Try to change the values and look # # at variables of interest. # # pairs.panels(exp.data[,2:7], lm = TRUE) # # # But of course we need to look at numbers in terms of their zscores! # # Let&#39;s first standardize our entire dataset using the apply function # # Note we only can do this on numeric values! # # # The apply function takes 3 argument # # The first is what you want to manipulate, the second is if it&#39;s rows 1 or columns 2 # # (remeber this because it&#39;s always rows then columns!), and the function. # # You can even write your own (though we&#39;ll get to functions later) # gmsi.z.scores &lt;- apply(exp.data[2:7],2,scale) # # exp.data.with.z &lt;- cbind(exp.data, gmsi.z.scores) # # # Now we can index this to find values above whatever theshold we want! # # table(gmsi.z.scores &gt; 2) # gmsi.z.indexer &lt;- gmsi.z.scores &gt; 2 # gmsi.z.scores[gmsi.z.indexer] # See what they are, find them , decide to get rid of # # # #====================================================================================================== # # Checking for Multivariate Outliers # #-------------------------------------------------- # # A bit tricker, I leanred how to do this off a blog post. # gmsi.responses &lt;- exp.data[,c(63:100)] # # mahal &lt;- mahalanobis(gmsi.responses, # colMeans(gmsi.responses, na.rm = TRUE), # cov(gmsi.responses, # use = &quot;pairwise.complete.obs&quot;)) ## Create Distance Measures # # cutoff &lt;- qchisq(.999, ncol(gmsi.responses)) ## Create cutoff object .001 signifiance and DF = obs # summary(mahal &lt; cutoff) ## 11 Subjects greater than 70 cutoff # # # Add On Variables # exp.data$mahal &lt;- mahal # exp.data &lt;- data.table(exp.data) # To use easier indexer, needs data.table # exp.data[exp.data$mahal &lt; cutoff] # # #====================================================================================================== # # Checking for Skew and Kurtosis # #-------------------------------------------------- # apply(gmsi.responses, 2, skew) # apply(gmsi.responses, 2 , kurtosi) # # #====================================================================================================== # # Exporting Data # #-------------------------------------------------- # # It&#39;s best practice to separate your cleaning and your analysis into separate scripts. # # Export the dataset you have into a new csv file into a directory that would make sense to # # someone who has never seen your project before. # # write.csv(exp.data,&quot;My_Experiment_Data.csv&quot;) # #====================================================================================================== "],
["epistemology-of-statistics.html", "Chapter 4 Epistemology of Statistics", " Chapter 4 Epistemology of Statistics We describe our methods in this chapter. "],
["descriptive-statistics-z-scores-central-limit.html", "Chapter 5 Descriptive Statistics, z Scores, Central Limit 5.1 Descriptive Statistics and the Normal Distribution 5.2 Practice", " Chapter 5 Descriptive Statistics, z Scores, Central Limit 5.1 Descriptive Statistics and the Normal Distribution 5.1.1 Organizing Data Descriptive statistics are traditionally used to summarize data from observed samples. Most often, sample data are organized into distributions of information based on ascending scores. For example, we might have a table with some SAT-Verbal scores from a few different students. Before going on to think about this, also take note that the shape of this data (though very minimal) is in the tidy format. According to Hadley Wickham on the tidyr CRAN page for data to be tidy it must have the following properties: Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. This isn’t very important right now, but once you get to more complex designs, it will be good to have had thought about this before. student &lt;- c(1,2,3,4,5,6) SAT &lt;- c(480,530,560,650,720,760) satData &lt;- data.frame(student,SAT) satData ## student SAT ## 1 1 480 ## 2 2 530 ## 3 3 560 ## 4 4 650 ## 5 5 720 ## 6 6 760 5.1.2 Shape of Data When visualized, data can take on a variety of shapes. Below are a few of the shapes you might come across when analyzing data. The first, and probably least likely distribution you will find is the uniform or rectangular distribution. We can create this plot and the others by using the distribution functions from R’s functionality. In each case we are going to take 1,000 samples from 0 to 1. We’ll plot everything using ggplot2 so we can also get used to using it for our packages. library(ggplot2) set.seed(666) uniformData &lt;- runif(n=1000, min=0, max=100) distributions &lt;- data.frame(uniformData) # Make variable to remove ggplot elements cleanUpPlots &lt;- theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) ggplot(distributions, aes(distributions$uniformData)) + geom_histogram(binwidth = 5) + labs(x = &quot;Independent Variable&quot;, y = &quot;Frequency&quot;, title = &quot;Uniform Distribution&quot;) + cleanUpPlots Try to run the above code with different arguments in the binwidth argument. You’ll notice that the way you plot the data will actually represent it differntly. We can also have positively skewed and negatively skewed distributions. If a distribution is skewed, it usually means that the mode does not equal the mean. We’re going to approximate both of these with another one of R’s probability functions. positiveSkewData &lt;- rchisq(1000,6) # Chi Square distributions are positively skewed negativeSkewData &lt;- - positiveSkewData # Flip it! distributions &lt;- data.frame(uniformData, positiveSkewData, negativeSkewData) ggplot(distributions, aes(distributions$positiveSkewData)) + geom_histogram(binwidth = 1) + labs(x = &quot;Independent Variable&quot;, y = &quot;Frequency&quot;, title = &quot;Positive Skew Distribution&quot;) + cleanUpPlots ggplot(distributions, aes(distributions$negativeSkewData)) + geom_histogram(binwidth = 1) + labs(x = &quot;Independent Variable&quot;, y = &quot;Frequency&quot;, title = &quot;Negative Skew Distribution&quot;) + cleanUpPlots The most important distribution in the world of Frequentist statistics is a normal distribution. A normal distribution is defined by THIS HERE. normalData &lt;- rnorm(n = 1000,mean = 0,sd = 2) # Flip it! distributions &lt;- data.frame(uniformData, positiveSkewData, negativeSkewData, normalData) ggplot(distributions, aes(distributions$normalData)) + geom_histogram(binwidth = 1) + labs(x = &quot;Independent Variable&quot;, y = &quot;Frequency&quot;, title = &quot;Normal Distribution&quot;) + cleanUpPlots And the lastly we can have both leptokurtic and platykurtic distributions. leptoData &lt;- rnorm(n = 1000,mean = 0,sd = 2) platyData &lt;- rnorm(n = 1000,mean = 0,sd = 2) distributions &lt;- data.frame(uniformData, positiveSkewData, negativeSkewData, normalData,leptoData, platyData) ggplot(distributions, aes(distributions$leptoData)) + geom_histogram(binwidth = 1) + labs(x = &quot;Independent Variable&quot;, y = &quot;Frequency&quot;, title = &quot;Leptokurtic Distribution, FIX ME&quot;) + cleanUpPlots ggplot(distributions, aes(distributions$platyData)) + geom_histogram(binwidth = 1) + labs(x = &quot;Independent Variable&quot;, y = &quot;Frequency&quot;, title = &quot;Platykurtic Distribution, FIX ME&quot;) + cleanUpPlots Generally measurses of central tendency are used to characterize the most typical score in a distribution. For example we could calculate the mean or the median of our SAT data. The mean is calculated by adding up all our numbers, designated with the Greek letter Sigma \\(\\Sigma\\) then dividing by the amount of numbers we added up, or \\(n\\). As an equation it would look like this. \\[\\bar{X} = \\frac{(\\Sigma\\ x_i)}{n}\\] Take a second to talk yourself through that so later you will start to feel more comfortable with more complex equational notation. Some people find it helpful to just try to say the equation in plain English. In this case it would be the mean, or \\(\\bar{x}\\) is defined as or equal to what happens when you add up \\(\\Sigma\\) every single value \\(x\\) that you have going up to \\(i\\), then divide all those numbers by the amount of numbers you have, or \\(n\\). The median is defined by finding the middle number. If there is a tie because we have an even set of numbers, we take the mean of the middle two numbers. We can do both of these in R as well. Below we can either type in the numbers as you would with a calculator, or use a function in R. Notice that each step when its typed out is saved into an object. By starting to think this way, it will pave the way for writing more elegant code later on. # Typing it out our.data &lt;- c(480 + 530 + 560 + 650 + 720 + 760) how.many &lt;- length(our.data) our.data/how.many ## [1] 3700 # Inbuilt functions mean(satData$SAT) ## [1] 616.6667 median(satData$SAT) ## [1] 605 Notice here that for adding up the means by hand I could have done what programmers call hard coded the equation in. That would have looked like this. our.answer &lt;- c(480 + 530 + 560 + 650 + 720 + 760) / 6 our.answer ## [1] 616.6667 The problem with this, is that every time you get a new SAT score you have to both enter the score and update how many scores you are dividing by. Whenever you see a chance to take a shortcut like this, do it! It will save you tons of time in the future. 5.1.3 Important Considerations for Central Tendency There are three big considerations to think about when choosing numbers to represent your data. The mode is the most variable from sample to sample; the mean is the least variable The mean is the most sensitive to extreme scores; e.g., skewed distributions The mean is the most frequently used measure because The sum of the deviations around the mean is 0 The sum of the squared deviations is the smallest around the mean, rather than the mode or median; this is known as the least squares principle 5.1.4 Measures of Variability The range is simply the largest score minus the smallest score. It is the crudest measure of variability. In our dataset we would calculate it with the following code. 760 - 480 ## [1] 280 # OR range(satData$SAT) ## [1] 480 760 max(satData$SAT) - min(satData$SAT) ## [1] 280 The interquartile rangerepresents the spread between the score at the 75th and 25th percentiles. Boxplots are often used to graphically represent inner 50% of the scores. y &lt;- satData$SAT boxplot.example &lt;- data.frame( x = 1, y0 = min(y), y25 = quantile(y, 0.25), y50 = median(y), y75 = quantile(y, 0.75), y100 = max(y) ) ggplot(boxplot.example, aes(x)) + labs(title = &quot;Example of Boxplot&quot;) + geom_boxplot(aes(ymin = y0, lower = y25, middle = y50, upper = y75, ymax = y100), stat = &quot;identity&quot;) The variance is essentially the averaged squared deviation around the mean. Now there is a very important distinction that we will get to a bit later on, but that is the difference between a population value or \\(\\sigma^2\\) and a sample variance or \\(s^2\\). In order to do frequentist statistics, we need to assume that there is some sort of True value that the group we are measuring has and it is a fundamental property of the group! For more on this see CHAPTER 3 and the work of Zoltan Dienes. Somewhere, maybe written on a golden plate in heaven is the actual value of the average weight of a labrador retriever. The problem is we will never have access to that information so we need to estimate it by using a sample. The logic is that if we can truly draw in a random way from our entire population, in this case labrador retrievers, the central limit theroum will give us a good approximation of what that True value will be. Since we want to be clear about when we are talking about the Platonic, True value and the actual sample we collected, we use different Greek notation. The \\(\\sigma^2\\) refers to the Platonic value and the \\(\\sigma^2\\) is the sample. They are defined as follows: \\[\\sigma^2 = \\frac{\\Sigma(X_i - \\mu)^2}{N}\\] \\[s^2 = \\frac{\\Sigma(X_i - \\mu)^2}{n-1}\\] Note here that each of these formulas needs a mean. In the population equation that is defined as \\(\\mu\\) and in samples we used \\(\\bar{X}\\). In our case with the SAT scores, we are wanting to know the True value of the SAT scores of whatever population our six students are theorized to come from. To do the calulations below we need to know the mean which we calcualted above to be 616.67. Now since these scores are to serve as a represntive sample in hopes of getting at the true population value we need to use the formula reflecting the sample variance or \\(s^2\\). \\[s^2 = \\frac{(480-616.7)^2 + . . . + (760 - 616.676)^2}{6-1}\\] Doing this by hand we get an \\(s^2\\) value of 12346.67. Or running it in R, we would use. var(satData$SAT) ## [1] 12346.67 The standard deviation is the square root of the variance of the sample. \\[s = \\sqrt\\frac{\\Sigma(X_i - \\mu)^2}{n-1}\\] And since we know \\(s^2\\) from above, we can shorten this to \\[s = \\sqrt{s^2} = \\sqrt{12345.67} = 111.12\\] Or run it in R and get sd(satData$SAT) ## [1] 111.1156 Standard scores represent the distance of raw scores form their mean in standard deviation units. \\[z = \\frac{x_i - \\bar{X}}{s}\\] So if we needed to find the \\(z\\) score or standardized score for someone who got a 560 on their SAT we could compute the following. \\[z_{560} = \\frac{560 - 616.67}{111.12} = -0.51\\] Interpreted in-context, this would mean that if you scored a 560 on the SAT, based on our sample (which we think helps us get at the True popuation value), you would be scoring about less than 1 standard deviation (the unit of z) below the average. 5.1.5 Properties of z Scores Z scores are defined by having having three separte properties: The standardized distribution preserves the shape of the original raw score distribution The mean of the standardized distribution is always 0 The variance &amp; standard deviation are always 1 Many of the variables in behavioral sciences are distributed normally. In addition, the basis for parametric inferential statistics is based on the normal distribution. The normal distribution is unimodal, symmetrical, bell shaped, with a maximum height at the mean. The normal distribution is continuous and additionallythe normal distribution is asymptotic to the X axis—it never actually touches the X axis and theoretically goes on to infinity. The normal distribution is really a family of distributions defined by all possible combinations of means \\(\\mu\\) and standard deviations \\(\\sigma\\). We can use the standardized normal distribution to find areas of probability under the curve. With a normal distribution, there is always a fixed area under the curve which we take the reflect the probability of getting a score when sampling from a population. For example if we go 1 z unit (1 SD) away from the mean we find 34% of the total area of the curve there. If you then extend that out to the negative side, you then encapsulate 68% of the distribution. This would translate to a scenario where if you were to get a score at random from the distribution, 68% of the time you would get a score between 1 and -1 SD units from your mean. This process can be extended as seen in the figure below. z Scores and Areas Under the Curve We could also calculate the area between two z scores as shown here. And we could also look at how much area under the distribution exists beyond two standard deviations beyond the mean. z Scores and Areas Under the Curve Or we could pick any z score values and find the area under the mean! z Scores and Areas Under the Curve 5.2 Practice We can now start to put this to use. Here are some past homework examples. During tryouts, a sample of ballet dancers were rated on their athletic ability andoverall knowledge of the art. Below are the ratings for each dancer (a score above 75 percent means that the dancer will join the troupe 83, 98, 45, 69, 52, 94, 82, 74, 71, 83, 62, 85, 90, 97, 61, 74, 74, 88 Let’s put them into R so we can use answer a few questions about our data. ballet &lt;- c(83, 98, 45, 69, 52, 94, 82, 74, 71, 83, 62, 85, 90, 97, 61, 74, 74, 88) What is the median percentage? median(ballet) ## [1] 78 What is the mean percentage? mean(ballet) ## [1] 76.77778 What is the standard deviation for the sample (assume we don’t know any population characteristics)? sd(ballet) ## [1] 15.02373 Demonstrate the least squares principle by showing that the sum of squares (SS) around the mean is smaller than the sum of squares around the median (remember to show your work for each). ballet_mean &lt;- mean(ballet) ballet_median &lt;- median(ballet) ballet_sd &lt;- sd(ballet) Now we can use these values to do our math! sum((ballet - ballet_mean)^2) ## [1] 3837.111 sum((ballet - ballet_median)^2) ## [1] 3864 # Then having R do the final work for us sum((ballet - ballet_mean)^2) &gt; sum((ballet - ballet_median)^2) ## [1] FALSE What are the standardized (z) scores for the raw scores 73, 99, and 66? If you know the population mean and the sd, you can calulate a z score using the formula \\[z = \\frac{x_i - \\bar{X}}{s}\\] Or in our case (73 - ballet_mean)/ ballet_sd ## [1] -0.2514541 (99 - ballet_mean)/ ballet_sd ## [1] 1.479142 (66 - ballet_mean)/ ballet_sd ## [1] -0.7173837 What proportion of scores exceeds a raw score of 73? pnorm(q = 73, mean = ballet_mean, sd = ballet_sd) ## [1] 0.4007315 To get the other side of the probability we can remember that we can treat the line above as an object! 1 - pnorm(q = 73, mean = ballet_mean, sd = ballet_sd) ## [1] 0.5992685 What proportion of scores lies between the raw scores of 75 and 100? Let’s be clever for this one and just put the two equations together for this one. Or if you want, you could save them into objects. pnorm(q = 100, mean = ballet_mean, sd = ballet_sd) - pnorm(q = 75, mean = ballet_mean, sd = ballet_sd) ## [1] 0.4860093 pnorm(q = 76, mean = ballet_mean, sd = ballet_sd) ## [1] 0.479356 What raw score represents the 55thpercentile? To find out what raw score represents a percentile we can go back and use the formula from above, just rearranged a bit. \\[z = \\frac{x_i - \\bar{X}}{s}\\] or with a bit of basic algerbra \\[x_i = (z * s) + \\bar{X}\\] (.05 * ballet_sd) + ballet_mean ## [1] 77.52896 Between what raw scores does the middle 60% of the distribution lie? Lastly, we then need to find first what z scores map on 30% on either side of the disribution, then convert those z scores to raw scores on our data using the z score formula. First we find the z score associated with what is 30% left and right of the mean (it will be the same number, only negative). In this case, it is +/-.84. With that established, we then first solve for x \\[-0.84 = \\frac{x - 76.78}{15.02}\\] Giving us a value of 64.1 And we do it again with the positive number. \\[0.84 = \\frac{x - 76.78}{15.02}\\] Resulting in 89.547. "],
["sampling-distributions.html", "Chapter 6 Sampling Distributions", " Chapter 6 Sampling Distributions In this chapter, we’ll cover three ideas/questions. What are inferential statistics and the logic behind them What the underlying distribution of all hypothetical sample estimates is known as the sampling distribution, and it constitutes the third of the three important distributions. Several important implications follow from an understanding of the sampling distribution as a normal distribution and from the central limit theorem Spoken about a bit before in the other chapter, we have both sample statistics like \\(\\bar{X}\\) and population parameters \\(\\mu\\). The idea of how frequentist inferential statistics is as follows. Samples must be selected randomly in order to make appropopriate inferences about the parent population. Sample estimates must be compared to an underlying distributionof estimates of all other hypothetical samples of that same sizefrom the parent population. Based on this comparison and the associated probability of obtaining certain outcomes, inferences can be made about population parameters. Sampling It’s important to note that there are three different distributions that we typically talk about. Two you should be familiar with – the populatation and the sample. The third is the sampling distribution which is a distribution of sample means. The sampling distribution of the mean is generated by considering all possible sample means of a given sample size. As is demonstratd from the image below, in A we can see there is some sort of distribution, then with one sample (notice the \\(\\bar{X}\\)), we now have one wide sample. As we increase that to \\(N = 16\\), the sampling distribution becomes more narrow. This narrowing is reflective of the idea we are coming in on the true value of the population via our random sampling. The central limit theorem states that as the sample size \\(n\\) increases, the sampling distribution of the mean for simple random samples of \\(n\\) cases, taken from a population with a mean equal to \\(\\mu\\) and a finite variance equal to \\(\\sigma^2\\), approximates a normal distribution. From this, three points follow: 1.The shape of the sampling distribution is normal 2. The mean of the sampling distribution is \\(\\mu\\) 3. The standard deviation of the sampling distribution, or standard error of the mean, is \\[\\frac{\\sigma}{\\sqrt{n}}= \\sigma_\\bar{X}\\] Several important implications follow from an understanding of the sampling distribution as a normal distribution and from the central limit theorem. Because we know the mean and standard error, we can calculate the probability of selecting a random sample mean that is at or more extreme than a particular value on the distribution. \\[z = \\frac{\\bar{X}-\\mu}{\\sigma_\\bar{X}}\\] We can appeal to the table of z scores on the standard normal distribution to find the probability. For example, consider a sampling distribution of SAT scores with a mean of 455 and a standard error of 8.33. This standard error was generated with \\(n\\)= 144 and \\(\\sigma\\)= 100. So if you wanted to find the liklihood of finding as ample mean equal to or more than extreme of 480, we would plug it into the follow equation. \\[z = \\frac{480 - 455}{8.33} = 3.00\\] And if we look that up a table of z distributions, we got a probability of \\(p=.0013\\). Sampling Because we know the mean and standard error, we can calculate the probability of selecting a random sample mean that is at or more extreme than a particular value on the distribution. As sample size (\\(n\\)) increases, the variability of the sampling distribution (\\(\\sigma_\\bar{X}\\)) decreases. Even when the parent population is not normally distributed, the sampling distribution becomes normal as sample size (n) increases. You can see this demonstrated in THIS LINK. "],
["hypothesis-testing.html", "Chapter 7 Hypothesis Testing 7.1 Steps of Hypothesis Testing 7.2 Two Sample", " Chapter 7 Hypothesis Testing The sampling distribution of the mean helps us to make hypotheses about the likelihood that a given sample mean comes from a sampling distribution with a given mean. Stated differently, a hypothesis test helps us determine whether the observed difference between a sample mean and a hypothetical population mean is either negligible or meaningful. The Null Hypothesis \\(H_o : \\mu = some value\\) The alternative Hypothesis \\(H_o : != \\mu = some value\\) The Alternative Hypothesis A sample mean of a given sample size is produced and compared to the hypothetical sampling distribution’s mean to test the null hypothesis Imagine we know the population (for some reason) and the True value is 455. True Value is 455 Descision Tree The hypothesis test is based on inference (i.e., inductive reasoning), and therefore there is a chance that mistaken inferences will be made. The 2 ×2 matrix of decision outcomes given the state of nature and the decision made Matrix A Type I erroris produced when we mistakenly reject the null. It is associated with probability alpha (\\(\\alpha\\)), or the level of significance. We conventionally set this level to be .05in psychology, but there are considerations to be made for increasing or decreasing this value (e.g., .01 or .10). A Type II erroris produced when we mistakenly fail to reject the null. It is associated with probability beta (\\(\\beta\\)), which is related to, but not the same as, alpha. The level of significance creates the bounds for the rejection region—the extreme region(s) under the sampling distribution equal to αif the null hypothesis is true. Matrix The directionality of the test should also be considered. Matrix Hypothesis tests differ slightly when population parameters, such as \\(\\sigma^2\\), are known versus unknown. The general formula for a test statistic \\(Statistic - parameter /SE\\) When \\(\\sigma^2\\)is known, we use the standard error of the normal distribution as the denominator. The result is the ztest. Z TEST FORMULA STANDARD ERROR HERE When \\(\\sigma^2\\) is unknown, we use the t distributionas our sampling distribution, with a standard error that must be estimated from \\(s^2\\) or \\(s\\). The result is the ttest. T TEST AND STANDARD ERROR FORMULA HERE Left off on Page 11 in onesampleNHST.png The concept of degrees of freedom (df)must be considered for the ttest. For each sample drawn, df= n–1. VARIANCE FORMULSA DF on t statistic 7.1 Steps of Hypothesis Testing On a standardized anagram task, \\(\\mu\\)= 26 anagrams solved with a \\(\\simga\\)= 4. A researcher tests whether the arousal from anxiety is distracting and will decrease performance. A sample of \\(n\\)= 14 anxiety patients is tested on the task. There average performance is 23.36 anagrams. Step one: State the null and alternative hypotheses $H_O = : = 26 $ $H_O = : != 26 $ Consider directionality. Step two: Set the criterion for rejecting H0. Alpha is usually set to .05, but could be other values depending on the research context. Again, directionality is important to consider. Step three: Select the sample and collect your data. Step four: Locate the region of rejection and the critical value(s) of your test statistic. Again, directionality is important to consider. Step five: Compute the appropriate test statistic. σis known, so we use the ztest. Convert me Convert me Step six: Decide whether to reject H0. Is -2.47 more extreme than the critical value? Step five: Compute the appropriate test statistic. \\(\\sigma\\) is unknown, so we use the ttest. Step six: Decide whether to reject H0. Is -3.00 more extreme than the critical value? df= 13, look up critical value in table C.3 and find ±1.77. T distribution here How do we report this result in a typical research article? “The mean number of anagrams solved by anxiety patients (M= 23.36) was significantly lower than the mean established by test norms (M= 26), t(13) = 3.00, p&lt; .05.” Sometimes you’ll find people report the pvalue lower than .01 if it passes this criterion as well. For example, t(13) = 3.00, p&lt; .01. Don’t be confused by the meaning of this, however. 7.1.1 Other important considerations. The hypothesis test is a test of the NULL hypothesis, assuming that the null is true. Thus, the test gives you the probability of your sample mean being that different (or more) from the population mean by chance IFFthe null is true. Statistical significance is not the same as practical significance. Being able to report the result of a hypothesis test statistically versus being able to describe the result to a lay person. Relate the inference back to the original research question! 7.2 Two Sample In cases where we wish to compare two sample means, the hypothesis testing logic is essentially the same as with the one-sample tests, with some slight differences in the null hypothesis, in the sampling distribution, and in the computation. When different people (or animals) contributed to the two samples, the comparison distribution that represents the null hypothesis is a sampling distribution of differences between means. The hypothesis test is therefore referred to as an independent-samples test. When both sample means were produced by the same participants, we conduct what is known as a dependent-samples test. This is a test of the average difference between the scores in one condition and the scores in another condition—thus, the unit of measurement is a difference score. Nondirectional Null Hypothesis $H_O : _1 - _2 = 0 | H_O : _1 = _2 $ Nondirectional Alternative Hypothesis $H_a : _1 - _2 != 0 | H_a : _1 = _2 $ Directional Null Hypothesis $H_O : _1 &gt; _2 | _1 &lt; _2 $ Directional Alternative Hypothesis $H_a : _1 &lt; _2 | H_a : _1 &gt; _2 $ Convert me This is basically a subtraction of one sampling distribution from another, to produce a distribution of possible differences between sampling distributions. $ _1 - _2$ * The mean of this sampling distribution is $ _1 - _2$ * The shape of this sampling distribution is approximately normal. * When 2is known for each distribution, the standard error of the difference between means is s2is considered a pooled estimateof the population variance because the individual estimates are literally summed together in the computation: \\(s^2 = \\frac{SS_1 + SS_2}{n_1 + n_2 -2}\\) If you know the individual group variances or standard deviations, then \\(s^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 -2}\\) Convert me \\(t = \\frac{(\\bar{X_1}-\\bar{X_2})-(\\bar{\\mu_1}-\\bar{\\mu_2})}{s_{\\bar{X_1}-\\bar{X_2}}}\\) Example of the independent samples ttest The instructor of an introductory psychology course is interested in knowing if there is a difference in the mean grades on the final exam between the fall and spring semester classes. Summary data for the two samples is below: Convert me Are the final exam grades for the two classes equivalent? Step one: State the null and alternative hypotheses \\(H_o:\\mu_1 = \\mu_2\\) \\(H_a:\\mu_1 != \\mu_2\\) b.Step two: Set the criterion for rejecting H0. Alpha is usually set to .05, but could be other values depending on the research context. Make sure you’ve considered directionality! c.Step three: Select the sample and collect your data. d.Step four: Locate your region of rejection and critical values. Locate your region of rejection and critical values. \\(t_{cv,dv=298, \\alpha=.05}= +/- 1.96\\) Step five: Compute the appropriate statistic. We were never given \\(\\sigma\\) or \\(\\sigma^2\\), so we use the t test. Convert me Convert me Convert me Convert me Step six: Decide whether to reject H0. Is -4.86 more extreme than the critical value? \\(t_{cv,dv=298, \\alpha=.05}= +/- 1.96\\) The effect sizerefers to the magnitude of the phenomenon being tested and is calculated as Convert me This statistic reflects the standardized distance between two populationmeans. J. Cohen provides guidelines to interpret the value of d: “The average final exam score from the fall semester (M= 82.4) was significantly lower than the average score from the spring semester (M= 84.2), t(298) = 4.86, p&lt; .05.” Small: = 0.25Medium: = 0.50Large: = 1.0 “The average final exam score from the fall semester (M= 82.4) was significantly lower than the average score from the spring semester (M= 84.2), t(298) = 4.86, p&lt; .05, Cohen’s d= 0.53.” "],
["power-confidence-intervals-effect-size-measures.html", "Chapter 8 Power, Confidence Intervals, Effect Size Measures 8.1 Power 8.2 Confidence Ientervals", " Chapter 8 Power, Confidence Intervals, Effect Size Measures 8.1 Power We have discussed the fact that the conclusions drawn from hypothesis tests are essentially inferences about population parameters, based on sample information. But we have thus far neglected a discussion of what statistical factors should be considered in planning and assessing research-based hypothesis tests. By minimizing the probability of a Type II error (β), we are at the same time increasing the amount of powerof our hypothesis test (1-β). Power is defined as the probability of rejecting the null hypothesis when it is false (i.e., should be rejected). Four important factors affect the power of a statistical test. Knowing any three of these factors mathematically fixes the fourth. Thus, one can use these factors in determining the appropriate design for a particular study. Sample size is most often the targeted factor in formulating such a plan. Table Careful planning of research involves minimizing Type I andType II errors. One rule of thumb is that \\(\\beta\\) should be no more than .20 (e.g., if \\(\\alpha\\) = .05, \\(\\beta\\) = .20). c.For the t test, if the null hypothesis distribution is centered on a t value of 0, then the noncentralt distribution represents the alternative hypothesis distribution, centered on \\(\\delta\\). -This represents the average t value one would expect for a given effect size and sample size Noncentral T 8.1.1 Test Equations One Sample tests $ = d$ \\(d = \\frac{\\bar{X}-\\mu}{\\sigma}\\) \\(g = \\frac{\\bar{X}-\\mu}{s}\\) Two sample tests \\(\\delta = d\\sqrt{\\frac{n}{2}}\\) \\(d = \\frac{\\mu_1-\\mu_2}{\\sigma}\\) \\(g = \\frac{\\bar{X_1}=\\bar{X_2}}{s_p}\\) For unequal n \\(n_n = \\frac{2n_2n_2}{n_1 + n_2}\\) \\(g = t\\sqrt{\\frac{n_1 + n_2}{n_1n_2}}\\) Noncentral T Effect size (ES) or standardized effect size (e.g., d). i. The difference between population means (e.g., \\(\\mu1-\\mu2\\)). ii. The population standard deviation (\\(\\sigma\\)). Noncentral T 8.1.2 Factors of Power Effect size (ES) or standardized effect size (e.g., d). The difference between population means (e.g., \\(\\mu_1-\\mu_2\\)). The population standard deviation (\\(\\sigma\\)). Sample size (\\(n\\)). Significance level (\\(\\alpha\\)). Directionality of the hypothesis test (one-tailed vs. two-tailed). One sample case \\(\\delta = d\\sqrt{n}\\) \\(n = (\\frac{\\delta}{d})^2\\) two sample case \\(delta = d\\sqrt{\\frac{n}{2}}\\) \\(n = 2(\\frac{\\delta}{d})^2\\) A clinical psychologist wants to test the hypothesis that people who seek treatment for psychological problems have higher IQs than the general population. To test her hypothesis, she wants to use the IQ values from 25 randomly selected clients and also to calculate the power to find a 5-point difference in IQ. The mean of the population would be 100 and, therefore, the mean of her clients a 105. The population SD for IQ is 15. (This scenario is from Howell’s 2002 text “Statistical Methods for Psychology”) Known: \\(\\mu_{client}=105\\) \\(\\mu_{pop}\\) \\(\\sigma_{pop} = 15\\) Calculate it with \\(d = \\frac{105-100}{15} = 0.33\\) \\(\\delta = d\\sqrt{n} = 0.33\\sqrt{25}=1.65\\) Given this information and an expected alpha (two-tailed) of .05, we can find in Table A.4 in the Cohen text that the power is between .25 and .50, and more exactly about halfway in between (around .38). What does this value of .38 mean? If power should be at or above .80, what does the clinician do? Increase alpha? Decrease the population SD? Increase the difference between population means? Increase sample size? For a power level of .80, δneeds to be 2.80, from Table A.4 \\(n = (\\frac{\\delta}{d})^2=(\\frac{2.80}{0.33}^2 = 8.48^2 = 71.91\\) 8.2 Confidence Ientervals Sample measures of central tendency, such as the mean, are considered point estimates of population parameters. Confidence intervals are considered a type of interval estimation for population parameters. Computation of confidence intervals. Capture percentage, prediction, and replication Over repeated sampling from a known distribution, the confidence interval represents the percentage of such intervals that contain the population mean. Can be set at any percentage: 90%, 95%, 99% Based on characteristics of the sampling distribution (zor t) and therefore highly related to the manner in which sampling distributions are used for NHST  But CIs and pvalues from NHST are not the same thing!!! Hicks CI 1 Confidence Interval for single sample mean \\(\\bar{X} = +- (t_{cv})s_{\\bar{X}}\\) Confidence interval for a mean difference scores (dependent samples) \\(\\bar{D} +- (t_{cv})s_{\\bar{D}\\) Confidence interval for a difference between sample means (independent samples) \\((\\bar{X_1}-\\bar{X_2}) +- (t_{cv})(s_{\\bar{X_1}-\\bar{X_2}})\\) The instructor of an introductory psychology course is interested in knowing if there is a difference in the mean grades on the final exam between the fall and spring semester classes. Summary data for the two samples is below: What are the 95% confidence intervals around each sample mean, and around the difference between the sample means? fall &lt;- c(82.4,150,11.56) spring &lt;- c(84.2,150,11.44) stat &lt;- c(&quot;Mean&quot;,&quot;N&quot;,&quot;s2&quot;) grades &lt;- data.frame(stat,fall, spring) grades ## stat fall spring ## 1 Mean 82.40 84.20 ## 2 N 150.00 150.00 ## 3 s2 11.56 11.44 What are the 95% confidence intervals around each sample mean, and around the difference between the sample means? Confidence Interval for single sample mean \\(\\bar{X} = +- (t_{cv})s_{\\bar{X}}\\) This has critical value of 1.97 +-. First need to find the standard error with each one. $s_{X} = = = = 0.28 $ $s_{X} = = = = 0.28 $ Now do CI for both \\(\\bar{X} = +- (t_{cv})s_{\\bar{X}}\\) \\(82.4 +- (1.97)(0.28)\\) \\(82.4 +- 0.55\\) \\((81.85, 82.95)\\) \\(84.2 +- (1.97)(0.28)\\) \\(84.4 +- 0.55\\) \\((83.65, 84.75)\\) Calculate that now for differences \\((\\bar{X_1}-\\bar{X_2}) +- (t_{cv})(s_{\\bar{X_1}-\\bar{X_2}})\\) \\(s_{\\bar{X_1}-\\bar{X_2}}= \\sqrt{0.07 + 0.07} = \\sqrt{0.14}=0.37\\) Alpha value associated with this is 1.96. \\(82.4 - 84.2) +- (1.96)(0.37)\\) \\(-1.8 +- 0.73 = (-2.53,-1.07)\\) Important points to keep in mind regarding confidence intervals: They are two-tailed by nature (i.e., on either side of a sample mean). For a given sample size, increasing the level of confidence (e.g., from 95% to 99%) increases the interval width. The narrower the interval (at a given level of confidence!) reflects better statistical precision. Sample size directly affects the width of the interval by affecting the standard error estimate. 8.2.1 Capture Percentage Capture percentage, prediction, and replication What is the likelihood that a subsequent experiment will replicate? Concept of capture percentage: likelihood that a subsequent sample mean will fall into the CI of the current sample. Hicks CI 2 "],
["correlation-and-regression.html", "Chapter 9 Correlation and Regression 9.1 Calculating Person C", " Chapter 9 Correlation and Regression Statistical correlation simply refers to the notion that two variables are related to one another—when one varies, the other varies in a predictable manner. There are typically two ways to understand the nature of a correlation: visually and numerically. There are multiple ways to understand and, therefore, calculate the Pearson correlation coefficient. Relying on Pearson’s rto describe a relationship assumes that certain conditions have been met. Other factors may impact how one interprets the linear correlation. Testing the “significance” of the correlation coefficient. There is a correlation coefficient to deal with ordinal variables, or those measured by ranks. The variables are represented in the abstract as Xand Y. b.Scatterplotsare figures that represent a pairof scores for each individual, one score for each measured variable. The pattern of the scatter indicates the nature of the relationship. Hicks Hicks Hicks The statistical representation of the correlation was developed by Karl Pearson and is called the Pearson product-moment correlation coefficient, or r. r varies from -1 to 1. The strength, or magnitude, of the relationship increases as distance from 0 increases. iii.The sign of the correlation represents the direction, or slopeof the relationship. Hicks A graduate student samples 50 college professors in social sciences at the same university who have been there for at least 10 years. She measures them on a number of characteristics: average quality of their instruction, average quality of their courses, number of publications, and number of citations by other authors. Is there a relationship between the number of publications and number of citations? Hicks Hicks 9.1 Calculating Person C One method is to examine the cross-products, which represent the multiplication of Xand Y, usually on a standard scores. The result is the standard score formula: Hicks Note these formulas (e.g., z-score) use population characteristics This approach is very tedious, and does not make sense unless you already have the z-scores calculated for some reason. Hicks The deviation score formulais a computational formula that relies on using deviations from respective means: Hicks There is a raw score, computational formula that bypasses the need to calculate deviations from each mean: Hicks Finally, we can use the covarianceto compute the correlation. Covariance is the average sum of the cross-products of deviations: Hicks Dividing this number by the cross-product of the unbiased sample standard deviations produces: Hicks 9.1.1 Assumptions The scores are pairs—the same set of individuals needs to contribute both scores. b.Because the mean and variance are used to compute r, the variables need to be measured on an interval or ratio scale. c.Xand Yare normally distributed. d.The observations were randomly sampled. e.The relationship between the variables is linear, rather than curvilinear. Values of rare uninterpretable (and are underestimates) for curvilinear relationships. If the rangeon one or both variables is restricted(i.e., the group is very homogeneous on Xand/or Y), then the value of rtends to become smaller. Theoretically, the correlation can get bigger with range restriction, although this is VERY rare with moderate-to-large sample sizes. Hicks The sample size will not affect the value of r(except when N= 2), but will affect its accuracyin terms of statistical significance. -when N= 2, rALWAYS equals ±1.0 c.The coefficient of determinationrepresents the amount of variance in Ythat can be associated with the variance in X. Hicks Conceptual Hicks Finally, a measured correlation says nothing about whether Xand Yare causally linked, only that there is an association. There are three generic possible reasons for the association. Hicks Testing the “significance” of the correlation coefficient. a.Is the correlation different from 0? Hicks Or look up critical values in A5 and compare with r 48df .05 alpha of .273 Hicks Hicks There is a correlation coefficient to deal with ordinal variables, or those measured by ranks, called Spearman rho. Hicks d= the difference between paired ranks, rather than paired scores. Hicks "],
["regression.html", "Chapter 10 Regression", " Chapter 10 Regression Regression essentially involves creating a mathematical function that best describes the functional relationship between variables Xand Y. It is used in a two-step process: estimating the function for a full data set, and then applying it to a partial data set in which X, but not Y, is known (i.e., prediction). There are different ways to compute the components of the regression line. We can also identify aspects of the regression line that give us an idea about the accuracy of prediction. We can test the significance of the regression coefficient (the slope) under the null hypothesis that the slope = 0. This is analogous to the null hypothesis test that the correlation = 0. Xis referred to as the predictorvariable and Yis referred to as the criterionvariable. The slope-intercept form of the line is (SLOPE OF LINE see slides) i.The predictedscore on Yis ii.bis the slopeof the line iii.ais the Y-interceptof the line; that is, where the line intercepts the Yaxis when X= 0 ˆYbXa We fit the “best” line to the data set by using the least squaresprinciple—minimizing the summed, squared distance from each data point to the line. i. Error (e) is represented by the distance from each Y-point to the line. Hicks When we have found the lowest possible total squared error, we have identified the best-fitting line. Hicks Hicks Hicks Hicks To find a predicted score for a data point, we simply substitute the Xvalue into the prediction equation. Hicks Hicks Hicks We can also identify aspects of the regression line that give us an idea about the accuracy of prediction. a. The error in prediction (e) is associated with its own distribution. The varianceof this distribution (i.e., the ‘variance of the estimate’) is Hicks The standard deviation of this distribution is also called the standard error of the estimate Hicks Hicks We can also identify aspects of the regression line that give us an idea about the accuracy of prediction We can understand the standard error of the estimate in terms of the conditional distributionof Yfor each value of X. Each conditional distribution is assumed to have the same variance—homoscedasticity—and to have a normaldistribution. Hicks Because of these assumptions, we can make statements about the probability of predicted scores defined by the regression line What’s the probability of Y around Yhat at anyscore of X? Hicks Hicks Hicks Hicks Because of these assumptions, we can make statements about intervals around true values of Yestimated by the regression line (prediction intervals) note y hat note y hat note y hat note y hat We can test the significance of the regression coefficient (the slope) under the null hypothesis that the slope = 0. note y hat note y hat note y hat PUT THE R OUTPUT HERE "],
["matched-t-test.html", "Chapter 11 Matched T Test 11.1 Theory", " Chapter 11 Matched T Test 11.1 Theory When both sample means were produced by the same participants, we conduct what is known as a dependent-samplestest. This is a test of the average difference between the scores in one condition and the scores in another condition—thus, the unit of measurement is a difference score. \\(\\bar{D}=\\Sigma D /n\\) \\(D= X_{i1} - X_{i2}\\) The mean of this sampling distribution is \\(\\delta\\)= 0. The shape of this sampling distribution is approximately normal. The standard error of the sampling distribution of mean difference scores is \\(s_{\\bar{D}} = \\frac{s_D}{\\sqrt{n}}\\) \\(s_d = \\sqrt{\\frac{\\Sigma{(D-\\bar{D})^2}}{n-1}}\\) The test statistic is \\(t = \\frac{\\bar{D}-\\delta}{s_{\\bar{D}}}\\) \\(s_\\bar{D} =\\frac{s_D}{\\sqrt{n}}\\) The effect size is calculated \\(d =\\frac{\\bar{D}}{S_D}\\) \\(S_d = \\sqrt{\\frac{\\Sigma{(D-\\bar{D})^2}}{n-1}}\\) You are investigating whether the older or younger male in a pair of brothers tends to be more extroverted. So you test where each one falls on an introversion-extroversion scale. The results are as follows: # Dep younger &lt;- c(10,11,18,12, 15) older &lt;- c(18,17,19,16,15) dep &lt;- data.frame(younger, older) Step one: State the null and alternative hypotheses \\(H_o : \\delta = \\mu_1 - \\mu_2 = 0\\) \\(H_a : \\delta = \\mu_1 - \\mu_2 != 0\\) Step two: Set the criterion for rejecting H0. Alpha is usually set to .05, but could be other values depending on the research context. Make sure you’ve considered directionality! Step three: Select the sample and collect your data. Step four: Locate the region of rejection and critical values. \\(t_{cv,dv=4, \\alpha=.05}= +/- 2.77\\) Step five: Compute the appropriate statistic. We were never given \\(\\sigma\\)or \\(\\sigma^2\\), so we use the t test. Convert me Convert me Convert me \\(s_D=3.55\\) \\(s_\\bar{D} =\\frac{s_D}{\\sqrt{n}}= \\frac{3.35}{\\sqrt{5}}\\) \\(t = \\frac{\\bar{D}-\\delta}{s_\\bar{D}} = \\frac{-3.8-0}{1.50}= -2.53\\) Step six: Decide whether to reject H0. Is -2.53 more extreme than the critical value? \\(t_{cv,dv=4, \\alpha=.05}= +/- 2.77\\) “The average extroversion value for the younger male siblings (M= 13.2) did not differ significantly from the extroversion value for the older siblings (M= 17.0), t(4) = 2.53, p&gt; .05.” Effect size computation \\(d = \\frac{\\bar{D}}{s_D}= \\frac{-3.8}{3.35} = -1.13\\) Small: = 0.25Medium: = 0.50Large: = 1.0 “The average extroversion value for the younger male siblings (M= 13.2) did not differ significantly from the extroversion value for the older siblings (M= 17.0), t(4) = 2.53, p&gt; .05, Cohen’s d= 1.13.” So, why does the effect size calculation disagree with the result of the hypothesis test? Template file "],
["one-way-anova.html", "Chapter 12 One Way ANOVA 12.1 Effect Size Measures", " Chapter 12 One Way ANOVA In cases where the number of groups in a study (K) is more than two, we cannot use the ttest for the hypothesis test because there is an associated cost. 2. The Analysis of Variance (ANOVA), or Ftest, controls the experimentwiseType I error rate while simultaneously allowing a test of the equality of multiple population means. 3. Every given score in a data set differs somewhat from the overall, or grand, mean of the data set. But this distance from the grand mean can be partitioned into the distance from the score to its group mean and the distance from that group mean to the grand mean. 4. There are several assumptions that should be met when using ANOVA. 5. The steps of the hypothesis test for ANOVA 6. Effect size and power for the ANOVA Performing multiple ttests across multiple groups in a single study increases the likelihood of at least one Type I error occurring across the “family” of comparisons b. Experimentwise(familywise) Type I errorrate = 1 –(1 –α)c, where c= number of independent ttests to be conducted For example, if I wanted to conduct a test for eachpairwise comparison in a 3-group study, my experimentwiseerror rate would be 1 –(1 -.05)3= .142 For 4 conditions, it would be 6 tests and 1 –(1 -.05)6= .265 (1)max possible comparisons with groups The one-way ANOVAis used to analyze data generated by the manipulation of one independent variable with at least 2 levels. Each group, or condition, created by the manipulation is called a level. b. The null hypothesis—H0: c.The alternative hypothesis—Ha: , for some pair of groups iand k The concept behind the ANOVA is that we use two estimates of the population variance associated with the null hypothesis i. One estimate is the within-groups variation ( ): the influence on variance due to error (chance factors like individual differences), which is presumed to be the same within each group ( ) ii. The other estimate is the between-groupsvariation ( ): the influence on variance due to the independent variable or treatment ( ), plus error due to the random process of group assignment ( ) The test statistic involves created a ratio of the between-groups to the within-group variation: Every given score in a data set differs somewhat from the overall, or grand, mean of the data set. But this distance from the grand mean can be partitioned into the distance from the score to its group mean and the distance from that group mean to the grand mean. Obtaining estimates of variance in our data is the trick in the ANOVA. a. An individual score in an ANOVA model is comprised of 3 components. The linear model is , where μis the grand mean, αkis the effect of belonging to group k, and eikis random error. b. The distance from a score to its group mean essentially reflects error only, whereas the distance from a group mean to the grand mean reflects the effect of the treatment + error Obtaining estimates of variance in our data is the trick in the ANOVA. c. The total variation in a data set, then, can be split into that due to within-group variation + between-group variation. In terms of sums of squares: d. These SS estimates must be divided by their respective dfto obtain the estimates of the average variability within-groups and the average variability between-groups. three F STATISTIC FROMULA If there is no treatment effect due to the independent variable (i.e., the null hypothesis is true), then we expect this ratio to be roughly equal to 1. However, if there is a treatment effect due to the independent variable (i.e., the null hypothesis is false), then we expect this ratio to be &gt; 1. three three 12.0.1 Assumptions of ANOVA To be representative of the populations from which they were drawn, the observations in our samples must be random and independent. The population distributions from which our samples were drawn are normal, which implies that our dependent variable is normally distributed. c.The variances of our population distributions are equal (homogeneity of variance). d.Generally speaking, the ANOVA is robust to minor violations of the normality and variance assumptions, except for cases in which heterogeneity is coupled with unequal sample sizes 12.0.2 Practice Scenario: Does the ethnicity of a defendant affect the likelihood that he is judged guilty? People were given transcripts of a trial and asked to judge the likelihood that a defendant was guilty, on a 0 –10 scale. The transcript was identical, but across 3 conditions, the reported ethnicity of the defendant varied. The results were as follows (study based on Stephen, 1975): white &lt;- c(6,7,3,2,3,5,0) black &lt;- c(10,9,4,10,10,3) hispanic &lt;- c(6,10,5,5,10,2) three three three three anova Step six: Interpret (was null rejected?). We do NOT reject the null, because the Fvalue did not exceed the Fcv “The ethnicity of the defendant did not significantly affect the average guilt rating given by mock jurors, F(2, 15) = 2.34, p&gt; .05.” 12.1 Effect Size Measures An effect size measure, f, is used to represent the population SD between groups from the grand mean, versus population SD within a group anova We can then use the noncentrality parameter for the Fdistribution, related highly to phi, to estimate power \\(\\phi = f * \\sqrt{n}\\) \\(n = \\frac\\{\\phi}{f}^2\\) Consult Table A.10 for values of phi, based on number of groups (K) and the likely dfw(although this has a modest influence) \\(n = \\frac\\{\\phi}{f}^2\\) What if we desired .80 power? How many subjects? In Table A.10, with k= 3 and dfw= 16, phi = 2.0 anova anova This type of measure is comparable to R2in regression Interpretation: roughly 28% of the variance in the DV (i.e., guilt ratings) can be attributed to the IV (i.e., the race of the defendant). But it is upwardly biased—it overpredictsthe population eta-squared anova Interpretation: roughly 13% of the variance in the DV (i.e., guilt ratings) can be attributed to the IV (i.e., the race of the defendant). Cohen (1977) recommends the following convention: ≈ .01 is “small” ≈ .06 is “medium” ≥.15 is “large” Look for “adjusted R squared” notation in the SPSS ANOVA output from the Univariatemethod of performing the ANOVA anova "],
["multiple-comparisons.html", "Chapter 13 Multiple Comparisons 13.1 Trend analysis", " Chapter 13 Multiple Comparisons In past lectures we have discussed the problem with the familywise (or what your text calls “experimentwise”) Type I error rate. There are different methods for holding the familywise error rate at no more than .05. This methodology is most commonly used in post hoc tests designed to test specific contrasts among conditions following a significant ANOVA. In addition to comparisons done post hoc, others can be done a priori, commonly known as planned comparisons. Because such comparisons are usually done with a particular theoretical outcome in mind, they are not as conservative as post hoc comparisons. Familywise Type I error rate = , where j = number of independent t tests to be conducted. When comparisons are not independent, the familywise rate can be approximated by the formula . The comparisonwise error rate is the Type I error rate per comparison. Comparisons are orthogonal if the outcome of one test is not redundant with the outcome of another. There are (k – 1) orthogonal comparisons in a data set, which will be discussed in more detail. There are different methods for holding the familywise error rate at no more than .05. This methodology is most commonly used in post hoc tests designed to test specific contrasts among conditions following a significant ANOVA. The simplest, and most powerful test, is called the Fisher Least Significant Difference test (LSD). It is sometimes called the “protected t” test, and the formula is: IMG You then use the df from the overall error term for finding the critical value of t. This test should be used only following a significant F test, and for no more than 3 groups. If homogeneity of variance is not tenable, then use the individual variances from the groups being tested, as the case for a traditional separate variances t test with df = N - 2. How does the Fisher LSD protect against Type I error inflation with only 3 groups? Complete null is true \\(H_0 : \\mu_1 = \\mu_2 = \\mu_3\\) In this case, if the overall F is significant, you have already committed a Type I error and any more of them don’t contribute further to αFW (i.e., you are just going to commit the ‘same’ error in a particular pairwise comparison. This is the case no matter how many groups are in the experiment. Partial null is true \\(H_0 : \\mu_1 = \\mu_2 != \\mu_3\\) With 3 groups, if the overall F is significant, it was not a Type I error; there is only one real chance of a Type I error—that being when the partial null is true. Here αFW was never in danger of being more than .05. But with more than 3 groups, and when more than one partial null is true, you have the chance of committing multiple errors. Another common method is to use the studentized range (Q) sampling distribution as the source of critical values for minimum differences needed to declare population means significantly different. The Tukey method computes the minimum difference needed for any pair of means to be significantly different. We use alpha, the number of means being compared (r), and the df associated with MSW to find a critical Q value. Image With this method, you are protecting yourself against the most extreme possible Type I error (i.e., between the largest and smallest means), and therefore all other comparisons are likewise protected. Scenario: A researcher gives a test of creativity to four groups of children. The age difference between each group is 2 years (range 4-10 years). Do the children demonstrate significantly different levels of creativity? Image Image The Student Newman-Keuls method is similar to the Tukey, except that the critical value is determined by the number of steps separating each pair of means (r): range = i – k + 1 Problem: we do lose some control over αEW, as it actually goes over .05 Image Image Image The Tukey/Kramer method is used when sample sizes are unequal. We calculate the harmonic mean of the sample sizes to achieve the appropriate number in the denominator: Image The Ryan (or REGWQ) test is a modification of the SNK to achieve increased power but also keeps αEW no greater than .05. It is preferred over Tukey to get better power and over SNK to keep αEW ≤ .05. This means it uses fractional values of alpha that are not easy to tabulate—but SPSS and other packages offer it as an option so it’s easy to use. Image Another conservative method, the Scheffeaccent test, uses the F distribution, but can also be used to test complex comparisons involving combined groups, called linear contrasts. Image The critical value of F sets the experimentwise error rate against all possible linear contrasts, not just pairwise contrasts. Image Image Image Image Image In addition to comparisons done post hoc, others can be done a priori, commonly known as planned comparisons. Because such comparisons are usually done with a particular theoretical outcome in mind, they are typically not as conservative as post hoc comparisons. The linear contrast described earlier is generally used to complete planned comparisons. There are two primary differences as compared to the post hoc version of the test. If multiple contrasts are to be done, then it’s best if they are orthogonal, or independent. There are K – 1 orthogonal comparisons available in any one set of contrasts. Most (e.g., your text author) argue that you can keep each contrast at α = .05 when the set is orthogonal. The linear contrast method described earlier is also used to complete planned comparisons. There are two primary differences as compared to the post hoc Scheffé test. Because of the orthogonality principle, the critical value for each comparison allows for more power (i.e., it is more liberal than the post hoc version Scheffé developed): \\(F_{cv}=F_{1,N-K}\\) Image Image A very conservative, but simple, method is to figure the comparisonwise error rate is to divide the familywise error rate by the number of comparisons one wishes to make. This is known as the Bonferroni test or the Dunn test. \\(\\alpha_{pc}= \\alpha_{EW}/J\\) For example, with 3 comparisons, α = .05/3 = .0167. You don’t set the modified alpha based on all possible comparisons, just based on the number you wish to conduct. 13.1 Trend analysis A statistical procedure called trend analysis can be done to examine the functional relationship between a quantitative IV and the DV (e.g., linear, quadratic, cubic). The same linear contrast comparison method is used as with regular contrasts, and the trend contrast coefficients are a special set of orthogonal comparisons based on K. Image Image Image A statistical procedure called trend analysis can be done to examine the functional relationship between a quantitative IV and the DV (e.g., linear, quadratic, cubic). Trend analysis works only when the IV is a quantitative variable (e.g., drug dosage). More than one trend may be significant, and therefore inspecting a graph of the means in tandem with the analysis is highly recommended. Image "],
["factorial-anova.html", "Chapter 14 Factorial Anova 14.1 Assumptions of ANOVA 14.2 Power and Sample Size Estimatoin", " Chapter 14 Factorial Anova Hypothesis Testing with Multiple Samples: The Factorial ANOVA 1.In cases where multiple IVs are crossed with one another in the same design, the factorial ANOVAis used to partition the variance. 2.There are multiple null hypotheses tested simultaneously in a factorial ANOVA, making the design and analysis somewhat more complicated than one-factor designs. 3.The assumptions in the factorial ANOVA are the same as in the one-way ANOVA. 4.Other considerations in factorial ANOVA Such designs incorporate multiple IVs to examine the simultaneous effect these variables have on behavior—this adds efficiency to the design. a.Incorporating more than one IV allows researchers to control for the impact of a second variable by explicitly including it in the design. Controlling for a second factor allows for a reduction in the error term of the ANOVA. b.The unique benefit of the factorial design is the investigation of interactions—the possibility that the combined influence of multiple factors influences behavior in a way unpredictable from knowing the influence of individual variables in isolation 2.There are multiple null hypotheses tested simultaneously in a factorial ANOVA, making the design and analysis somewhat more complicated than one-factor designs. a.Factorial designs are typically represented in arrays with rows for the levels of one IV and columns as levels of the other IV. Each combination of levels from IV 1 and IV 2 is called a cell, or condition. Scenario: A researcher is interested in the role of “drive” level and certain drugs on learning in monkeys. The monkeys are given 20 “oddity” problems in which they are given 3 objects, one of which is new, and they are reinforced for picking the novel item. One IV is “drive” level, either 1 hour of food deprivation or 24 hours of food deprivation before the oddity task is undertaken. The other IV is drug condition, with 3 levels. One is a placebo, and the other two compare two different drugs believed to affect motivation. There are multiple null hypotheses tested simultaneously in a factorial ANOVA, making the design and analysis somewhat more complicated than one-factor designs. a.Factorial designs are typically represented in arrays with rows for the levels of one IV and columns as levels of the other IV. Each combination of levels from IV 1 and IV 2 is called a cell, or condition. i. Main effects refer to the influence of any one IV regardlessof the other IV(s) in the design. Simple effects (or simple main effects) refer to the effect of one IV at a particular level of another IV (e.g., the effect of columns at row 1). Interaction effects refer to the combined influence of the IVs. Interactions can be defined many ways, but one useful generic definition is that an interaction occurs when the simple effect of one IV is inconsistent, or not the same, at each level of another IV An interaction is present when the simple (main) effects of one IV are not the same at all levels of another IV An interaction is present when the main effect of an IV is not representative of the individual simple (main) effects of that IV An interaction is present when the effect of one IV is conditionally related to the levels of the other IV An interaction is present when one IV does not have a constant effect at all levels of another IV five In designs with two IVs, the variance between conditions is partitioned into 3 components: the overall influence of IV 1 ( ), the overall influence of IV 2 ( ), and the combined influence of IVs 1 and 2 ( ). The fourth component is the variance within conditions, or within cells, ( ). “A two-way between-subjects ANOVA generated a significant interaction between drug condition and level of food deprivation, F(2, 18) = 3.93, p &lt; .05. The main effect of food deprivation, F(1, 18) = 1.31, p &gt; .05, was not significant. The main effect of drug condition was just short of conventional significance, F(2, 18) = 3.06, p = .07.” 14.1 Assumptions of ANOVA The samples are independent, random samples from the populations. b.The scores on the DV are normally distributed in the population. c.The population variances in all cells (conditions) of the factorial design are equal—homogeneity of variance. An appropriate measure of variance explained (or association) is omega squared five Multiple comparisons for main effects (i.e., marginal means) can be accomplished with the standard methods, such as the Tukey method: five Multiple comparisons for main effects (i.e., marginal means) can be accomplished with the standard methods, such as the Tukeymethod. Simple effect (or simple main effect) comparisons are essentially a series of one-factor ANOVAs to help illuminate the nature of an interaction, but using the overall MSWas the error term. n prime is SUM of the n from each individual cell in the comparison (marginal n) five five Simple comparisons involve individual comparisons of cells within a simple main effect. In this case, standard post hoc tests (e.g., Tukey) could be used. Again, the overall MSWis used as the error term in these comparisons. “A two-way between-subjects ANOVA generated a significant interaction between drug condition and level of food deprivation, F(2, 18) = 3.93, p&lt; .05. The main effect of food deprivation, F(1, 18) = 1.31, p&gt; .05, was not significant. The main effect of drug condition was just shy of conventional significance,F(2, 18) = 3.06, p= .07. To reveal the nature of the interaction, simple main effects were conducted on drug condition at each level of food deprivation, with a Bonferroni adjustment for multiple comparisons applied. At 1 hour of food deprivation, the effect of drug was significant, F(2, 18) = 6.76, p&lt; .05, whereas the effect of drug was not significant at 24 hours of food deprivation, F(2, 18) = 0.22, p&gt; .05. A Tukey test among drug conditions at 1 hour of food deprivation revealed that the placebo and drug 2 differed significantly, whereas the other two pairwise comparisons were not significant.” 14.2 Power and Sample Size Estimatoin 14.2.1 Three Way Factorial five Example research scenario: A researcher tests driving ability based on three factors: Experience of driver (inexperienced vs. experienced) Road class (class 1, class 2, or dirt) Time of day (daytime vs. nighttime) 2 × 3 × 2 factorial design DV is the number of steering corrections made during a 1-mile section of roadway "],
["repeated-measures-anova.html", "Chapter 15 Repeated Measures ANOVA: 15.1 Assumptions of RMANOVA 15.2 Effect Size", " Chapter 15 Repeated Measures ANOVA: Repeated measures designs are those in which subjects are tested in each level of the independent variable. 2.The conceptual difference is that repeated measures designs allow for the separate estimation of the influence of individual differences from participant to participant, whereas between subject designs do not. 3.The repeated measures design is more economical and contains more statistical power as compared to its counterpart. 4.The assumptions of the repeated measures ANOVA 5.Power and effect size The linear model includes two more components, the source of an individual’s performance across the entire study and how an individual interacts with the treatment levels: This ability to isolate the source of individual differences allows us to further reduce the error variance (i.e., the denominator of the F ratio). \\(F = \\frac{MR_{RM}}{MS_{sub x RM}}\\) Variability due to subjects is taken into account, but then ignored in the computation of F. rma A consumer psychologist is interested in the effect of label information on the perceived quality of wine. Six individuals are asked to rate 3 different wines a scale of 1 to 20, with higher scores being a better quality. The wines were labeled as French, Italian, or American, but the wine was identical across the conditions. The results are shown below: 15.1 Assumptions of RMANOVA Sample randomly selected from the population The DV is normally distributed in the population Sphericity: the variances of difference scores from all possible pairs of conditions are equal rma rma rma If sphericityis violated, there are several avenues to correct for it that involve applying a correction for the Epsilon value (ε) In SPSS, you’ll see the following in the within-subject ANOVA output: Lower bound correction—this is a change to the critical Fvalue from df= K–1, (n–1)(K–1) to df= 1, n–1. This severelyincreases the critical Fvalue to 6.61 in our case. Huynh &amp; Feldtand Geisser-Greenhouse are corrections to the dfbased on the degree of violation to sphericity, and create more modest corrections to the critical Fvalue. The non-corrected dfare multiplied by the epsilon values for each respective procedure. If none of the Fs is significant, don’t worry about these corrections—fail to reject the null. If all of the Fs are significant, then reject the null. If one/some of the “corrected” Fs is significant but others are not, then most advocate the Huynh &amp; Feldtcorrection (it’s not as conservative as the lower bound). Field text advocates averaging the G-G and H&amp;F estimates—for a rule of thumb, average the significance values of these estimates 15.2 Effect Size An effect size measure, f, is used to represent the population SD between groups from the grand mean, versus population SD within a group "],
["factorial-designs-with-repeated-measures.html", "Chapter 16 Factorial Designs with Repeated Measures 16.1 Assumptions of Mixed ANOVA", " Chapter 16 Factorial Designs with Repeated Measures The partitioning of variance when all factors are repeated measures includes multiple subject × treatment interactions. A full computational example of a mixed design with one repeated measures factor and one between-subjects factor. The assumptions of designs with repeated measures In cases where both (or more) factors are repeated measures, each main effect and interaction is generated as usual, but each error term consists of some type of interaction with subjects. fdrm fdrm fdrm In cases where at least one factors us repeated measures and at least one other is between subjects, we have a mixed design. The sources of variation in a two-way mixed design are below: fdrm A memory psychologist is interested in how false memories are affected by changes in context. Three groups of people were asked to learn lists of words connected to a missing theme. For example, the words BED, REST, PILLOW, BLANKET, NIGHT, &amp; SLUMBERare all related to SLEEP. Group 1 studied many lists like these, with only the first two items for each list (e.g., only BED and REST). Group 2 studied the same lists, but including the first 4 items per list. Group 3 studied the same lists, but including all 6 items in each. Each list had a very distinctive font, as I used above. Each group then got a memory test. Included on this memory test were the “missing” theme items, such as SLEEP. Sometimes the theme items were in the same font as their list mates (e.g., SLEEP). Other times, these items were presented in a different font (e.g., SLEEP). There were 4 items in each of these test conditions. The data presented on the next slide represent how many, out of 4, of these missing theme items were called “old” on the memory test. These are, therefore, false memories. The question is whether the factors of list length and font style at test affect how many of these occur. fdrm fdrm In cases where at least one factors us repeated measures and at least one other is between subjects, we have a mixed design. The sources of variation in a two-way mixed design are below: fdrm fdrm 16.1 Assumptions of Mixed ANOVA Sample randomly selected from the population The DV is normally distributed in the population The variances of difference scores from all possible pairs of conditions are equal and the covariances(or correlations) between pairs of conditions are equal (sphericityor circularity). 4.These covariance structures, or matrices, are equal across the between- subject groups in the study (i.e., for each list length condition). 16.1.1 Equal Variance – Run Leven’s Test and Box’s Yuck! We don’t have equal variances for the ‘same font’ scores among our conditions. Nor do we have equal covariance structures across our 3 list length conditions (Box’s test). Basically, then, we shouldn’tuse a pooled error term for any post-hoc comparisons. One plan could be to run two separate one-way ANOVAs across condition to assess simple main effects, and checking the Welch’s and Brown-Forsythe ANOVA options for each. We should apply a Bonferronicorrection for any of these post-hoc comparisons. 16.1.2 Simple Main Effects Even with a Bonferronicorrection (.05/2), there are group differences for same font condition, but not for the different font condition. We could then perform a Tukeytest for the pairwisedifferences for the same font condition (i.e., simple comparison). "],
["multiple-regression.html", "Chapter 17 Multiple Regression 17.1 Conceptual Representation", " Chapter 17 Multiple Regression Multiple regression (and multiple correlation) refers the process of fitting a linear combination of multiple Xvariables to predict scores on Y. There are different ways to compute the components of the regression line. In multiple regression, we have to the multiple correlation coefficient(R) and its associated coefficient of determination (R2). We can test the significance of the individual regression coefficients under the null hypothesis that each is equal to 0. Scenario: A researcher is interested in predicting the number of substance abuse relapses in a five year period following entrance into substance abuse treatment. She measured the following data for 120 women upon entry into the treatment program: Depression Propensity for substance abuse Daily life stress Amount of social support from close others Perceived social support from close others CREATE CORRELATION TABLE The slope-intercept form of the equation is mr We fit the best “hyperplane” to the data set by using the least squaresprinciple. mr There are different ways to compute the components of the regression plane. Predicting number of relapses (Y) from substance abuse propensity (X1) and daily life stress (X2) mr Standardized coefficients (Beta weights) for two predictors mr mr mr mr R OUTPUT OF REGRESSION mr In multiple regression, we have to the multiple correlation coefficient(R) and its associated coefficient of determination (R2). Multiple Ris a correlation coefficient that represents the entire linear combination of predictors, rather than the simple correlation of a given Xvariable with Y. mr Multiple Rranges from 0 to 1 (i.e., cannot be negative). Multiple Ralso represents the correlation between each Yscore and its associated Ŷ score. Multiple R2is the amount of variance in Yshared by the linear combinationof predictors. R2 = .42^2 = .176 Multiple R2can overestimate the true amount of shared variance when there are numerous predictors and a small number of observations, so an adjustment for this problem has been identified: mr Testing for significance. mr mr 17.1 Conceptual Representation mr mr mr mr mr mr 17.1.1 Second Part Regression diagnostics involve identifying potential outlying observations to help refine the prediction equation. Constructing an efficient and valid regression equation requires a consideration of many factors. Various selection methods are available for finding the “best” regression model, but each has its own drawbacks. Distanceor discrepancyrefers to the error in prediction in the criterion variable. -could be random error, could be incorrectly recorded, couldbe an unusual case that is justified in being thrown out -a ttest on the residuals can be requested, called the Studentized residual, with (N-P-1) degrees of freedom -keep in mind that a ttest is being done for each observation, so a Bonferronicorrection for multiple comparisons is probably a good idea ˆYYe−= b.Leverage(often denoted hi) helps to identify outliers based on the predictor variables X1, X2, and so on. -possible values of leverage range from 1/N to 1.0, with a mean of [(P+1)/N] -some recommend carefully inspecting observations with leverage values of [3(P+1)/N] or greater Influencecombines distance and leverage for a given observation to ascertain whether that observation markedly changes the regression surface (i.e., the equation). -most common measure is Cook’s D -reflects the change in a regression coefficient if the offending observation were taken out and the regression re-computed -values over 1.0 are highly unusual and merit scrutiny -a more conservative rule of thumb is if Cook’s D &gt; [4/(N –k –1)] mr Mahabanolisdistance measures the uniqueness of a given observation based on its value and the mean values for all other cases of the predictors in the model mr Tolerancerefers to the degree to which one predictor can itself be predicted by the remaining predictors in the model -if the multiple correlation from all other predictors here is RX, then tolerance is the leftover unexplained variance, or -it tells us the degree of overlap among predictors—lower values of tolerance represent potential problems (lower than .20 or so is a flag) -we want the multiple correlation among predictors to be rather low, and therefore tolerance to rather high Variance inflation factor(VIF) is the reciprocal of tolerance and represents how much the standard error of a given predictor coefficient is made larger because of high correlations with other predictors -we want low standard errors for coefficients, and therefore low VIF values (values over 10 or so might cause concern). -if eliminating a predictor variable with high VIF and low tolerance helps to generate a more stable model (i.e., low sampling error), then it may be a good idea to do so Various selection methods are available for finding the “best” regression model, but each has its own drawbacks. a. The all subsetsmethod essentially requires fitting all possible combinations of predictor variables to see which provides the best R2or that minimizes MSres -one problem with this technique is that it could capitalize on chance -your particular sample may have data points that are not representative of the population b. Backward eliminationinvolves putting all predictors into the model and then removing those that contribute the least to the model -this is done iteratively until we are left with the model that only includes influential predictors Stepwiseregression is the opposite of backward elimination, where we add the most influential predictor, then the next most, and so on. -at each step in the process, before adding a new variable, we determine whether any current predictors should be removed on the basis that they no longer make a contribution -selection stops when remaining predictors will not make a contribution -forward selectionprocedures can be used so that previously added predictors are not removed at any future step Hierarchical regression is a method used to force the entry of variables in a particular order overall, the stepwise method is likely the best, although these methods should be used primarily when trying to create the best prediction, rather than trying to test theory EXAMPLE OF STEPWISE REGRESSION IN R CAN YOU GET THIS DATASET FROM JH "],
["chi-square.html", "Chapter 18 Chi-Square", " Chapter 18 Chi-Square There is a chi-square distribution and an hypothesis test commonly called the “chi-square” test. This test was developed by Karl Pearson. The one-way chi-square is commonly called a “goodness-of-fit” test. The two-way chi-square is called a test of independence (or association) and relies on the use of contingency tables. Like other statistical tests, the assumptions of chi-square need to be considered. -The distribution varies as a function of the df, and actually changes shape noticeably as the df increase; the distribution is skewed positively except for very large df -The chi-square distribution is an approximation to the multinomial distribution (multinomial = more than 2 variables) -The chi-square test was developed to ascertain the discrepancy between observedfrequencies of multinomial categories and the expected frequencies from those categories cs The one-way chi-square is commonly called a “goodness-of-fit” test. Like other statistical tests, the chi-square test compares the outcome of a formula to what would be expected by some null hypothesis -With one factor and many categories in that factor, the test is often called a “goodness-of-fit” test. In other words, do the observed frequencies “fit” the expectation? cs -How one determines what is ‘expected’ depends on the scenario—often this is determined by some expectation of a random observation Tolman, Ritchie, and Kalish (1946) performed one of the most famous experiments in animal learning. Rats were first taught how to run down a particular alley to reach a goal box (rewarded by food). After training, the original alley was blocked halfway down and other alleys were provided as choices. The data below are a simplification of the full set of choices (see figure below for full set). There were 32 rats used, and each was tested in the new maze after learning the original route. If rats were choosing alleys at random, then an equal number would choose each of the 4 (8 rats each). cs cs With df = 3, we look up the chi-square critical value in Table A.14, with α= .05 That critical value is 7.81. Thus, the rats are choosing alleys nonrandomly. Note that this is an omnibus test like the ANOVA; it only detects a discrepancy. You could perform similar chi-square tests on smaller portions of the categories, or perform a binomial test for a single category (e.g., is 15 out of 32 different from what would be expected by chance—25%)? The two-way chi-square is called a test of independence (or association). With two variables, one sets up a contingency table to see if the frequencies of one factor are contingent (i.e., depend) on the other Scenario: Another famous experiment by Darley and Latané (1968). Subjects participate in a discussion over intercom with an experimenter. Subjects thought that either 0, 1, or 4 others were also in the conversation. Partway through, the experimenter feigned illness and asked for help. The table shows how many subjects in each group sought out the experimenter. cs cs cs cs Like other statistical tests, the assumptions of chi-square need to be considered. a. The categories are mutually exclusive and exhaustive -no observation can be in more than one category -every possible category must be measured -sometimes researchers fail to consider ‘nonoccurrences’ or ‘no’ responses as a separate category b. The observations must be independent -only one response per subject! c. Normality -small expected frequencies (&lt; 5) often create a problem for normality "],
["non-parametric-data.html", "Chapter 19 Non-Parametric Data", " Chapter 19 Non-Parametric Data Template file "],
["advanced-data-cleaning.html", "Chapter 20 Advanced Data Cleaning", " Chapter 20 Advanced Data Cleaning Template file "],
["advanced-multiple-regression.html", "Chapter 21 Advanced Multiple Regression 21.1 Introduction to Multiple Regression 21.2 Basics of Multiple Regression 21.3 Types of Multiple Regression 21.4 Interpretation of Results 21.5 OLD MEDIATION MODERATION 21.6 Introductory Ramble 21.7 Multiple Regression 21.8 Surpressor Variables 21.9 Moderation and Mediation 21.10 Centering 21.11 Mediation", " Chapter 21 Advanced Multiple Regression 21.1 Introduction to Multiple Regression With multiple regression you are using multiple dependent varibles to predict one DV. When we interpret these Univariate regression we have r. Multiple regression we have R, the multivariate regression coeffecient. This R^2 takes in multiple independant variables. We use multiple regression to test relationship between DV and two or more IVs. We can look at that relationship multiple different ways. We want to know how well this set of DVs predict our IV. More often we look at contribution of individual predictors. How important is X2 in predicting Y. What is seffect of IV when controlling for effect of others IV. What is effect of gender on income we get after we control for educational background. With multiple regression, we can also compare sets of IVs to look at DV. Based on a number of things (ideally theory), we have one model that predicts others. Finally parameter estimation, the Bs are samples of paramters in population. We can estimate the actual value of these weights in population. There are a lot of really intersting questions we can answer with multiple regression. There is peril and limitations! Number one, with correlation you can’t say anything about causation. Another limiation of multiple regression is how many variables do you put in equation? If you have soemthing like census data, you can use tons of IVs. We we select IVs theoretically, ideally we have some DV, we choose IVs to predict it. We run prescreening to see if IVs are good predictors. We want IVs that are correlated with the DV. The IV’s co-vary with the DV. In a perfect world we want no correlation between the IVs. Ideally, but never happens. When we choose IVs to put in equation, important to to decide. Multiple regression is very sensitive to what we do as researchers. The ORDER which we put things in has interesting and counterintuitive results. In some rare cases, this can make cherubs or unicorns. What they are is statistical quirks from things we added to the model at some point. 21.2 Basics of Multiple Regression Tests we are going to run have assumptions. First thing we want to do is prescreen data for normality, linearity, outliers. After we run it we check residual plots. The first thing we need to check that you can’t check with outliers is appropriate Ns. We need enough to run a good test, but we don’t want too many. Textbooks suggests where N &gt; 50 + 8m where m is number of variables. That rule is for general prediction. Is it a good model, just interested in model. This is only one rule of thumb, for general regression. For specific predictors and beta weights. N &gt; 104 + m. So if we want to see if type of shoe is a good predictor of kicking and spitting. This is to check for specific predictors in the model. If we’re interested in type of shoe, we don’t want to go a lot more above this. If your N is too high, you will get a significant result. Regression is really sensitive to sample size. Total model becomes significant if you make it too big, same with beta weights. At that point we’re more interested in effect size, aka the coefficient. If you have a huge dataset, want to validate. Aka take a sample, test. If you get census data, everything is going to be significant. The more IVs we have, the blacker the box, the harder it is to understand the equation. Student suggests to keep sample at less than 5% of population. Next assumption of multivariate regression is outliers. Outliers can be very harmful, very important and most important in multiple regression. Effects solution to general model, can have strong effect on estimated coefficents. We always check for and be mindful of outliers. After we run regression, want to check with residuals. Sometimes residuals are a bit misleading (supposedly from text). Want to always check for multicollinearity. Correlations above .9 can be very dangerous. Can check for singularity with correlation matricies, but normally that’s a logical thing. We want to check tolerance (0-1) and variance inflation factor. You want tolerance to be higher. If VIF is over 10 you are in trouble. Finally, check for normality, linerity, and homoscedacity of residuals. We check these univariatly, are they normal, do we have to transform? We can’t prescreen check, but we can check most of these with residual plot. Non linearity of residuals looks like an inverted U, or maybe exponential. We are looking at overal pattern of bivariate pattern. All non-linearity does is weaken your results. If ou have an inverse U, you are going to have weaker effect. What is important about this is theoretical. If you are running a linear result, but if we want to describe the world more accureatly then you want to use the right type of relationship. Normality is a bit more serious of an assumpiton. If we violate normality, it has importatn type I error implications. If you have messed up residuals, increases type II error. 21.3 Types of Multiple Regression 21.3.1 Standard Multiple Regression We are interested in a full set of IVs to predict at DV. We put all of them in at the same time. All of the predictors at tested at the same time. Equation is built with all variables in at the same time. 21.3.2 Sequential Multiple Regression Most people call it the third type. But in sequential, we enter IVs in steps. So if we are interested in gender, age gap and want to control for educaiton level. First do educaiton level and income. Then we add gender. Our solution will tell us how good is predictor variable after our first one. In this case we are interested in how much the R^2 changes. Often we will put in sets of IVs. Normally we control for demographics first. Things like age, gender, race, SES. Then we add in sets of IVs. What is important about squential regression is that order that IVs are put in is by the researcher, guided by either theory or the question we want to answer. 21.3.3 Statistical or Stepwise Multiple Regression Basic gist of this is we put in all IVs, the let computer figure out what to add in what order to give us the best equation. This is essentially machine learning. Put in as much data as possible to predict. We have a few types Forward Selection-mix them all up to find strongest Backward Deletion - all IVs in at once, use tolerance to see if taking IVs out will take out ones that don’t matter *Stepwise Regression- does a bit of both Some people have a problem of this because it’s hard to generlize these models. These capitalize on random error, these methods are purely statistical. Only calculated based on idea of how thingsin one dataset predict things in this dataset. Any random corelation between is going to have magnified influence in the proceedures. Anytimes that you do a stitistical or stepwise regression you need to use validation. If we are only interested in predicting, don’t care what predicts. We always want to cross validate. We could either get new sample. If it’s a big dataset, 50/50. If not can split 80/20. Almost always, your predicition will be worse on second dataset. This is because the models are overfit. They capitalize on every correlation, even if it’s random. This is problem with machine learning. Huge amount of data. With a machine learning, you run into start up problem. With cognitive models, you have a starting assumption. Most of the people in this class, we are working on theory. Every so often you can use a tool to help narrow it down. If we have situation where all IVs don’t correlate with each other but do with DV, our analysis is simple. We R^2 which is proportion of IVs accoutned for by DV. We also have beta weights that tell us relationhip between unit increase in IV and DV. If IV and DV are correlated, we have problem with interpretation. This is because IV will share variance with DV it is predicting. IV has to have some relationhip with DV to be helpful in regression equation. These help explain difference between partial and semipartial coeffecients. In standard multiple regression, we have a correlation between IV and DV. When we get to our analysis this is zero order correlation. Bivariate rlationiop between IV and the DV. Zero order correlation shows us as if other circles were not there. In multivariate regression, we have all IVs in at once. We get partial and semi-partial correlation. In SPSS the call semi-partial part correlation. It tells us the unique amount of variance accounted for in the DV by th DV. Partial correlation tells us unique correlation that’s acounted for if we took it out. Semi-partial correlation tells us (more important in pscyhology as mesure of influence), it give us the same portion of unique variance but out of the DV without any of the other items. LOOK AT HICKS CHAPTER NOTES TO UNDERSTAND PART VS SEMIPARTIAL CORRELATIONS! 21.4 Interpretation of Results Our initial output on SPSS is confusing as hell. 21.4.1 Model Summary First output we get is model summary. We get R, R^2, and R^2 adjusted. These are global indicators of fit. How strong is realtionhip between set of IVs and DVs. They include all the IVs in our model and the DV all at once. Both of these measures in multiple reegression are inflated. Usually we look at R^2 adjusted to look at strength of model. If we are doing stepwise regression, we also get change in R^2. 21.4.2 ANOVA The next section of analysis is called ANOVA. ANOVA is significance test for that statistic. It gives us an F statistic and p value. Th F statistics tests for linear relationship between set of IVs and DV. 21.4.3 Coefficients Usually most interesting part of output. It gives us our bs, or multivariate regresion coeffients. It gives us our betas, our standardized regression coeffiecnts. Usually we report betas in multiple regression because IVs are on different scales. Makes it easier to look at effect sizes. Each of these will have t-test with a p value. Holding all other IVs constatnt is estimated weight of constant when contorl for others. Is that unique variance significant. That coeffecient also tells us our zero order bivariate correlations. It also gives up the partial and the part correlations. 21.5 OLD MEDIATION MODERATION 21.6 Introductory Ramble Do work that’s important and changes the world. Ger? “Moving goal posts: training for life” 21.7 Multiple Regression If you do serious mediation analysis, you have to read at least two full books. You could read a book for every paragraph in Tabachnink and Fidel. With multiple regression, want to predict DV based on two or more IVs. Because coefficients are often on different scales, we often use beta weights. We use real constants when it comes to real life stuff. 21.7.1 SPSS Notes Model summary is global summary. R^2 is amount of variance explained by whole model. We use adjusted R^2, always use that. ANOVA table has regression, residual, and total. Everything that is used to calculated our F test. Mostly just interested in F and p value. This is looking for significant linear relationships between all of IVs and DV. The null hypothesis is that there is no relationship between the two. Also have to look at change statistics, need to look if changed or not. Does the addition of a new IV in step II significantly improve our prediction? For each model, there is a significance test for each coefficient. Each test is looking at significant relationship with IV and DV with other IVs held constant. Zero sum correlation is each IV and the DV. Other IVs considered is where we get into partial and semi partial correlations. Partial correlations are unique variance given all the IVs that one IV accounts for. One IV by itself with no overlap by other IVs. The semi partial correlation is the variance that is accounted for by the other IVs. Semi partial is variance that isn’t already accounted for by something else. Semi partial tells us if IV addition in last step increases the predictability of our model. 21.8 Surpressor Variables Variables in MR equation that are beneficial because of their effect on other IVs, but not the DV. They improve the equation, but not because they predict DV, because they have a quirky effect on other IVs. The way they work is by suppressing irrelevant variance that would normally decrease other IV’s ability to predict. Suppressor variables will improve R^2, even if it doesn’t relate to the DV. How you find a suppressor variable is find variable with high correlation with DV. Hallmarks of suppressor variable is correlation between DV IV correlation and now beta weight. If there is high positive relationship with DV and negative correlation (or VV). If the signs change, that is indicator of suppressor variable. Tabachnik and Fidel outline three types. 1. Classical Suppression - correlates with one other IV but not DV 2. Cooperative/Reciprocal Suppression - suppressor correlates with DV, neg with an IV 3.Negative/Net Suppression - Sign of correlation and coefficient are opposite. There are no rules in identifying a suppressor variable. Look at model with and without IVs, see how it improves. You can put it in the model, but identify it as a suppressor. You want it because it improves your predictability, but have to report it! 21.9 Moderation and Mediation Moderation and Mediation, all in mediation. Suggested reading. 21.9.1 Moderation Moderation is another word for interaction in multiple regression. The effect of on IV on a DV changes on a third variable. We have IV that predicts a DV, but that depends on IV. Example is that SES can predict smoking, but if you change stress, smoking changes. Classic iteration, the chart would flip. We create new iV for or interaction which is product of moderators. So in this case we would have Y= SES + Smoking + SES * Smoking We usually put in interactions last. If the coefficient for B_3 is relationship, we have interaction B_3 is amount of change in slope of regression of Y on X when Z changes by 1 unit. (Z is x 2) When we have simple interaction, we need to figure out what that means. We get old school and just create graphs to find that out. To interpret and communicate a significant interaction, you plot regression at different increments. 21.10 Centering Centering can sometimes be helpful. When you put in an interaction, you have a “weird ass combination” of things already in our model. If you have a moderator, you have to center your variables. Centering variables does two things. 1. Makes interpretation easier 2. Purported to reduce multicollinearity without affecting other statistics or measures. Centering is not normalizing. We are subtracting one variable from it’s mean. This creates a variable that becomes a difference score. This is a new variable. Nothing about the ordinality, distance between points, variability stays the same. But now the mean is zero. If you think multicollinearity, then you want to center all you IVs. Centering is only done on continuous variables. You want to center all three of them. Want to run the analysis uncentered before running it centered. This increases interpreability you’re just dealing with more standardized without zero point. Dirty secret that Tab and Fidel allude to (books on moderation and suppression), this does salivate multicollinearity. Centering most of the time in simple interactions doesn’t really help. That will probably help later in class. Centering all in moderation. 21.11 Mediation Mediation is first step towards structural equation modelling. Correlation does not cause causation is kind of sort of true. If there is a good theory and good correlations, could argue for causation. In mediation we have hypothetical causal sequence of three or more variables. We have IV1, IV2, and DV. In mediation we say that IV1 predicts something in IV2, which predicts the DV. We can have full mediation, we have IV1 predicts DV. Then when you put it all in model, IV1 loses it relationship to DV. If you put in IV2 theoretically you think IV1 causes IV2, put it all in IV1 and DV goes away. Then you have full mediation. Need to have a temporal element? Example is that we have relationship between parenting and externalizing behavior. But maybe causally, bad parenting causes other things like self esteem. It’s then self esteem that leads to externalizing behaviors. If we put SE in the measure, and the relationship between parenting and DV go away, it’s that parenting that leads to self esteem. When it’s all in model, let’s say we still see relationship between parenting and DV, it’s partial mediation. You list coefficient with and coefficient without. Test that for significance. This is common in social and cognitive psychology. Aiken and west have great work on mediation. Chris Preacher has books on mediation. There are a few ways to test for significance. Golden Rules for Mediation, confirmed if 1. There is significant relationship between IV1 and DV 2. Significant relationship between IV1 and IV2. 3. IV2 predicts DV, when controlling for IV1. 4. The relationship between IV1 and IV2 is reduced when IV2 is in equation. If it’s reduced to 0, it’s perfect mediation. Can’t have mediation without common sense. This is first step in proving causation with correlation. Need logic and theory! Mark is DV. We have 2 IVs, comp is IV 1, score on compulsory paper CERTIF is midterm exam We are using those to predict We’re gonna do everything! "],
["logistic-regression.html", "Chapter 22 Logistic Regression", " Chapter 22 Logistic Regression Template file "],
["mediation-and-moderation.html", "Chapter 23 Mediation and Moderation", " Chapter 23 Mediation and Moderation Template file "],
["ancova.html", "Chapter 24 ANCOVA 24.1 Theory 24.2 Practice", " Chapter 24 ANCOVA 24.1 Theory 24.1.1 Reducing Noise Just like the ANOVA (Analysis of Variance), the ANCOVA (Analysis of Covariance) is used to analyze experiments by calculating an F ratio for more than 2 groups. As with any F calculation, differences in the dependent variable are caused by two things: The independent variable (signal, systematic variation) Error (noise, unsysematic variation) The F statistic captures this \\[ F = \\frac{Variation\\ Due\\ to\\ IV}{Variation\\ Due\\ to\\ Error} \\] When there is small variation within groups and large variation between groups, we get a large F. CHART HERE When there is large variatin within groups and small variation between groups, we get a small F. The idea with the ANCOVA is that you can reduce your error term (the denominator from above) by choosing a covariate (CV) that is related to the DV and will soak up some random variation in your F test to give you a clearer picture of what is going on. Usually this means controlling for some sort of variable. The classic example is if you wanted to give some kids a test of some mental ability in a between subjects design but you don’t want their age to skew your results. You might have had a control condition, an intervention, and some sort of alternate intervention. Typically you would run an ANOVA on the three groups, but since you know age has been accounted for and you want to remove that effect from the model, you enter age as a covariate in this calculation. Outside of this there are three major applications for ANCOVA. Increase test sensitivity by using the CV(s) to account for more of the error variance Adjust DV scores to what they would be if everyone scored the same on the CV(s) Adjustment of a DV for other DVs taken as CVs Note that use of a CV can adjust DV scores and show a larger effect or the CV can even eliminate the effect. Looking at this another way: Reduces random error by increasing the size of F Reduces systematic error by adjusting for differences in means May increase differences by soaking up error It’s a good idea to use ANCOVA when you are removing variance in the DV related to covariate, but not related to the grouping variable. This decreases the error term and increases power. It’sa bad idea to use ANCOVA when groups differ on their mean level of the covariate. Usually here the covariate and the grouping variable are not independent. An example of this might be when “controlling” for anxiety when studying people with and without depression. Clearly people with depression will have higher levels of anxiety than their controls by nature of having depression! 24.1.2 Assumptions of ANOVA All ANOVA assumptions apply (errors are RIND) Random Independent Normally distributed The Covariate z is unrelated to x, and that z is related to, and in a sense, acts as a suppressor of y The covariate has a linear relationship with the dependent variable Variance of groups are equal. Correlation between y and z is equal for all levels of x (Homogeneity of Regression) You test for homogeneity of regression slopes by including a covariate by group interaction. If it’s significant, you violated an assumption and can’t to ANCOVA! CHART CHART 24.1.3 Best Practice Things you should control for: 1. Broad Descriptors (Age, Social Class) 2. General Ability (intellgience, memory, speed of movement) 3. Personality measures (extraversion, neuroticism) 4. Things you think are relevant 5. Pick CVs that are correlated with DV * Height good for basketball * Height poor for test of math skills 6. CV should not be influenced by treatment 24.2 Practice In order to get our hands dirty with ANCOVA we’re going to look at a dataset where a bunch of sheep were slaughtered. We have three types of sheep (ewe, wether, ram) and a measure of fatness and weight. Let’s first run an ANOVA to see if fatness differs between animal type. library(data.table) ## Warning: package &#39;data.table&#39; was built under R version 3.4.2 deadsheep &lt;- fread(&quot;datasets/deadsheep.csv&quot;) deadsheep[, Animal := as.factor(animal)] sheep.model.1 &lt;- aov(fatness ~ Animal, data=deadsheep) summary(sheep.model.1) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Animal 2 124.4 62.21 3.165 0.0566 . ## Residuals 30 589.6 19.65 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Looking at the output, we note that There was no significant effect of fatness on levels of animal, \\[F(2,30)=3.165, p &gt; .05\\]. Now since we know that the whole weight of the animal might be confounding our answer (it’s related to the DV of fatness, but not nessecarily related to the type of sheep), let’s run an ANCOVA with carcass weight as a covariate and see if that changes our answer. Before we run the ANCOVA, we need to first check that our IV is not significantly related to our CV. We do this by running an ANOVA predicting weigth by animal. #Test IND of IV and CV ind.sheep &lt;- aov(weight ~ Animal, data=deadsheep) ## Check CV for IV indep, want non-sig summary(ind.sheep) # p=.104 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Animal 2 14.20 7.102 2.438 0.104 ## Residuals 30 87.41 2.914 Here we get\\[ F(2,30) = 2.44, p&gt; .05\\] and can move forward. Let’s now run our ANCOVA. Note that R’s default is Type I Sum of Squares, we need to load the car package to give us the correct sum of squares. For further reading on this, check out the Andy Field book on this. sheepANCOVA &lt;- aov(fatness ~ Animal + weight, data=deadsheep) library(car) ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:psych&#39;: ## ## logit Anova(sheepANCOVA, type=&quot;III&quot;) ## Anova Table (Type III tests) ## ## Response: fatness ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 117.17 1 26.529 1.669e-05 *** ## Animal 332.31 2 37.621 8.772e-09 *** ## weight 461.56 1 104.506 3.994e-11 *** ## Residuals 128.08 29 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can then report The covariate, weight, was not significantly related to the the fatness of the animal, \\[F(1, 29) =,p=.0565 \\] and that there was a significant effect of the animal after controlling for weight of the animal, \\[F(2,28) = 37.62, p &lt; .05.\\]. Let’s now check on our marginal means. These are our group means after we have controlled for our covariate. For this we need the effects package. library(effects) ## Loading required package: carData ## ## Attaching package: &#39;carData&#39; ## The following objects are masked from &#39;package:car&#39;: ## ## Guyer, UN, Vocab ## lattice theme set by effectsTheme() ## See ?effectsTheme for details. mean.sheep &lt;- effect(&quot;Animal&quot;, sheepANCOVA, se=TRUE) summary(mean.sheep) ## ## Animal effect ## Animal ## Ewe Ram Wether ## 18.90219 10.53996 14.83058 ## ## Lower 95 Percent Confidence Limits ## Animal ## Ewe Ram Wether ## 17.565871 9.183322 13.532447 ## ## Upper 95 Percent Confidence Limits ## Animal ## Ewe Ram Wether ## 20.23851 11.89660 16.12870 We can then note the estimated marginal means here (18.9 for Ewe, 10.53 for Ram, and 14.83 for Wether) are mean values for fatness once the CV of weight has been controlled for. Lastly, let’s test the homogenaity of the regression slopes assumption (though we should have done this first!). We do this by running the model with an interaction term included. hoRS.sheep &lt;- aov(fatness ~ weight + Animal + animal:weight, data=deadsheep) Anova(hoRS.sheep, type=&quot;III&quot;) ## Anova Table (Type III tests) ## ## Response: fatness ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 64.571 1 14.6355 0.0007007 *** ## weight 180.144 1 40.8308 7.624e-07 *** ## Animal 5.293 2 0.5998 0.5560672 ## weight:animal 8.957 2 1.0151 0.3757923 ## Residuals 119.123 27 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The interaction term is not significant, thus no violation. We can also plot this to see it. All regression lines should be pointing in the same direction when we plot the groups separately. library(ggplot2) ggplot(deadsheep, aes(x = weight, y = fatness, color = Animal)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + labs(title = &quot;Homogeneity of Regression Slopes&quot;, x = &quot;Weight&quot;, y = &quot;Fatness&quot;) If one of these lines would be pointed downwards, we would have an interaction. "],
["manova.html", "Chapter 25 MANOVA", " Chapter 25 MANOVA "],
["repeated-measures-anova-1.html", "Chapter 26 Repeated Measures ANOVA 26.1 Practice", " Chapter 26 Repeated Measures ANOVA Profile analysis is the repeated measures extension of MANOVA where a set of DVs are commensurate (on the same scale). The common use is where a set of DVs represent the same DV measured at multiple time points used in this way it is the multivariate alternative to repeated measures or mixed ANOVA The choice often depends on the number of subjects, power and whether the assumptions associated with within subjects ANOVA can be met (e.g. sphericity) Profile1 The less common use is to compare groups on multiple DVs that are commensurate (e.g. subscales of the same inventory) Current stat packages can be used to perform more complex analyses where there are multiple factorial between subjects effects Questions asked by profile analysis There is one major question asked by profile analysis; Do groups have similar profiles on a set of DVs? profile – sets of scores. profile is really a line, differences scores get put into vectors, then analyze if line is equal to zero or not (see last quote). Segments – difference scores (or other linear combinations) between adjacent DV scores that are used in two of the major tests of profile analysis 26.0.1 Null Hypothesis Parallelism (Interaction) (multivariate) – Parallel Profiles Are the profiles for the two groups the same? This is a test for the interaction in repeated measures ANOVA This is usually the main test of interest in profile analysis An interaction occurs when the profiles are not parallel Profile 2 Equal Levels (Between Subjects) (univariate) On average does one group score higher than the other Averaging across DVs are the groups different This would be the between-groups main effect in mixed ANOVA Profile 3 Flatness (Within Subjects) (multivariate) – This is equivalent to the within subjects main effect in repeated measures ANOVA In profile analysis terms this is a test for the flatness of the profiles “Do all DVs elicit the same average response?” Profile 3 If any of the hypotheses tested by profile analysis are significant, they can be followed by contrasts. Contrasts (on the main effects, with no interaction) Simple effects Simple contrasts Interaction contrasts (done when the interaction and both main effects are significant) Interpretation Usually done through plots of the actual profiles If the flatness hypothesis is rejected than you would plot the average DV scores averaged across groups If equal levels hypothesis is rejected than you would plot the groups scores averaged across DVs And if the parallel profiles hypothesis is rejected you would plot the mean of each group on each DV 26.0.2 Strength of association Calculated in the same way i.e. Eta squared and Partial Eta squared 26.0.3 Limitation Data must be on the same scale This means that any alterations done to one variables need to be applied to the rest This is why it is used often with repeated measures since it is the same variable multiple times Data can be converted to Z-scores first and profile analysis can be applied Done by using the pooled within-subjects standard deviation to standardize all scores Factor scores can also be used (more later) Dangerous since it is based on sample estimates of population standard deviation Causality is limited to manipulated group variables Generalizability is limited to population used Assumptions should be tested on combined DVs but often difficult so screening on original DVs is used Sample size needs to be large enough; more subjects in the smallest cell than number of DVs This affects power and the test for homogeneity of covariance matrices Data can be imputed Power is also determined on whether the univariate assumptions were met or not; profile analysis has more power than univariate tests adjusted for sphericity violations Multivariate normality If there are more subjects in the smallest cell than number of DVs and relatively equal n than PA is robust violations of multivariate normality If very small samples and unequal n than look at the DVs to see if any are particularly skewed All DVs should be checked for univariate and multivariate outliers Homogeneity of Variance-Covariance matrices If you have equal n than skip it If there are unequal n across cells interpret Box’s M at alpha equals .001. Linearity It is assumed that the DVs are linearly related to one another inspection of bivariate plots of the DVs is used to assess this If symmetric DVs (normal) and large sample this can also be ignored 26.0.4 Doubly Repeated MANOVA Doubly manova is a generalization of MANOVA and Profile analysis taken together in one set of data The basic design is multiple DVs taken at multiple time points, but the multiple DVs do not have to be commensurate. For example, students at different schools (private vs. public) are measured on basic math, reading, athleticism and IQ in grades 7 through 12. Profile 5 This can be treated as a between-within (groups by time) singly multivariate design but the time effect has to meet the sphericity assumption Sphericity can be circumvented by using both the DVs and Time in a multivariate design. Called Doubly MANOVA because linear combinations of DVs (at each time) are linearly combined across time. Within subjects and interaction effects are doubly multivariate while the between groups effects is singly multivariate. This can be performed using the repeated measures ANOVA function in SPSS (now with two within subjects IVs) and just interpreting the multivariate tests 26.0.5 Post Tests EQUAL LEVELS If the equal levels or flatness hypotheses are rejected and there are more than levels you need to break down the effect to see where the differences lie. For a significant equal levels test simply use the compute function in SPSS to create averages over all of the DVs. Use this new variable as a DV in a univariate ANOVA where you can use post hoc tests or implement planned comparisons using syntax. FLATNESS If the multivariate test for flatness is rejected than you turn to interpreting comparisons in a univariate within subjects ANOVA. You can rerun the analysis removing the between subjects variables and implement post hoc tests on the within subjects variable or use syntax to use planned comparisons. 26.0.6 Testing interactions - Simple Effects, Simple Comparisons and Interaction Contrasts Profile 6 Whenever the parallelism hypothesis is rejected you need to pull apart the data to try and pinpoint what parts of the profile are causing the interaction Parallelism and Flatness significant, equal levels not significant Simple effects would be used to compare the groups while holding each of the DVs constant Parallelism and Flatness significant, equal levels not significant This is the same as doing a separate ANOVA between groups for each DV A Scheffe adjustment is recommended if doing this post hoc Fs=(k – 1)F(k – 1), k(n – 1) K is number of groups and n is number of subjects Parallelism and Flatness significant, equal levels not significant If any simple effect is significant than it should be followed by simple contrasts that can be implemented through syntax if planned or by post hoc adjustment Parallelism and Equal levels significant, flatness not significant This happens “rarely because if parallelism and levels are significant, flatness is nonsignificant only if profiles for different groups are mirror images that cancel each other out”. Parallelism and Equal levels significant, flatness not significant This is done by doing a series of one-way within subjects ANOVAs for each group separately. Parallelism and Equal levels significant, flatness not significant A Scheffe adjustment is recommended if doing this post hoc Fs=(p – 1)F(p – 1), k(p – 1)(n – 1) P is number of repeated measures, n is number of subjects If any are significant, follow up with simple contrasts on the within subjects variable. If all effects are significant Perform interaction contrasts by separating the data into smaller two by two interactions Profile 7 If all effects are significant This can be done by using the select cases function in SPSS, selecting two groups and doing a mixed ANOVA with just two of the DVs; this will break down the interaction into smaller interactions that are easier to interpret. It can also be done by averaging over groups (form comparisons on the BG variable) and averaging over DVs (form comparisons on the WG variable) and taking the interaction between them. 26.1 Practice "],
["factor-analysis.html", "Chapter 27 Factor Analysis 27.1 Data Preparation 27.2 Analysis I 27.3 Analysis II", " Chapter 27 Factor Analysis Theory Introduction PCA is is first foray into exploring latent variables. Idea is to measure a lot of variables, from a few to a shitton. We get all sorts of scores on all these things, too many for regression, so we run a factor analysis. Question then is are there some existing variables that we are measuring with all DVs, not directly, to capture one underlying construct. FA uses matrix algebra to analyze the correlation between variables. We run FA, throw everything into SPPS (R) and see if we get any really big factors. Then end goal is that these three things will eventually explain most of the variance. PowerPoint, first half will be tons of terms. Powerpoint Used to uncover latent structures of a set of variables. Reduces attribute space from large to small factors. It’s a non-dependent procedure. It does not assume a DV is specified. Factor Analysis Purposes Reduce large number of variables to a small number of factors for modeling purposes Useful when large number of variables precludes modeling all the measures individually Establish that multiple tests measure the same factor Gives justification for administering fewer tests To validate a scale or index Done by demonstrating things that load on same factor Drop items which cross-load on more than one factor Select a subset of variables from larger set based on which original variables have the highest correlations with the principal component factors To create a set of factors to be treated as uncorrelated variables as one approach to handling multicollinearity in such procedures as multiple regression Identify clusters of cases/or outliers Cluster analysis happens with individuals To determine network groups, Q-Mode Factor Types of Analysis Exploratory Factor Analysis (EFA) * Theory building analysis * Commonly uses an extraction technique as Principal Component Analysis (PCA) Confirmatory Factor Analysis * Theory testing * Not done in SPSS * For some people CFA means Con figural Frequency Analysis * In some CFA is better than EFA (vice versa) Factors and components These are dimensions (or latent variables) identified with clusters of variables, as computed using factor analysis Technically speaking, factors (as from principal factor analysis PFA) represents the common variance of variables, excluding unique variance This is a correlation focused approach seeking to reproduce the inter correlation among the variables. Components (as from PCA) reflect both common and unique variance of the variables Maybe be seen as variance focused approach seeking to reproduce both the total variable variance whit all components and to reproduce the correlations. Quick Notes PCA is far more common than PFA. It is common to sue “factors” interchangeable with “components” PCA is used when research purpose is data reduction, reduce data set PFA is generally used when the research purpose is to id latent variables which contribute to the common variance of the set of measured variables, excluding variable specific (unique) variance FA looks at how variance is shared amongst items. PCA PCA is most common form of FA Seeks linear combo of variable such that the maximum variance is extracted from the variables. It then removes this variance and seeks a second linear combination which explains the max proportion of the remaining variance. This is called the principal axis method and results in orthogonal uncorrelated factors. Canonical factor analysis (what the hell is that?) also called d Rao’s cnonicalf actor is differthmethdo few comping the same model as PCA. Looks for highest Caloocan correlation Common factor analysis AKA Principal factor analysis (PFA) or principal axis factor (PAF), common Is form of FA which seeks the least number of factors which can account for the common variance (correlation) of a set of variables Again is different from PCA which seeks the set of factor switch can account for all the common and unique (specific plus error) variance in a set of variables For the geeks: PFA ….. PCA vs Common Factor Analysis For most data sets PCA and common factor analysis lead to same thing (Paper!) PCA is preferred for purpose of data reduction. FA is preferred when the research purpose is detecting data structure or causal modeling. FA is also used in SEM. Other Extration Methods Image factoring: based on correlation matrix of predicted variables rather than actual variables, where each variable is predicted from the others using multiple regression Maximum Likelihood factoring * Based on a linear combination of variables to form factors, where the parameter estimates are those most likely to have resulted in the observed correlation matrix, using MLE methods and assuming multivariate normality * Correlations are weighted by each variable’s uniqueness * Generates a chi-square goodness-of-fit test Alpha factoring * Based on maximizing the reliability of factors, assuming variables are randomly sampled from a universe of variables * All other methods assume cases to be sampled and variables fixed Unweighted lead squares factoring * Based on minimizing sum of squares … Generalized least squares factoring * Based on minimizing the sum of squared differences between observed and estimated correlation matrices, not counting the diagonal. * Based on adjusting ULS by weighting the correlations inversely according to their uniqueness (more unique variables are weighted less) * GLS also generates a chi-square goodness-of-fit test Factor Loadings Analogous to Pearson’s r, the squared factor loading is the percent of variance in that indicator variable explained by the factor. Also called component loading in PCA. If it’s high we call it an indicator variable, as long as it’s not highly correlated with other variables. To get the percent of variance in all variables accounted for by each factor, add the sum of the squared factor loading for that factor (column ) and divide by the number of variables. Th number of variables equals the sum of their variance as the variance of standard variable is 1. This is the same as dividing the factor’s eigen value by the number of variables. Communality or \\[h^2\\] This is the squared multiple correlation for the variable as dependent using the factors as predictors The commonality measures the percent of variance in a given variable expand by all the factors JOINTLY and may be interpreted as the RELLIABILITY OF THE INDICATOR. Low Communality When an indicator variable has a low communality, the factor model is not working well for that indicator and possibly it should be removed from the model. Low communalities communalities across the set of variables indicates the variables are little related to each other However communalities must be interpreted in relation to the interpretability of the factors. If item has LOW commonality, might want to think about removing that variable. Related in relation to interpreitablity of the factors. A commonality of .75 seems high but is meaningless unless the factor on which the variable is loaded in interpretable, though it usually will be. A commonality of .25 seems low but may be meaningful if the item is contributing to a well-defined factor. What is critical is not the coefficient per se, but rather the extent to which the item plays a role in the interpretation of the factor, though often this role is greater when communally is high. Suprrious solutions If the commonality exceeds 1, there is a supruious solution, which reflect too small a sample or the researcher has too many or two few factors. If it looks too good, it probably is. Every model we come up with in psychology is underspecified. Eigenvalues These are byproduct of of matrix transformations. Eigenvalue for given factor measure the variance in all the variable which is accounted for by that factor. Really what we are doing here is looking at correlations in a lot of different ways. The ration of eigen values = ration of explanatory importance of vectors with respect to the variables. The sum of eigen values of one factor over the sum of ALL the eigen values is the explanatory ability of that factor. If a factor ha a low eigen value then it is contribution little to the explanation of variance in the variables and maybe be ignored as reduction with more important factors. The eigenvalue is not the % of variance explained! It is a measure of amount of variance in relation to total variance. Since variables are standardized to have mean of 0 and variance of 1, total variance is equal to the number of variables. Criterial for determing the number of factors There are no rules, just some pointers The most important pointer for determining the number of factors is comprehensibility which is not mathematical criterion, these factors have to make sense. You have to be able to explain them at some point to another human being. In most FA you will find that once you get to beyond 2-3-4 factors, it’s hard to think what each of those factors represent. Often you use one or more of the methods below to determine an appropriate range of solutions to investigate. Kaiser Criterion A commons rule of thumb for dropping the least important factor from the analysis in the KI rule Drop all components with eigenvalues under 1.0 The may overestimate or underestimate the true number of factors Considerable simulation study evinced suggest it usually overestimates the true number of factors No recommended when used as the sole cut off criterion for estimated the number of factors. Screen Plots The cattell scree test plots the components as the X axis and the corresponding eigenvalues as the Y axis. As you move to the right, the eigenvalues drop. When the drop ceases and the curve make an below toward less steep decolien, Cattells scree test says to drop all further comments AFTER the one starting the elbow. This rule is sometimes criticized for begin amenable to research-controlled fudging. That is as picking the elbow as picking the “elbow” can be subjective because the curve has multiple elbows or is a smooth curve, the researcher may be tempted to set the cut-off at the number of factors desired by his or her research agenda. The scree criterion may result in fewer or more factor than the Kaiser criterion. Find the elbow and delete everything AFTER it. Parallel Analysis New one, often recommended as best method to asses the true number of factors. Selects factors that are greater than random. The actual data are factor analyzed, and separately one does a factor analysis of a matrix of random numbers representing the same number of cases and variables For both actual an d random solutions, the number of factors on the x axis and cumulative eigenvalues on the y axis is plotted When the two lines intersect determines the number of factors to be extracted. Variance Explained Criteria Some researcher just just rule to keep enough factors to account for 90% of variation When the researchers goal emphasis parsimony (explaining with as few factor as possible) the criterion could be as low as 50% Joliffee Criterion Delete all components with eigen values under .7 May restful in twice as many factors as the Kaiser criterion. A less used, liberal rule of them Mean Eigen Value Use factors whose eigen values are at or above the mean. This strict rule may result in too few factors. Precautions Before dropping a factor below one’s cut-off. Check it’s correlation with the DV. A very small factor can have a large correlation with DV, in which it should not be dropped Problem,: You have to know hat your DV is. As a rule of thumb, factors should have at least three high, interpretable loadings. Fewwer may suggest that he research has asked for too may factors. Want at least three items that that load on each factor. Rotatation Serves to make the output more understandable and is usually needed to facilitate the interpretation of factors. If you multiply a matrix by sign and cosines, it rotates it in space and what it does is minimize small correlations and maximizes big ones. Rotation retains all the information, but then just changes it. You are just rotation the matrix of factor loadings. Normally you do rotation after. The sum of the eigen values is not affected by rotation Rotation will alter the eigenvalues (and percent of variance explained) of particular factors and will change the factor loadings. Since alternative rotations may explain the same variance. This is a problem often cited as a drawback factor. You can get different meanings based on different rotations! If factor analysis is used, you might want to look for different rotations method to see which lead to the most interpretable factor structure. Realistically, you will often get very similar solutions (at least relatively) No rotation is the default in SPSS. The original, unrotated principal components solution maximizes the sum of squared factor loadings, efficiently creating a set of factors which explain as much of the variance in the original variables as possible. The amount explained is reflected in the sum of the eigenvalues of all factors However,unrotated solutions are hard to interpret because variables tend to load on multiple factors. Big problem with not rotating is that variables load on multiple factors. Varimax Rotation An orthogonal rotation of the factor axes to maximize the variance of the squared loadings of a factor (column) on all the variables (rows) in a factor matrix Has the effect of of differentiating the original variables by extracted factor. Each factor will tend to have either large or small loadings of any particular variable. A varimax solution yields results which make it as easy as possible to identify each variable with a single factor. This is the most common rotation option. Orthogonality assumes that the latent variables are not related to one another, they are independent. Quartimax Rotation An orhtogional alternative which minimizes the number of factors needed to explain each variable. This type of rotation generates a general factor on which most variables are loaded to a high or medium degree. Such a factor structure is usually not helpful to the research purpose. Maximizes loading on one most important factor. Equimax Rotation A compromise between equimax and quartimax. Direct Oblmin The standard method when you want a non-orthogonal (oblique) solution As such, factors are allowed to be correlated This is will result in higher eigen values but diminishes interpretation of the factors. Promax Rotation An alternative non-orthogonal (oblique) rotation method which is computationally faster than than the direct oblimn method ad therefore is sometimes used for very large datasets. Oblique Rotation In oblique rotation you get both a patter matrix and a structure matrix The structure matrix is simply the factor loading matrix as in orthogonal rotation. The pattern matrix, in contrast, contains coefficient which just represents unique contributions. The more factors, the lower the pattern coefficient as a rule since there will e more common contributes to variance explained. For oblique rotation, research looks at both the structure and patter coefficient when attributing a label to a factor. Assumptions Interval data is assumed. Kim and Mueller (1978 b 74-75) note that ordinal data may be sued if it is though that the assignment of ordinal categories other that do not seriously distort the underlying metric scaling Problems wiht Catheorica Variables Note that categorical variables with similar splits will necessarily tend to correlate with each other, regardless of their content (see Gorsuch, 1983) This is particularly apt to occur when dichotomies are used. The correlation will reflect similarity of “difficulty” for items in a testing context, hence such correlated variables are called difficulty factors The researcher should examine the factor loadings of categorical variables with care to assess whether common loading reflects a difficulty factor or substantive correlation. * Improper use of dichotomies can result in too many factors. Problems with Dichotomous Data Dichotomous data tend to yield many factors (by the usual Kaiser criterion), and many variables loaded on these factors (by the usual .40 cutoff), even for randomly generated data. Valid Imputation of Factor Labels Factor analysis is notorious for the subjective involved in imputing factor labels from factor loading. No selection bias/proper specification The exclusion of relevant variables and inclusion of irrelevant variable in correlation matrix being factored will affect, often substantially, the factors which are uncovered. No Outliers Outlive can impact correlations heavily distorting results. The better your correlations, but better your factor analysis. Linearity Same as other tests Multivariate nomrality Ideally also MVN. Homoscedacity Since factors are linear function of measured variables, homscedascity of the relationship is assumed. Not considered a critical assumption of factor analysis. Doing It Factory Analysis Select and measures variables Prepare the correlation matrix Extract factors Determine Number of Factors Rotate factors Interpret results Selecting and Measuring Variables Sample size: T&amp;F recommends at least 300. Number of variables to include, at least 3 measures per factor, preferable 4 or more. Factor Extraction Extraction is the process by which factors are determined from a larger set of variables. Are multiple factor extraction methods PCA is the most common extraction method. The goal is to extract factors that explain as much variance as possible with s few factors as possible– parsimony! Determien Number of Factors Kaisers? Scree? A priori? Factor Rotation Rotation is sued to improve interpret ability Orthogonal rotation == Factors are ind == Varimax Oblique Rotation Factors are allowed to correlate Promax is recommended by Russel (2002) Interpretation Interpret from rotation solution Naming Components, Ideally want variable to load &gt;.40 on one factor and &lt;.3 on all other factors. Generally exclude variable that load &lt;.3 when interpreting a factor. Print option in SPSS to help interpretation: do not print loading &lt; .30. order variable according to loading starting with Factor I. If one item loads highly on 2, either new rotation or throw it out. Further Readings… Kim, Jae-On and Charles W. Mueller (1978a). Introduction to factor analysis: What it is and how to do it. Thousand Oaks, CA: Sage Publications, Quantitative Applications in the Social Sciences Series, No. 13. Kim, Jae-On and Charles W. Mueller (1978b). Factor Analysis: Statistical methods and practical issues. Thousand Oaks, CA: Sage Publications, Quantitative Applications in the Social Sciences Series, No. 14. Kline, Rex B. (1998). Principles and practice of structural equation modeling. NY: Guilford Press. Covers confirmatory factor analysis using SEM techniques. See esp. Ch. 7. Lance, Charles E, Marcus M. Butts, and Lawrence C. Michels (2006). The sources of four commonly reported cutoff criteria: What did they really say? Organizational Research Methods 9(2): 202-220. Discusses Kaiser and other criteria for selecting number of factors. 27.1 Data Preparation Load in data, clean it. require(psych) require(MASS) ## Loading required package: MASS require(GPArotation) ## Loading required package: GPArotation library(data.table) fa7111data &lt;- fread(&quot;datasets/sexRoles.csv&quot;) fa7111matrix &lt;- as.matrix(fa7111data) Initial Assumptions Tests KMO(fa7111matrix) ## Kaiser-Meyer-Olkin factor adequacy ## Call: KMO(r = fa7111matrix) ## Overall MSA = 0.84 ## MSA for each item = ## subno helpful reliant defbel yielding cheerful indpt athlet ## 0.52 0.90 0.87 0.85 0.86 0.76 0.85 0.77 ## shy assert strpers forceful affect flatter loyal analyt ## 0.84 0.88 0.87 0.87 0.83 0.76 0.82 0.86 ## feminine sympathy moody sensitiv undstand compass leaderab soothe ## 0.74 0.89 0.63 0.83 0.88 0.85 0.83 0.87 ## risk decide selfsuff conscien dominant masculin stand happy ## 0.85 0.86 0.84 0.87 0.90 0.81 0.88 0.77 ## softspok warm truthful tender gullible leadact childlik individ ## 0.78 0.89 0.70 0.88 0.72 0.81 0.67 0.86 ## foullang lovchil compete ambitiou gentle ## 0.64 0.74 0.82 0.81 0.87 cortest.bartlett(fa7111matrix,n=369) ## R was not square, finding R from data ## $chisq ## [1] 5809.284 ## ## $p.value ## [1] 0 ## ## $df ## [1] 990 27.2 Analysis I Look at all factor model allFactor7111 &lt;- fa(fa7111matrix,nfactors=45,rotate=&quot;none&quot;) plot(allFactor7111$values) Do a 2 factor analysis with rotation twoFactor7111 &lt;- fa(fa7111matrix,nfactors=2,rotate=&quot;varimax&quot;) twoFactor7111$loadings ## ## Loadings: ## MR1 MR2 ## subno -0.108 ## helpful 0.312 0.389 ## reliant 0.462 0.144 ## defbel 0.439 0.199 ## yielding -0.140 0.344 ## cheerful 0.146 0.384 ## indpt 0.539 ## athlet 0.265 ## shy -0.406 ## assert 0.606 ## strpers 0.658 ## forceful 0.642 -0.115 ## affect 0.160 0.542 ## flatter 0.224 ## loyal 0.146 0.419 ## analyt 0.301 0.127 ## feminine 0.336 ## sympathy 0.531 ## moody -0.173 ## sensitiv 0.121 0.429 ## undstand 0.621 ## compass 0.631 ## leaderab 0.740 ## soothe 0.590 ## risk 0.439 0.162 ## decide 0.545 0.131 ## selfsuff 0.526 0.157 ## conscien 0.320 0.330 ## dominant 0.667 -0.237 ## masculin 0.282 -0.287 ## stand 0.617 0.183 ## happy 0.114 0.441 ## softspok -0.235 0.326 ## warm 0.712 ## truthful 0.110 0.313 ## tender 0.694 ## gullible -0.172 0.118 ## leadact 0.734 ## childlik -0.101 -0.120 ## individ 0.464 ## foullang 0.138 ## lovchil 0.318 ## compete 0.450 ## ambitiou 0.419 0.142 ## gentle 0.691 ## ## MR1 MR2 ## SS loadings 6.060 5.189 ## Proportion Var 0.135 0.115 ## Cumulative Var 0.135 0.250 27.2.1 Analysis I Using the criterion that items should load at least .4 on one factor and less than .3 on another factor, the following items were sorted out of the initial factor analysis. Taken together, these items seem to load on a factor that I would deem as patriarchal. 3 reliant 0.452 0.123 A 4 defbel 0.433 0.198 A 7 indpt 0.521 A 10 assert 0.605 A 11 strpers 0.657 A 12 forceful 0.650 -0.118 A 23 leaderab 0.764 A 25 risk 0.439 0.167 A 26 decide 0.540 0.120 A 27 selfsuff 0.509 0.141 A 29 dominant 0.671 -0.236 A 31 stand 0.605 0.179 A 38 leadact 0.761 A 40 individ 0.444 A 42 lovchil 0.326 A 43 compete 0.451 A 44 ambitiou 0.412 0.142 A And the following items loaded on the second factor and given the high loadings of certain items, I think that this factor could be deemed as matriarchal. 5 yielding -0.136 0.336 B 6 cheerful 0.147 0.373 B 13 affect 0.171 0.556 B 15 loyal 0.146 0.419 B 17 feminine 0.108 0.325 B 18 sympathy 0.526 B 20 sensitiv 0.129 0.426 B 21 undstand 0.611 B 22 compass 0.106 0.628 B 24 soothe 0.581 B 32 happy 0.113 0.433 B 33 softspok -0.236 0.334 B 34 warm 0.720 B 35 truthful 0.106 0.316 B 36 tender 0.711 B 45 gentle 0.702 B 27.2.1.1 Interpretation Interpret the meaning of the two factors (make up a story one or two paragraphs long based on which variable load heaviest on which factors). In other words, does factor one seem to capture some underlying characteristic of the variables, and does factor two seem to capture some different underlying characteristic of the variables. To make your task easier, I have listed the variable labels and the items to which they refer below. I think the two characteristics here exemplify two qualities that pertain to gender roles that people are assumed to take in the public discourse. On one hand, the first factor has tons of items that seem like they would describe individuals that exemplify traditional patriarchal values (ala Butler, hooks). People who are able to identify with those types of values seem to benefit from societies that are based around power structures where it’s advantageous to be able to be on top of the competition in a very alpha-male-ish kind of way. That is not to say these qualities are intrinsically good or bad, but they seem to be the types of values that are valued by more conservative, traditional cultures and seen as strong desirable characteristics. The second factor seems to load more on things that you would ascribe to matriarchal culture. Here you have a lot of the words that would traditionally be associated with motherhood. While this again is not claiming any sort of value judgment, or making a is/ought fallacy, I think most people would agree (especially in older generations) that the words in the second factor tend to be more associated with values that women in society are expected to adhere to and when they step outside of those bounds, are looked on as abnormal. 27.3 Analysis II Since I used the suggested criterion of factors loading above .4 on one factor an less than .3 on the other, I decided to remove items that did not adhere to those loadings. 27.3.0.1 Instructions Delete these variables from the analysis, and re-run the factor analysis (eigenvalues over 2.0). What effects did this have on your results (for example, look at variance explained among other things)? #Removal of Items removed_fa7111data &lt;- fa7111data[,c(3,4,7,10,11,12,23,25,27,29,31,38,40,42,43,44,5,6,13,15,17,18,20,21,22,24,32,33,34,35,36,45)] str(removed_fa7111data) ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 369 obs. of 32 variables: ## $ reliant : int 7 6 6 6 6 6 4 6 6 4 ... ## $ defbel : int 5 6 4 7 7 7 6 7 6 7 ... ## $ indpt : int 7 3 5 6 7 6 3 7 5 5 ... ## $ assert : int 7 4 4 4 7 4 3 5 5 5 ... ## $ strpers : int 7 1 4 3 7 2 4 6 7 6 ... ## $ forceful: int 2 3 3 3 5 2 1 6 6 4 ... ## $ leaderab: int 6 4 4 2 7 3 1 6 5 4 ... ## $ risk : int 2 3 3 5 7 1 1 4 4 2 ... ## $ selfsuff: int 7 5 6 6 5 7 4 6 6 5 ... ## $ dominant: int 1 4 2 4 6 2 1 4 5 4 ... ## $ stand : int 7 4 4 6 7 7 5 7 7 6 ... ## $ leadact : int 2 4 3 3 6 3 1 5 6 5 ... ## $ individ : int 7 4 6 5 6 7 5 7 6 6 ... ## $ lovchil : int 7 7 5 6 7 7 7 6 5 7 ... ## $ compete : int 7 4 2 4 7 1 1 4 3 4 ... ## $ ambitiou: int 7 4 4 6 7 5 1 4 5 4 ... ## $ yielding: int 5 6 4 4 4 4 6 5 4 4 ... ## $ cheerful: int 7 2 5 6 7 6 6 6 4 7 ... ## $ affect : int 7 5 5 5 7 5 7 7 6 7 ... ## $ loyal : int 7 5 7 7 7 7 6 7 6 7 ... ## $ feminine: int 2 5 6 6 4 7 4 6 6 5 ... ## $ sympathy: int 6 5 5 5 7 6 7 6 4 5 ... ## $ sensitiv: int 7 5 4 6 7 5 6 6 6 5 ... ## $ undstand: int 7 5 6 6 6 5 6 6 6 6 ... ## $ compass : int 7 4 6 6 7 5 6 6 5 6 ... ## $ soothe : int 7 4 7 6 7 5 6 5 5 6 ... ## $ happy : int 7 4 6 6 6 6 5 6 3 7 ... ## $ softspok: int 7 6 4 5 4 6 1 6 4 6 ... ## $ warm : int 7 5 5 6 7 6 5 6 5 7 ... ## $ truthful: int 7 6 7 6 7 7 5 7 6 7 ... ## $ tender : int 7 5 6 5 7 4 5 6 5 6 ... ## $ gentle : int 7 4 5 5 7 6 5 6 5 7 ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; removed_fa7111matrix &lt;- as.matrix(removed_fa7111data) #View(removed_fa7111matrix) removed_all &lt;- fa(removed_fa7111matrix, nfactors = 32, rotate=&quot;none&quot;) plot(removed_all$values) # Run Two factor model removed_all_2 &lt;- fa(removed_fa7111matrix, nfactors = 2, rotate = &quot;varimax&quot;) removed_all_2$loadings ## ## Loadings: ## MR1 MR2 ## reliant 0.448 0.123 ## defbel 0.436 0.201 ## indpt 0.529 ## assert 0.617 ## strpers 0.684 ## forceful 0.661 -0.104 ## leaderab 0.737 ## risk 0.435 0.170 ## selfsuff 0.495 0.129 ## dominant 0.676 -0.228 ## stand 0.605 0.185 ## leadact 0.733 ## individ 0.461 ## lovchil 0.314 ## compete 0.449 ## ambitiou 0.416 0.128 ## yielding -0.135 0.329 ## cheerful 0.151 0.367 ## affect 0.184 0.546 ## loyal 0.145 0.418 ## feminine 0.101 0.313 ## sympathy 0.540 ## sensitiv 0.123 0.446 ## undstand 0.628 ## compass 0.102 0.654 ## soothe 0.589 ## happy 0.109 0.418 ## softspok -0.235 0.307 ## warm 0.725 ## truthful 0.319 ## tender 0.697 ## gentle 0.697 ## ## MR1 MR2 ## SS loadings 5.106 4.673 ## Proportion Var 0.160 0.146 ## Cumulative Var 0.160 0.306 After removing the items that did not load on to the big variables, the amount of variance explained went up, as did the individual factor loadings. 27.3.0.2 Instructions Re-run the factor analysis one more time, this time, set eigenvalues to be over 1.0. What effects did this have on your results (for example, how many factors are there now compared to your 2nd and 3rd runs, also look at variance explained among other things)? #removed_all &lt;- fa(removed_fa7111matrix, nfactors = 32, rotate=&quot;none&quot;) plot(removed_all$values) # Run Two factor model removed_all_2.1 &lt;- fa(removed_fa7111matrix, nfactors = 5, rotate = &quot;varimax&quot;) removed_all_2.1$loadings ## ## Loadings: ## MR1 MR2 MR3 MR4 MR5 ## reliant 0.200 0.104 0.616 ## defbel 0.402 0.150 0.195 0.215 ## indpt 0.279 0.603 0.102 ## assert 0.634 0.141 0.120 ## strpers 0.701 0.140 0.149 0.149 ## forceful 0.721 0.132 ## leaderab 0.517 0.347 0.404 ## risk 0.260 0.119 0.111 0.155 0.390 ## selfsuff 0.154 0.692 0.180 ## dominant 0.626 -0.178 0.186 0.249 ## stand 0.508 0.165 0.149 0.287 0.140 ## leadact 0.554 0.308 0.372 ## individ 0.262 0.120 0.346 0.225 ## lovchil 0.299 0.120 ## compete 0.258 0.100 0.622 ## ambitiou 0.134 0.170 0.644 ## yielding -0.250 0.221 0.184 0.106 ## cheerful 0.467 0.219 ## affect 0.240 0.628 0.194 -0.117 ## loyal 0.183 0.477 0.147 ## feminine 0.250 0.154 0.211 ## sympathy 0.166 0.680 ## sensitiv 0.617 0.106 ## undstand 0.247 0.685 0.148 ## compass 0.282 0.734 ## soothe 0.314 0.552 ## happy 0.578 0.205 ## softspok -0.436 0.209 0.130 0.171 ## warm 0.675 0.322 0.125 ## truthful 0.312 0.153 0.116 -0.117 ## tender 0.659 0.285 0.196 ## gentle -0.187 0.626 0.292 0.219 ## ## MR1 MR2 MR3 MR4 MR5 ## SS loadings 3.570 3.231 2.714 2.078 1.641 ## Proportion Var 0.112 0.101 0.085 0.065 0.051 ## Cumulative Var 0.112 0.213 0.297 0.362 0.414 Now that we have five factors, the amount of total variance explained goes down and the factors as they are do not seem to make much sense. The earlier interpretation of the data seemed to be much clearer. "],
["mixed-effects-models.html", "Chapter 28 Mixed Effects Models", " Chapter 28 Mixed Effects Models Template file "],
["confirmatory-factor-analysis.html", "Chapter 29 Confirmatory Factor Analysis", " Chapter 29 Confirmatory Factor Analysis Template file "],
["this-is-a-template-file.html", "Chapter 30 This is a template file", " Chapter 30 This is a template file Template file "],
["references.html", "References", " References "]
]
