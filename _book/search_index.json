[
["index.html", "R for Psych Handbook Chapter 1 Preface", " R for Psych Handbook David John Baker 2018-03-22 Chapter 1 Preface This book serves as a collection of resources used in the LSU psychological statistics courses. It contains some lecture notes, as well as R code and data to run each of the examples in R. The book is still under construction. "],
["intro.html", "Chapter 2 Introduction to R", " Chapter 2 Introduction to R You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2017) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["data-manipulation-in-r.html", "Chapter 3 Data Manipulation in R", " Chapter 3 Data Manipulation in R Here is a review of existing methods. "],
["episotomology-of-statistics.html", "Chapter 4 Episotomology of Statistics", " Chapter 4 Episotomology of Statistics We describe our methods in this chapter. "],
["descriptive-statistics-z-scores-central-limit.html", "Chapter 5 Descriptive Statistics, z Scores, Central Limit 5.1 Example one 5.2 Example two", " Chapter 5 Descriptive Statistics, z Scores, Central Limit Some significant applications are demonstrated in this chapter. 5.1 Example one 5.2 Example two "],
["data-transformations-and-confidence-intervals.html", "Chapter 6 Data Transformations and Confidence Intervals", " Chapter 6 Data Transformations and Confidence Intervals We have finished a nice book. "],
["power-and-effect-size-measures.html", "Chapter 7 Power and Effect Size Measures", " Chapter 7 Power and Effect Size Measures attempt here "],
["correlation-and-regression.html", "Chapter 8 Correlation and Regression", " Chapter 8 Correlation and Regression attempt here "],
["matched-t-test.html", "Chapter 9 Matched T Test", " Chapter 9 Matched T Test Template file "],
["one-way-anova.html", "Chapter 10 One Way ANOVA", " Chapter 10 One Way ANOVA Template file "],
["multiple-comparisons.html", "Chapter 11 Multiple Comparisons", " Chapter 11 Multiple Comparisons Template file "],
["simple-effects-interactions-mixed-designs.html", "Chapter 12 Simple Effects, Interactions, Mixed Designs", " Chapter 12 Simple Effects, Interactions, Mixed Designs Template file "],
["repeated-measures-anova.html", "Chapter 13 Repeated Measures ANOVA:", " Chapter 13 Repeated Measures ANOVA: Template file "],
["mixed-two-way-anova.html", "Chapter 14 Mixed Two Way ANOVA", " Chapter 14 Mixed Two Way ANOVA Template file "],
["multiple-regression.html", "Chapter 15 Multiple Regression", " Chapter 15 Multiple Regression Template file "],
["chi-square.html", "Chapter 16 Chi-Square", " Chapter 16 Chi-Square Template file "],
["non-parametric-data.html", "Chapter 17 Non-Parametric Data", " Chapter 17 Non-Parametric Data Template file "],
["advanced-data-cleaning.html", "Chapter 18 Advanced Data Cleaning", " Chapter 18 Advanced Data Cleaning Template file "],
["advanced-multiple-regression.html", "Chapter 19 Advanced Multiple Regression", " Chapter 19 Advanced Multiple Regression Template file "],
["logistic-regression.html", "Chapter 20 Logistic Regression", " Chapter 20 Logistic Regression Template file "],
["mediation-and-moderation.html", "Chapter 21 Mediation and Moderation", " Chapter 21 Mediation and Moderation Template file "],
["ancova.html", "Chapter 22 ANCOVA 22.1 Theory 22.2 Practice", " Chapter 22 ANCOVA 22.1 Theory 22.1.1 Reducing Noise Just like the ANOVA (Analysis of Variance), the ANCOVA (Analysis of Covariance) is used to analyze experiments by calculating an F ratio for more than 2 groups. As with any F calculation, differences in the dependent variable are caused by two things: The independent variable (signal, systematic variation) Error (noise, unsysematic variation) The F statistic captures this \\[ F = \\frac{Variation\\ Due\\ to\\ IV}{Variation\\ Due\\ to\\ Error} \\] When there is small variation within groups and large variation between groups, we get a large F. CHART HERE When there is large variatin within groups and small variation between groups, we get a small F. The idea with the ANCOVA is that you can reduce your error term (the denominator from above) by choosing a covariate (CV) that is related to the DV and will soak up some random variation in your F test to give you a clearer picture of what is going on. Usually this means controlling for some sort of variable. The classic example is if you wanted to give some kids a test of some mental ability in a between subjects design but you don’t want their age to skew your results. You might have had a control condition, an intervention, and some sort of alternate intervention. Typically you would run an ANOVA on the three groups, but since you know age has been accounted for and you want to remove that effect from the model, you enter age as a covariate in this calculation. Outside of this there are three major applications for ANCOVA. Increase test sensitivity by using the CV(s) to account for more of the error variance Adjust DV scores to what they would be if everyone scored the same on the CV(s) Adjustment of a DV for other DVs taken as CVs Note that use of a CV can adjust DV scores and show a larger effect or the CV can even eliminate the effect. Looking at this another way: Reduces random error by increasing the size of F Reduces systematic error by adjusting for differences in means May increase differences by soaking up error It’s a good idea to use ANCOVA when you are removing variance in the DV related to covariate, but not related to the grouping variable. This decreases the error term and increases power. It’sa bad idea to use ANCOVA when groups differ on their mean level of the covariate. Usually here the covariate and the grouping variable are not independent. An example of this might be when “controlling” for anxiety when studying people with and without depression. Clearly people with depression will have higher levels of anxiety than their controls by nature of having depression! 22.1.2 Assumptions of ANOVA All ANOVA assumptions apply (errors are RIND) Random Independent Normally distributed The Covariate z is unrelated to x, and that z is related to, and in a sense, acts as a suppressor of y The covariate has a linear relationship with the dependent variable Variance of groups are equal. Correlation between y and z is equal for all levels of x (Homogeneity of Regression) You test for homogeneity of regression slopes by including a covariate by group interaction. If it’s significant, you violated an assumption and can’t to ANCOVA! CHART CHART 22.1.3 Best Practice Things you should control for: 1. Broad Descriptors (Age, Social Class) 2. General Ability (intellgience, memory, speed of movement) 3. Personality measures (extraversion, neuroticism) 4. Things you think are relevant 5. Pick CVs that are correlated with DV * Height good for basketball * Height poor for test of math skills 6. CV should not be influenced by treatment 22.2 Practice In order to get our hands dirty with ANCOVA we’re going to look at a dataset where a bunch of sheep were slaughtered. We have three types of sheep (ewe, wether, ram) and a measure of fatness and weight. Let’s first run an ANOVA to see if fatness differs between animal type. library(data.table) ## Warning: package &#39;data.table&#39; was built under R version 3.4.2 deadsheep &lt;- fread(&quot;datasets/deadsheep.csv&quot;) deadsheep[, Animal := as.factor(animal)] sheep.model.1 &lt;- aov(fatness ~ Animal, data=deadsheep) summary(sheep.model.1) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Animal 2 124.4 62.21 3.165 0.0566 . ## Residuals 30 589.6 19.65 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Looking at the output, we note that There was no significant effect of fatness on levels of animal, \\[F(2,30)=3.165, p &gt; .05\\]. Now since we know that the whole weight of the animal might be confounding our answer (it’s related to the DV of fatness, but not nessecarily related to the type of sheep), let’s run an ANCOVA with carcass weight as a covariate and see if that changes our answer. Before we run the ANCOVA, we need to first check that our IV is not significantly related to our CV. We do this by running an ANOVA predicting weigth by animal. #Test IND of IV and CV ind.sheep &lt;- aov(weight ~ Animal, data=deadsheep) ## Check CV for IV indep, want non-sig summary(ind.sheep) # p=.104 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Animal 2 14.20 7.102 2.438 0.104 ## Residuals 30 87.41 2.914 Here we get\\[ F(2,30) = 2.44, p&gt; .05\\] and can move forward. Let’s now run our ANCOVA. Note that R’s default is Type I Sum of Squares, we need to load the car package to give us the correct sum of squares. For further reading on this, check out the Andy Field book on this. sheepANCOVA &lt;- aov(fatness ~ Animal + weight, data=deadsheep) library(car) Anova(sheepANCOVA, type=&quot;III&quot;) ## Anova Table (Type III tests) ## ## Response: fatness ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 117.17 1 26.529 1.669e-05 *** ## Animal 332.31 2 37.621 8.772e-09 *** ## weight 461.56 1 104.506 3.994e-11 *** ## Residuals 128.08 29 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can then report The covariate, weight, was not significantly related to the the fatness of the animal, \\[F(1, 29) =,p=.0565 \\] and that there was a significant effect of the animal after controlling for weight of the animal, \\[F(2,28) = 37.62, p &lt; .05.\\]. Let’s now check on our marginal means. These are our group means after we have controlled for our covariate. For this we need the effects package. library(effects) ## Loading required package: carData ## ## Attaching package: &#39;carData&#39; ## The following objects are masked from &#39;package:car&#39;: ## ## Guyer, UN, Vocab ## lattice theme set by effectsTheme() ## See ?effectsTheme for details. mean.sheep &lt;- effect(&quot;Animal&quot;, sheepANCOVA, se=TRUE) summary(mean.sheep) ## ## Animal effect ## Animal ## Ewe Ram Wether ## 18.90219 10.53996 14.83058 ## ## Lower 95 Percent Confidence Limits ## Animal ## Ewe Ram Wether ## 17.565871 9.183322 13.532447 ## ## Upper 95 Percent Confidence Limits ## Animal ## Ewe Ram Wether ## 20.23851 11.89660 16.12870 We can then note the estimated marginal means here (18.9 for Ewe, 10.53 for Ram, and 14.83 for Wether) are mean values for fatness once the CV of weight has been controlled for. Lastly, let’s test the homogenaity of the regression slopes assumption (though we should have done this first!). We do this by running the model with an interaction term included. hoRS.sheep &lt;- aov(fatness ~ weight + Animal + animal:weight, data=deadsheep) Anova(hoRS.sheep, type=&quot;III&quot;) ## Anova Table (Type III tests) ## ## Response: fatness ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 64.571 1 14.6355 0.0007007 *** ## weight 180.144 1 40.8308 7.624e-07 *** ## Animal 5.293 2 0.5998 0.5560672 ## weight:animal 8.957 2 1.0151 0.3757923 ## Residuals 119.123 27 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The interaction term is not significant, thus no violation. We can also plot this to see it. All regression lines should be pointing in the same direction when we plot the groups separately. library(ggplot2) ggplot(deadsheep, aes(x = weight, y = fatness, color = Animal)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + labs(title = &quot;Homogeneity of Regression Slopes&quot;, x = &quot;Weight&quot;, y = &quot;Fatness&quot;) If one of these lines would be pointed downwards, we would have an interaction. "],
["manova.html", "Chapter 23 MANOVA", " Chapter 23 MANOVA Template file "],
["repeated-measures-anova-1.html", "Chapter 24 Repeated Measures ANOVA", " Chapter 24 Repeated Measures ANOVA Template file "],
["factor-analysis.html", "Chapter 25 Factor Analysis", " Chapter 25 Factor Analysis Template file "],
["mixed-effects-models.html", "Chapter 26 Mixed Effects Models", " Chapter 26 Mixed Effects Models Template file "],
["confirmatory-factor-analysis.html", "Chapter 27 Confirmatory Factor Analysis", " Chapter 27 Confirmatory Factor Analysis Template file "],
["this-is-a-template-file.html", "Chapter 28 This is a template file", " Chapter 28 This is a template file Template file "],
["references.html", "References", " References "]
]
